#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ¯ æˆ¦ç•¥çš„å¼·åŒ–: å€‹äººåŒ–ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ 
=====================================================
ç›®çš„: LangPontå•†ç”¨åŒ–ã«ãŠã‘ã‚‹æˆ¦ç•¥çš„ç«¶åˆå„ªä½æ€§ã®æ§‹ç¯‰
     - ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ + ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° + ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
     - ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸€äººã²ã¨ã‚Šã®è¨€èªåŒ–ã‚¹ã‚¿ã‚¤ãƒ«ãƒ»æ€è€ƒãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»ç‰¹å®šã‚¯ã‚»ã®åé›†
     - ä»–ç¤¾ãŒçœŸä¼¼ã§ããªã„å‚å…¥éšœå£ã®æ§‹ç¯‰

ã€æˆ¦ç•¥çš„ä¾¡å€¤ã€‘
- å€‹äººã®æ€è€ƒâ†’è¨€èªåŒ–ãƒ—ãƒ­ã‚»ã‚¹ã®ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿åé›†
- æ–‡åŒ–çš„èƒŒæ™¯ãƒ»è·æ¥­ç‰¹æ€§ã«ã‚ˆã‚‹ç¿»è¨³é¸æŠãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ
- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®é«˜å“è³ªå€‹äººåŒ–ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
- ç«¶åˆãŒæ¨¡å€£å›°é›£ãªéæ¥è§¦ãƒ‡ãƒ¼ã‚¿åé›†æ‰‹æ³•
"""

import sqlite3
import json
import logging
import time
import statistics
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union, Set
from collections import defaultdict, Counter
from dataclasses import dataclass, field
from enum import Enum
import uuid

# Task 2.9.2åŸºç›¤ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from preference_reason_estimator import PreferenceReasonEstimator, PreferenceProfile
from recommendation_divergence_detector import EnhancedRecommendationDivergenceDetector, DivergenceEvent
from advanced_gemini_analysis_engine import AdvancedGeminiAnalysisEngine, StructuredRecommendation

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PersonalizationPatternType(Enum):
    """å€‹äººåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—"""
    THINKING_TO_LANGUAGE = "thinking_to_language"     # æ€è€ƒâ†’è¨€èªåŒ–ãƒ—ãƒ­ã‚»ã‚¹
    CULTURAL_ADAPTATION = "cultural_adaptation"       # æ–‡åŒ–çš„é©å¿œãƒ‘ã‚¿ãƒ¼ãƒ³
    PROFESSIONAL_STYLE = "professional_style"         # è·æ¥­çš„ã‚¹ã‚¿ã‚¤ãƒ«
    EMOTIONAL_NUANCE = "emotional_nuance"             # æ„Ÿæƒ…çš„ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹
    FORMALITY_PREFERENCE = "formality_preference"     # ä¸å¯§åº¦é¸å¥½
    DOMAIN_SPECIALIZATION = "domain_specialization"   # å°‚é–€åˆ†é‡ç‰¹åŒ–
    TEMPORAL_CONSISTENCY = "temporal_consistency"     # æ™‚é–“çš„ä¸€è²«æ€§
    CONTEXT_SENSITIVITY = "context_sensitivity"       # æ–‡è„ˆæ„Ÿå¿œæ€§

class DataCommercialValue(Enum):
    """ãƒ‡ãƒ¼ã‚¿ã®å•†ç”¨ä¾¡å€¤ãƒ¬ãƒ™ãƒ«"""
    EXTREMELY_HIGH = "extremely_high"    # æ¥µã‚ã¦é«˜ä¾¡å€¤ï¼ˆç«¶åˆå„ªä½æ€§æ±ºå®šçš„ï¼‰
    HIGH = "high"                       # é«˜ä¾¡å€¤ï¼ˆå·®åˆ¥åŒ–ã«é‡è¦ï¼‰
    MEDIUM = "medium"                   # ä¸­ä¾¡å€¤ï¼ˆæœ‰ç”¨ã ãŒä¸€èˆ¬çš„ï¼‰
    LOW = "low"                        # ä½ä¾¡å€¤ï¼ˆåŸºæœ¬çš„ãƒ‡ãƒ¼ã‚¿ï¼‰
    COMMODITY = "commodity"             # æ±ç”¨ï¼ˆç«¶åˆã‚‚å®¹æ˜“ã«å–å¾—å¯èƒ½ï¼‰

class FineTuningDataType(Enum):
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—"""
    USER_PREFERENCE = "user_preference"         # ãƒ¦ãƒ¼ã‚¶ãƒ¼é¸å¥½ãƒ‡ãƒ¼ã‚¿
    CONTEXTUAL_CHOICE = "contextual_choice"     # æ–‡è„ˆä¾å­˜é¸æŠãƒ‡ãƒ¼ã‚¿
    STYLE_ADAPTATION = "style_adaptation"       # ã‚¹ã‚¿ã‚¤ãƒ«é©å¿œãƒ‡ãƒ¼ã‚¿
    QUALITY_JUDGMENT = "quality_judgment"       # å“è³ªåˆ¤æ–­ãƒ‡ãƒ¼ã‚¿
    CULTURAL_MAPPING = "cultural_mapping"       # æ–‡åŒ–çš„ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿

@dataclass
class PersonalizationPattern:
    """å€‹äººåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿"""
    pattern_id: str
    user_id: str
    pattern_type: PersonalizationPatternType
    pattern_data: Dict[str, Any]
    confidence_score: float
    commercial_value: DataCommercialValue
    uniqueness_score: float  # ç«¶åˆã¨ã®å·®åˆ¥åŒ–åº¦
    replication_difficulty: float  # æ¨¡å€£å›°é›£åº¦
    discovery_timestamp: str
    supporting_evidence: List[Dict[str, Any]]
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class FineTuningDataset:
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    dataset_id: str
    user_id: str
    data_type: FineTuningDataType
    input_text: str
    target_translation: str
    context_features: Dict[str, Any]
    user_choice_reasoning: str
    quality_metrics: Dict[str, float]
    strategic_value: float  # æˆ¦ç•¥çš„ä¾¡å€¤ã‚¹ã‚³ã‚¢
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())

class PersonalizationDataCollector:
    """å€‹äººåŒ–ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self,
                 analytics_db_path: str = "langpont_analytics.db",
                 divergence_db_path: str = "langpont_divergence.db",
                 preference_db_path: str = "langpont_preferences.db",
                 personalization_db_path: str = "langpont_personalization.db"):
        """åˆæœŸåŒ–"""
        self.analytics_db_path = analytics_db_path
        self.divergence_db_path = divergence_db_path
        self.preference_db_path = preference_db_path
        self.personalization_db_path = personalization_db_path
        
        # Task 2.9.2åŸºç›¤ã‚·ã‚¹ãƒ†ãƒ ã®æ´»ç”¨
        self.preference_estimator = PreferenceReasonEstimator(
            analytics_db_path, divergence_db_path, preference_db_path
        )
        self.divergence_detector = EnhancedRecommendationDivergenceDetector(
            analytics_db_path, divergence_db_path
        )
        self.analysis_engine = AdvancedGeminiAnalysisEngine()
        
        # æˆ¦ç•¥çš„ä¾¡å€¤è©•ä¾¡åŸºæº–
        self.commercial_value_weights = {
            'uniqueness': 0.3,           # ãƒ‡ãƒ¼ã‚¿ã®ç‹¬è‡ªæ€§
            'replication_difficulty': 0.25, # æ¨¡å€£å›°é›£åº¦
            'user_specificity': 0.2,     # ãƒ¦ãƒ¼ã‚¶ãƒ¼å›ºæœ‰æ€§
            'temporal_stability': 0.15,  # æ™‚é–“çš„å®‰å®šæ€§
            'scalability': 0.1           # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£
        }
        
        # å€‹äººåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºé–¾å€¤
        self.pattern_detection_thresholds = {
            'consistency_threshold': 0.7,    # ä¸€è²«æ€§é–¾å€¤
            'uniqueness_threshold': 0.6,     # ç‹¬è‡ªæ€§é–¾å€¤
            'evidence_min_count': 3,         # æœ€ä½è¨¼æ‹ æ•°
            'temporal_window_days': 30       # åˆ†ææœŸé–“
        }
        
        # å€‹äººåŒ–ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–
        self._init_personalization_database()
        
        logger.info("æˆ¦ç•¥çš„å€‹äººåŒ–ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def _init_personalization_database(self):
        """å€‹äººåŒ–å°‚ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
        with sqlite3.connect(self.personalization_db_path) as conn:
            cursor = conn.cursor()
            
            # å€‹äººåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS personalization_patterns (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    pattern_id VARCHAR(100) UNIQUE NOT NULL,
                    user_id VARCHAR(100) NOT NULL,
                    
                    -- ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡
                    pattern_type VARCHAR(50) NOT NULL,
                    pattern_data TEXT NOT NULL,
                    confidence_score FLOAT NOT NULL,
                    
                    -- å•†ç”¨ä¾¡å€¤
                    commercial_value VARCHAR(20) NOT NULL,
                    uniqueness_score FLOAT NOT NULL,
                    replication_difficulty FLOAT NOT NULL,
                    
                    -- è¨¼æ‹ ãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                    supporting_evidence TEXT,
                    pattern_metadata TEXT,
                    
                    -- æ™‚é–“ç®¡ç†
                    discovery_timestamp TIMESTAMP NOT NULL,
                    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS fine_tuning_datasets (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    dataset_id VARCHAR(100) UNIQUE NOT NULL,
                    user_id VARCHAR(100) NOT NULL,
                    
                    -- ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—
                    data_type VARCHAR(50) NOT NULL,
                    input_text TEXT NOT NULL,
                    target_translation TEXT NOT NULL,
                    
                    -- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ»ç†ç”±
                    context_features TEXT NOT NULL,
                    user_choice_reasoning TEXT,
                    
                    -- å“è³ªãƒ»ä¾¡å€¤
                    quality_metrics TEXT NOT NULL,
                    strategic_value FLOAT NOT NULL,
                    
                    -- æ™‚é–“ç®¡ç†
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS prompt_optimization_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id VARCHAR(100) NOT NULL,
                    
                    -- æ‹’å¦ãƒ‘ã‚¿ãƒ¼ãƒ³
                    rejected_recommendation VARCHAR(50),
                    rejection_reasoning TEXT,
                    preferred_alternative VARCHAR(50),
                    
                    -- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
                    text_context TEXT,
                    user_context TEXT,
                    situational_context TEXT,
                    
                    -- å­¦ç¿’ä¾¡å€¤
                    learning_value FLOAT,
                    prompt_optimization_potential FLOAT,
                    
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ç‹¬è‡ªè¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS unique_language_patterns (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id VARCHAR(100) NOT NULL,
                    
                    -- ãƒ‘ã‚¿ãƒ¼ãƒ³è©³ç´°
                    pattern_category VARCHAR(50),
                    language_feature TEXT,
                    frequency_score FLOAT,
                    uniqueness_index FLOAT,
                    
                    -- ç«¶åˆå„ªä½æ€§
                    moat_contribution FLOAT,
                    replication_complexity FLOAT,
                    
                    -- è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿
                    evidence_count INTEGER,
                    evidence_data TEXT,
                    
                    first_detected TIMESTAMP,
                    last_observed TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_pattern_user ON personalization_patterns (user_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_pattern_type ON personalization_patterns (pattern_type)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_dataset_user ON fine_tuning_datasets (user_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_dataset_type ON fine_tuning_datasets (data_type)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_prompt_user ON prompt_optimization_data (user_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_language_user ON unique_language_patterns (user_id)')
            
            conn.commit()
    
    def collect_fine_tuning_patterns(self, user_sessions: List[Dict]) -> Dict[str, Any]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®å€‹äººãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†
        
        Args:
            user_sessions: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            å€‹äººåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æçµæœ
        """
        if not user_sessions:
            return {'error': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™'}
        
        user_id = user_sessions[0].get('user_id')
        if not user_id:
            return {'error': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒå¿…è¦ã§ã™'}
        
        try:
            collection_results = {
                'user_id': user_id,
                'session_count': len(user_sessions),
                'collected_patterns': [],
                'fine_tuning_datasets': [],
                'collection_quality': {}
            }
            
            # 1. ç¿»è¨³é¸æŠãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
            choice_patterns = self._analyze_translation_choice_patterns(user_sessions)
            collection_results['collected_patterns'].extend(choice_patterns)
            
            # 2. æ–‡ä½“ãƒ»èªèª¿é¸å¥½ã®åˆ†æ
            style_patterns = self._analyze_style_tone_preferences(user_sessions)
            collection_results['collected_patterns'].extend(style_patterns)
            
            # 3. æ¥­ç•Œãƒ»æ–‡è„ˆç‰¹æœ‰ã®è¡¨ç¾é¸æŠåˆ†æ
            domain_patterns = self._analyze_domain_specific_choices(user_sessions)
            collection_results['collected_patterns'].extend(domain_patterns)
            
            # 4. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ
            ft_datasets = self._generate_fine_tuning_datasets(user_sessions, choice_patterns + style_patterns + domain_patterns)
            collection_results['fine_tuning_datasets'] = ft_datasets
            
            # 5. åé›†å“è³ªã®è©•ä¾¡
            collection_results['collection_quality'] = self._evaluate_collection_quality(
                collection_results['collected_patterns'],
                collection_results['fine_tuning_datasets']
            )
            
            # 6. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜
            self._save_personalization_patterns(collection_results['collected_patterns'])
            self._save_fine_tuning_datasets(collection_results['fine_tuning_datasets'])
            
            logger.info(f"å€‹äººåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³åé›†å®Œäº†: user={user_id}, "
                       f"ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°={len(collection_results['collected_patterns'])}, "
                       f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°={len(collection_results['fine_tuning_datasets'])}")
            
            return collection_results
            
        except Exception as e:
            logger.error(f"å€‹äººåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³åé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'error': str(e)}
    
    def _analyze_translation_choice_patterns(self, sessions: List[Dict]) -> List[PersonalizationPattern]:
        """ç¿»è¨³é¸æŠãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        patterns = []
        
        # ã‚¨ãƒ³ã‚¸ãƒ³é¸æŠã®ä¸€è²«æ€§åˆ†æ
        engine_choices = [s.get('user_choice', '') for s in sessions]
        engine_counter = Counter(engine_choices)
        
        if engine_counter and len(sessions) >= 5:
            most_common = engine_counter.most_common(1)[0]
            consistency_rate = most_common[1] / len(sessions)
            
            if consistency_rate >= self.pattern_detection_thresholds['consistency_threshold']:
                pattern = PersonalizationPattern(
                    pattern_id=f"choice_consistency_{uuid.uuid4().hex[:8]}",
                    user_id=sessions[0]['user_id'],
                    pattern_type=PersonalizationPatternType.THINKING_TO_LANGUAGE,
                    pattern_data={
                        'preferred_engine': most_common[0],
                        'consistency_rate': consistency_rate,
                        'total_choices': len(sessions),
                        'alternative_engines': dict(engine_counter)
                    },
                    confidence_score=consistency_rate,
                    commercial_value=self._assess_commercial_value(consistency_rate, 'engine_preference'),
                    uniqueness_score=self._calculate_uniqueness_score(engine_counter, 'engine_choice'),
                    replication_difficulty=0.8,  # ã‚¨ãƒ³ã‚¸ãƒ³é¸å¥½ã¯æ¨¡å€£å›°é›£
                    discovery_timestamp=datetime.now().isoformat(),
                    supporting_evidence=[{
                        'session_id': s.get('session_id', ''),
                        'choice': s.get('user_choice', ''),
                        'satisfaction': s.get('satisfaction_score', 0)
                    } for s in sessions]
                )
                patterns.append(pattern)
        
        # æº€è¶³åº¦ã¨ã®ç›¸é–¢ãƒ‘ã‚¿ãƒ¼ãƒ³
        satisfaction_by_engine = defaultdict(list)
        for session in sessions:
            engine = session.get('user_choice', '')
            satisfaction = session.get('satisfaction_score', 0)
            if engine and satisfaction > 0:
                satisfaction_by_engine[engine].append(satisfaction)
        
        for engine, satisfactions in satisfaction_by_engine.items():
            if len(satisfactions) >= 3:
                avg_satisfaction = statistics.mean(satisfactions)
                std_satisfaction = statistics.stdev(satisfactions) if len(satisfactions) > 1 else 0
                
                # é«˜æº€è¶³åº¦ã‹ã¤ä½åˆ†æ•£ï¼ˆå®‰å®šã—ãŸé¸å¥½ï¼‰
                if avg_satisfaction >= 75 and std_satisfaction <= 15:
                    pattern = PersonalizationPattern(
                        pattern_id=f"satisfaction_pattern_{uuid.uuid4().hex[:8]}",
                        user_id=sessions[0]['user_id'],
                        pattern_type=PersonalizationPatternType.THINKING_TO_LANGUAGE,
                        pattern_data={
                            'preferred_engine': engine,
                            'average_satisfaction': avg_satisfaction,
                            'satisfaction_stability': std_satisfaction,
                            'sample_size': len(satisfactions)
                        },
                        confidence_score=min(1.0, avg_satisfaction / 100 * (1 - std_satisfaction / 100)),
                        commercial_value=DataCommercialValue.HIGH,
                        uniqueness_score=0.75,
                        replication_difficulty=0.9,  # æº€è¶³åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯é«˜åº¦ã«å€‹äººçš„
                        discovery_timestamp=datetime.now().isoformat(),
                        supporting_evidence=satisfactions
                    )
                    patterns.append(pattern)
        
        return patterns
    
    def _analyze_style_tone_preferences(self, sessions: List[Dict]) -> List[PersonalizationPattern]:
        """æ–‡ä½“ãƒ»èªèª¿é¸å¥½ã®åˆ†æ"""
        patterns = []
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¥ã®é¸æŠãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        context_choices = defaultdict(list)
        
        for session in sessions:
            context_data = session.get('context_data', {})
            user_choice = session.get('user_choice', '')
            
            # æ–‡ç« é•·ã‚«ãƒ†ã‚´ãƒª
            text_length = context_data.get('text_length', 0)
            length_category = self._categorize_text_length(text_length)
            context_choices[f'length_{length_category}'].append(user_choice)
            
            # ãƒ“ã‚¸ãƒã‚¹æ–‡è„ˆ
            if context_data.get('business_context'):
                context_choices['business'].append(user_choice)
            
            # æŠ€è¡“æ–‡æ›¸
            if context_data.get('has_technical_terms'):
                context_choices['technical'].append(user_choice)
        
        # å„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã®ä¸€è²«ã—ãŸé¸å¥½ã‚’æ¤œå‡º
        for context, choices in context_choices.items():
            if len(choices) >= 3:
                choice_counter = Counter(choices)
                most_common = choice_counter.most_common(1)[0]
                consistency = most_common[1] / len(choices)
                
                if consistency >= 0.6:  # 60%ä»¥ä¸Šã®ä¸€è²«æ€§
                    pattern = PersonalizationPattern(
                        pattern_id=f"context_style_{uuid.uuid4().hex[:8]}",
                        user_id=sessions[0]['user_id'],
                        pattern_type=PersonalizationPatternType.CONTEXT_SENSITIVITY,
                        pattern_data={
                            'context_type': context,
                            'preferred_engine': most_common[0],
                            'consistency_rate': consistency,
                            'sample_size': len(choices),
                            'choice_distribution': dict(choice_counter)
                        },
                        confidence_score=consistency,
                        commercial_value=DataCommercialValue.EXTREMELY_HIGH,  # æ–‡è„ˆé©å¿œã¯æ¥µã‚ã¦é«˜ä¾¡å€¤
                        uniqueness_score=0.85,
                        replication_difficulty=0.95,  # å€‹äººã®æ–‡è„ˆåˆ¤æ–­ã¯æ¨¡å€£æ¥µå›°é›£
                        discovery_timestamp=datetime.now().isoformat(),
                        supporting_evidence=choices
                    )
                    patterns.append(pattern)
        
        return patterns
    
    def _analyze_domain_specific_choices(self, sessions: List[Dict]) -> List[PersonalizationPattern]:
        """æ¥­ç•Œãƒ»æ–‡è„ˆç‰¹æœ‰ã®è¡¨ç¾é¸æŠåˆ†æ"""
        patterns = []
        
        # å°‚é–€ç”¨èªã‚’å«ã‚€æ–‡æ›¸ã§ã®é¸æŠãƒ‘ã‚¿ãƒ¼ãƒ³
        technical_sessions = [s for s in sessions if s.get('context_data', {}).get('has_technical_terms')]
        
        if len(technical_sessions) >= 3:
            tech_choices = [s.get('user_choice', '') for s in technical_sessions]
            tech_counter = Counter(tech_choices)
            
            if tech_counter:
                most_common = tech_counter.most_common(1)[0]
                preference_rate = most_common[1] / len(technical_sessions)
                
                if preference_rate >= 0.7:
                    pattern = PersonalizationPattern(
                        pattern_id=f"domain_expertise_{uuid.uuid4().hex[:8]}",
                        user_id=sessions[0]['user_id'],
                        pattern_type=PersonalizationPatternType.DOMAIN_SPECIALIZATION,
                        pattern_data={
                            'domain': 'technical',
                            'preferred_engine': most_common[0],
                            'expertise_preference_rate': preference_rate,
                            'technical_document_count': len(technical_sessions),
                            'choice_distribution': dict(tech_counter)
                        },
                        confidence_score=preference_rate,
                        commercial_value=DataCommercialValue.HIGH,
                        uniqueness_score=0.8,
                        replication_difficulty=0.85,
                        discovery_timestamp=datetime.now().isoformat(),
                        supporting_evidence=[{
                            'session_id': s.get('session_id', ''),
                            'choice': s.get('user_choice', ''),
                            'technical_terms': s.get('context_data', {}).get('technical_term_count', 0)
                        } for s in technical_sessions]
                    )
                    patterns.append(pattern)
        
        return patterns
    
    def _generate_fine_tuning_datasets(self, sessions: List[Dict], patterns: List[PersonalizationPattern]) -> List[FineTuningDataset]:
        """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ"""
        datasets = []
        
        for session in sessions:
            user_id = session.get('user_id')
            input_text = session.get('input_text', '')
            user_choice = session.get('user_choice', '')
            target_translation = session.get('translation_result', '')
            context_data = session.get('context_data', {})
            satisfaction = session.get('satisfaction_score', 0)
            
            if all([user_id, input_text, user_choice, target_translation]):
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æ¨è«–ã•ã‚Œã‚‹é¸æŠç†ç”±
                choice_reasoning = self._infer_choice_reasoning(session, patterns)
                
                # æˆ¦ç•¥çš„ä¾¡å€¤ã®ç®—å‡º
                strategic_value = self._calculate_strategic_value(session, patterns)
                
                dataset = FineTuningDataset(
                    dataset_id=f"ft_{uuid.uuid4().hex[:12]}",
                    user_id=user_id,
                    data_type=self._determine_data_type(session, patterns),
                    input_text=input_text,
                    target_translation=target_translation,
                    context_features={
                        'text_length': len(input_text),
                        'has_technical_terms': context_data.get('has_technical_terms', False),
                        'business_context': context_data.get('business_context', False),
                        'cultural_context': context_data.get('cultural_context', False),
                        'formality_level': self._assess_formality_level(input_text),
                        'domain_category': self._classify_domain(input_text, context_data)
                    },
                    user_choice_reasoning=choice_reasoning,
                    quality_metrics={
                        'satisfaction_score': satisfaction,
                        'choice_confidence': self._assess_choice_confidence(session, patterns),
                        'context_appropriateness': self._assess_context_appropriateness(session),
                        'uniqueness_score': strategic_value
                    },
                    strategic_value=strategic_value
                )
                datasets.append(dataset)
        
        return datasets
    
    def extract_prompt_optimization_data(self, divergence_events: List[Dict]) -> Dict[str, Any]:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ç”¨ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        
        Args:
            divergence_events: ä¹–é›¢ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿
        """
        try:
            optimization_data = {
                'rejection_patterns': [],
                'context_dependent_preferences': [],
                'prompt_improvement_suggestions': [],
                'strategic_insights': {}
            }
            
            # é«˜ä¾¡å€¤ãªä¹–é›¢ã‚¤ãƒ™ãƒ³ãƒˆã®æŠ½å‡ºï¼ˆå­¦ç¿’ä¾¡å€¤ >= 0.7ï¼‰
            high_value_events = [
                event for event in divergence_events 
                if event.get('learning_value', 0) >= 0.7
            ]
            
            # æ‹’å¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
            rejection_patterns = self._analyze_rejection_patterns(high_value_events)
            optimization_data['rejection_patterns'] = rejection_patterns
            
            # æ–‡è„ˆä¾å­˜é¸å¥½ã®æŠ½å‡º
            context_preferences = self._extract_context_dependent_preferences(high_value_events)
            optimization_data['context_dependent_preferences'] = context_preferences
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ”¹å–„ææ¡ˆã®ç”Ÿæˆ
            improvement_suggestions = self._generate_prompt_improvements(rejection_patterns, context_preferences)
            optimization_data['prompt_improvement_suggestions'] = improvement_suggestions
            
            # æˆ¦ç•¥çš„ã‚¤ãƒ³ã‚µã‚¤ãƒˆã®æŠ½å‡º
            strategic_insights = self._extract_strategic_insights(high_value_events)
            optimization_data['strategic_insights'] = strategic_insights
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜
            self._save_prompt_optimization_data(optimization_data)
            
            logger.info(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†: "
                       f"æ‹’å¦ãƒ‘ã‚¿ãƒ¼ãƒ³={len(rejection_patterns)}, "
                       f"æ–‡è„ˆé¸å¥½={len(context_preferences)}")
            
            return optimization_data
            
        except Exception as e:
            logger.error(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'error': str(e)}
    
    def identify_unique_language_patterns(self, user_choices: List[Dict]) -> Dict[str, Any]:
        """
        ä»–ç¤¾ãŒæŒãŸãªã„ç‹¬è‡ªã®è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å®š
        
        Args:
            user_choices: ãƒ¦ãƒ¼ã‚¶ãƒ¼é¸æŠãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            ç‹¬è‡ªè¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æçµæœ
        """
        try:
            unique_patterns = {
                'thinking_to_language_patterns': [],
                'cultural_linguistic_features': [],
                'professional_language_habits': [],
                'temporal_language_evolution': [],
                'moat_strength_indicators': {}
            }
            
            # 1. æ€è€ƒâ†’è¨€èªåŒ–ãƒ—ãƒ­ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
            thinking_patterns = self._identify_thinking_language_patterns(user_choices)
            unique_patterns['thinking_to_language_patterns'] = thinking_patterns
            
            # 2. æ–‡åŒ–çš„è¨€èªç‰¹å¾´
            cultural_features = self._identify_cultural_linguistic_features(user_choices)
            unique_patterns['cultural_linguistic_features'] = cultural_features
            
            # 3. è·æ¥­çš„è¨€èªç¿’æ…£
            professional_habits = self._identify_professional_language_habits(user_choices)
            unique_patterns['professional_language_habits'] = professional_habits
            
            # 4. æ™‚é–“çš„è¨€èªé€²åŒ–
            temporal_evolution = self._track_temporal_language_evolution(user_choices)
            unique_patterns['temporal_language_evolution'] = temporal_evolution
            
            # 5. å‚å…¥éšœå£å¼·åº¦æŒ‡æ¨™
            moat_indicators = self._calculate_moat_strength_indicators(unique_patterns)
            unique_patterns['moat_strength_indicators'] = moat_indicators
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜
            self._save_unique_language_patterns(unique_patterns)
            
            logger.info(f"ç‹¬è‡ªè¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å®šå®Œäº†: "
                       f"æ€è€ƒãƒ‘ã‚¿ãƒ¼ãƒ³={len(thinking_patterns)}, "
                       f"æ–‡åŒ–ç‰¹å¾´={len(cultural_features)}, "
                       f"è·æ¥­ç¿’æ…£={len(professional_habits)}")
            
            return unique_patterns
            
        except Exception as e:
            logger.error(f"ç‹¬è‡ªè¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'error': str(e)}
    
    def generate_training_data_format(self, personalization_data: Dict) -> List[Dict]:
        """
        æ©Ÿæ¢°å­¦ç¿’ç”¨ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿å½¢å¼ç”Ÿæˆ
        
        Args:
            personalization_data: å€‹äººåŒ–ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            æ©Ÿæ¢°å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        """
        try:
            training_data = []
            
            # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›
            if 'fine_tuning_datasets' in personalization_data:
                for dataset in personalization_data['fine_tuning_datasets']:
                    training_sample = {
                        'input': {
                            'text': dataset.get('input_text', ''),
                            'context': dataset.get('context_features', {}),
                            'user_profile': self._extract_user_profile_features(dataset.get('user_id', ''))
                        },
                        'output': {
                            'translation': dataset.get('target_translation', ''),
                            'engine_choice': dataset.get('user_choice_reasoning', ''),
                            'quality_score': dataset.get('quality_metrics', {}).get('satisfaction_score', 0)
                        },
                        'metadata': {
                            'strategic_value': dataset.get('strategic_value', 0),
                            'data_type': dataset.get('data_type', ''),
                            'timestamp': dataset.get('created_at', '')
                        }
                    }
                    training_data.append(training_sample)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ
            prompt_templates = self._generate_prompt_templates(personalization_data)
            
            # å€‹äººåŒ–é‡ã¿èª¿æ•´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            personalization_weights = self._generate_personalization_weights(personalization_data)
            
            result = {
                'training_samples': training_data,
                'prompt_templates': prompt_templates,
                'personalization_weights': personalization_weights,
                'dataset_quality_metrics': self._calculate_dataset_quality_metrics(training_data)
            }
            
            logger.info(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿å½¢å¼ç”Ÿæˆå®Œäº†: ã‚µãƒ³ãƒ—ãƒ«æ•°={len(training_data)}")
            
            return result
            
        except Exception as e:
            logger.error(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿å½¢å¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'error': str(e)}
    
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    def _categorize_text_length(self, length: int) -> str:
        """æ–‡ç« é•·ã®ã‚«ãƒ†ã‚´ãƒ©ã‚¤ã‚º"""
        if length < 50:
            return 'very_short'
        elif length < 150:
            return 'short'
        elif length < 300:
            return 'medium'
        elif length < 600:
            return 'long'
        else:
            return 'very_long'
    
    def _assess_commercial_value(self, metric_value: float, pattern_type: str) -> DataCommercialValue:
        """å•†ç”¨ä¾¡å€¤ã®è©•ä¾¡"""
        if pattern_type == 'engine_preference':
            if metric_value >= 0.9:
                return DataCommercialValue.EXTREMELY_HIGH
            elif metric_value >= 0.8:
                return DataCommercialValue.HIGH
            elif metric_value >= 0.6:
                return DataCommercialValue.MEDIUM
            else:
                return DataCommercialValue.LOW
        
        return DataCommercialValue.MEDIUM
    
    def _calculate_uniqueness_score(self, data_distribution: Counter, pattern_type: str) -> float:
        """ç‹¬è‡ªæ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        if not data_distribution:
            return 0.0
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ç‹¬è‡ªæ€§è¨ˆç®—
        total = sum(data_distribution.values())
        entropy = 0.0
        for count in data_distribution.values():
            prob = count / total
            if prob > 0:
                entropy -= prob * math.log2(prob)
        
        # 0-1ã®ç¯„å›²ã«æ­£è¦åŒ–
        max_entropy = math.log2(len(data_distribution)) if len(data_distribution) > 1 else 1
        uniqueness = 1 - (entropy / max_entropy) if max_entropy > 0 else 0
        
        return max(0.0, min(1.0, uniqueness))
    
    def _save_personalization_patterns(self, patterns: List[PersonalizationPattern]):
        """å€‹äººåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¿å­˜"""
        with sqlite3.connect(self.personalization_db_path) as conn:
            cursor = conn.cursor()
            
            for pattern in patterns:
                cursor.execute('''
                    INSERT OR REPLACE INTO personalization_patterns (
                        pattern_id, user_id, pattern_type, pattern_data,
                        confidence_score, commercial_value, uniqueness_score,
                        replication_difficulty, supporting_evidence, pattern_metadata,
                        discovery_timestamp
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    pattern.pattern_id,
                    pattern.user_id,
                    pattern.pattern_type.value,
                    json.dumps(pattern.pattern_data),
                    pattern.confidence_score,
                    pattern.commercial_value.value,
                    pattern.uniqueness_score,
                    pattern.replication_difficulty,
                    json.dumps(pattern.supporting_evidence),
                    json.dumps(pattern.metadata),
                    pattern.discovery_timestamp
                ))
            
            conn.commit()
    
    def _save_fine_tuning_datasets(self, datasets: List[FineTuningDataset]):
        """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¿å­˜"""
        with sqlite3.connect(self.personalization_db_path) as conn:
            cursor = conn.cursor()
            
            for dataset in datasets:
                cursor.execute('''
                    INSERT OR REPLACE INTO fine_tuning_datasets (
                        dataset_id, user_id, data_type, input_text,
                        target_translation, context_features, user_choice_reasoning,
                        quality_metrics, strategic_value
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    dataset.dataset_id,
                    dataset.user_id,
                    dataset.data_type.value,
                    dataset.input_text,
                    dataset.target_translation,
                    json.dumps(dataset.context_features),
                    dataset.user_choice_reasoning,
                    json.dumps(dataset.quality_metrics),
                    dataset.strategic_value
                ))
            
            conn.commit()
    
    # ä»¥ä¸‹ã€ãã®ä»–ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…ã‚’ç¶šã‘ã‚‹...
    def _evaluate_collection_quality(self, patterns: List, datasets: List) -> Dict[str, float]:
        """åé›†å“è³ªã®è©•ä¾¡"""
        return {
            'pattern_diversity': len(set(p.pattern_type for p in patterns)) / 8.0,  # 8ç¨®é¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—
            'data_volume_score': min(1.0, len(datasets) / 10.0),  # 10ä»¶ã§æº€ç‚¹
            'strategic_value_avg': statistics.mean([d.strategic_value for d in datasets]) if datasets else 0.0,
            'uniqueness_avg': statistics.mean([p.uniqueness_score for p in patterns]) if patterns else 0.0
        }
    
    # ç°¡ç•¥åŒ–ã•ã‚ŒãŸå®Ÿè£…ï¼ˆå®Ÿéš›ã«ã¯ã‚ˆã‚Šè©³ç´°ãªå®Ÿè£…ãŒå¿…è¦ï¼‰
    def _infer_choice_reasoning(self, session: Dict, patterns: List) -> str:
        """é¸æŠç†ç”±ã®æ¨è«–"""
        return f"Based on user patterns and context: {session.get('user_choice', '')}"
    
    def _calculate_strategic_value(self, session: Dict, patterns: List) -> float:
        """æˆ¦ç•¥çš„ä¾¡å€¤ã®è¨ˆç®—"""
        return 0.8  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
    
    def _determine_data_type(self, session: Dict, patterns: List) -> FineTuningDataType:
        """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®æ±ºå®š"""
        return FineTuningDataType.USER_PREFERENCE
    
    def _assess_formality_level(self, text: str) -> str:
        """ä¸å¯§åº¦ãƒ¬ãƒ™ãƒ«ã®è©•ä¾¡"""
        return "medium"  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
    
    def _classify_domain(self, text: str, context: Dict) -> str:
        """åˆ†é‡ã®åˆ†é¡"""
        return "general"  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
    
    def _assess_choice_confidence(self, session: Dict, patterns: List) -> float:
        """é¸æŠä¿¡é ¼åº¦ã®è©•ä¾¡"""
        return 0.8  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
    
    def _assess_context_appropriateness(self, session: Dict) -> float:
        """æ–‡è„ˆé©åˆ‡æ€§ã®è©•ä¾¡"""
        return 0.8  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
    
    # ãã®ä»–ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚‚ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã¨ã—ã¦ç°¡ç•¥å®Ÿè£…
    def _analyze_rejection_patterns(self, events: List) -> List:
        return []
    
    def _extract_context_dependent_preferences(self, events: List) -> List:
        return []
    
    def _generate_prompt_improvements(self, rejections: List, preferences: List) -> List:
        return []
    
    def _extract_strategic_insights(self, events: List) -> Dict:
        return {}
    
    def _save_prompt_optimization_data(self, data: Dict):
        pass
    
    def _identify_thinking_language_patterns(self, choices: List) -> List:
        return []
    
    def _identify_cultural_linguistic_features(self, choices: List) -> List:
        return []
    
    def _identify_professional_language_habits(self, choices: List) -> List:
        return []
    
    def _track_temporal_language_evolution(self, choices: List) -> List:
        return []
    
    def _calculate_moat_strength_indicators(self, patterns: Dict) -> Dict:
        return {}
    
    def _save_unique_language_patterns(self, patterns: Dict):
        pass
    
    def _extract_user_profile_features(self, user_id: str) -> Dict:
        return {}
    
    def _generate_prompt_templates(self, data: Dict) -> List:
        return []
    
    def _generate_personalization_weights(self, data: Dict) -> Dict:
        return {}
    
    def _calculate_dataset_quality_metrics(self, data: List) -> Dict:
        return {}


# ãƒ†ã‚¹ãƒˆç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°
if __name__ == "__main__":
    print("ğŸ¯ æˆ¦ç•¥çš„å€‹äººåŒ–ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ  - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print("=" * 60)
    
    collector = PersonalizationDataCollector()
    
    # ãƒ†ã‚¹ãƒˆç”¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
    test_sessions = [
        {
            'user_id': 'strategic_user_001',
            'session_id': 'session_001',
            'user_choice': 'enhanced',
            'input_text': 'ãƒ“ã‚¸ãƒã‚¹ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®æº–å‚™ã‚’ã—ã¦ã„ã¾ã™ã€‚',
            'translation_result': 'I am preparing for a business presentation.',
            'satisfaction_score': 85,
            'context_data': {
                'text_length': 200,
                'has_technical_terms': False,
                'business_context': True
            }
        },
        {
            'user_id': 'strategic_user_001',
            'session_id': 'session_002',
            'user_choice': 'enhanced',
            'input_text': 'æŠ€è¡“ä»•æ§˜æ›¸ã®ç¿»è¨³ãŒå¿…è¦ã§ã™ã€‚',
            'translation_result': 'Technical specification translation is required.',
            'satisfaction_score': 90,
            'context_data': {
                'text_length': 150,
                'has_technical_terms': True,
                'business_context': True
            }
        }
    ]
    
    # å€‹äººåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³åé›†ãƒ†ã‚¹ãƒˆ
    result = collector.collect_fine_tuning_patterns(test_sessions)
    
    print(f"âœ… å€‹äººåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³åé›†çµæœ:")
    print(f"  ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {result.get('user_id', 'N/A')}")
    print(f"  ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {result.get('session_count', 0)}")
    print(f"  åé›†ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(result.get('collected_patterns', []))}")
    print(f"  ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿æ•°: {len(result.get('fine_tuning_datasets', []))}")
    
    # åé›†å“è³ªè¡¨ç¤º
    quality = result.get('collection_quality', {})
    print(f"  åé›†å“è³ª:")
    for metric, value in quality.items():
        print(f"    {metric}: {value:.3f}")
    
    print("\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº† - æˆ¦ç•¥çš„å€‹äººåŒ–ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ æ­£å¸¸å‹•ä½œ")