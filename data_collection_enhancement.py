#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ¯ Task 2.9.2: ãƒ‡ãƒ¼ã‚¿åé›†å¼·åŒ–ã‚·ã‚¹ãƒ†ãƒ 
=====================================================
ç›®çš„: Task 2.9.1åŸºç›¤ã‚’æ´»ç”¨ã—ã€Geminiæ¨å¥¨æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ãƒ»ä¹–é›¢ã‚¤ãƒ™ãƒ³ãƒˆãƒ»
     ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¶™ç¶šè¿½è·¡ã«ã‚ˆã‚Šã€å€‹äººåŒ–ç¿»è¨³AIã®ãŸã‚ã®
     é«˜å“è³ªãƒ‡ãƒ¼ã‚¿åé›†ã‚’å®Ÿç¾ã™ã‚‹

ã€Task 2.9.1.5åŸºç›¤æ´»ç”¨ã€‘
- éæ¥è§¦ãƒ‡ãƒ¼ã‚¿åé›†åŸå‰‡ã®ç¶™æ‰¿
- analytics_events ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã®çµ±åˆ
- æ—¢å­˜ã®è¡Œå‹•è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ ã®æ‹¡å¼µ
"""

import sqlite3
import json
import logging
import time
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from collections import defaultdict, Counter
from dataclasses import dataclass, field
from enum import Enum

# Task 2.9.2ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from advanced_gemini_analysis_engine import AdvancedGeminiAnalysisEngine, StructuredRecommendation
from recommendation_divergence_detector import EnhancedRecommendationDivergenceDetector, DivergenceEvent
from preference_reason_estimator import PreferenceReasonEstimator, PreferenceProfile

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataQuality(Enum):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒ™ãƒ«"""
    EXCELLENT = "excellent"     # é«˜å“è³ªï¼ˆå®Œå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰
    GOOD = "good"              # è‰¯å“è³ªï¼ˆè»½å¾®ãªæ¬ æï¼‰
    FAIR = "fair"              # æ™®é€šå“è³ªï¼ˆä¸€éƒ¨æ¬ æï¼‰
    POOR = "poor"              # ä½å“è³ªï¼ˆå¤§å¹…æ¬ æï¼‰
    INVALID = "invalid"        # ç„¡åŠ¹ãƒ‡ãƒ¼ã‚¿

class CollectionStatus(Enum):
    """åé›†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
    SUCCESS = "success"         # æˆåŠŸ
    PARTIAL = "partial"         # éƒ¨åˆ†çš„æˆåŠŸ
    FAILED = "failed"          # å¤±æ•—
    SKIPPED = "skipped"        # ã‚¹ã‚­ãƒƒãƒ—

@dataclass
class DataCollectionResult:
    """ãƒ‡ãƒ¼ã‚¿åé›†çµæœ"""
    collection_id: str
    session_id: str
    user_id: Optional[str]
    status: CollectionStatus
    data_quality: DataQuality
    collected_data_types: List[str]
    quality_metrics: Dict[str, float]
    error_messages: List[str]
    collection_timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    metadata: Dict[str, Any] = field(default_factory=dict)

class DataCollectionEnhancement:
    """éæ¥è§¦ãƒ‡ãƒ¼ã‚¿åé›†ã®å¼·åŒ–ï¼ˆTask 2.9.1åŸºç›¤æ´»ç”¨ï¼‰"""
    
    def __init__(self, 
                 analytics_db_path: str = "langpont_analytics.db",
                 divergence_db_path: str = "langpont_divergence.db",
                 preference_db_path: str = "langpont_preferences.db"):
        """åˆæœŸåŒ–"""
        self.analytics_db_path = analytics_db_path
        self.divergence_db_path = divergence_db_path
        self.preference_db_path = preference_db_path
        
        # Task 2.9.2ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆ
        self.analysis_engine = AdvancedGeminiAnalysisEngine()
        self.divergence_detector = EnhancedRecommendationDivergenceDetector(
            analytics_db_path, divergence_db_path
        )
        self.preference_estimator = PreferenceReasonEstimator(
            analytics_db_path, divergence_db_path, preference_db_path
        )
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡åŸºæº–
        self.quality_thresholds = {
            'completeness': 0.9,        # å®Œå…¨æ€§
            'consistency': 0.8,         # ä¸€è²«æ€§
            'accuracy': 0.85,           # æ­£ç¢ºæ€§
            'timeliness': 0.95,         # é©æ™‚æ€§
            'validity': 0.9             # æœ‰åŠ¹æ€§
        }
        
        # åé›†å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–
        self._init_collection_database()
        
        logger.info("ãƒ‡ãƒ¼ã‚¿åé›†å¼·åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def _init_collection_database(self):
        """åé›†å¼·åŒ–ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
        with sqlite3.connect(self.analytics_db_path) as conn:
            cursor = conn.cursor()
            
            # ãƒ‡ãƒ¼ã‚¿åé›†å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS data_collection_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    collection_id VARCHAR(100) UNIQUE NOT NULL,
                    session_id VARCHAR(100) NOT NULL,
                    user_id VARCHAR(100),
                    
                    -- åé›†çµæœ
                    status VARCHAR(20) NOT NULL,
                    data_quality VARCHAR(20) NOT NULL,
                    collected_data_types TEXT,
                    
                    -- å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
                    quality_metrics TEXT,
                    error_messages TEXT,
                    
                    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                    collection_metadata TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # æ¨å¥¨æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆæ–°è¦ï¼‰
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS recommendation_extraction_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id VARCHAR(100) NOT NULL,
                    user_id VARCHAR(100),
                    
                    -- æ¨å¥¨æŠ½å‡ºçµæœ
                    gemini_analysis_text TEXT,
                    extracted_recommendation VARCHAR(50),
                    confidence_score FLOAT,
                    strength_level VARCHAR(20),
                    
                    -- ç†ç”±åˆ†æ
                    primary_reasons TEXT,
                    secondary_reasons TEXT,
                    reasoning_text TEXT,
                    
                    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                    extraction_metadata TEXT,
                    language VARCHAR(10) DEFAULT 'ja',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ç¶™ç¶šè¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆæ–°è¦ï¼‰
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS continuous_behavior_patterns (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id VARCHAR(100) NOT NULL,
                    
                    -- ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿
                    pattern_type VARCHAR(50),
                    pattern_data TEXT,
                    confidence_level VARCHAR(20),
                    
                    -- æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
                    observation_window VARCHAR(20),
                    pattern_evolution TEXT,
                    
                    -- çµ±è¨ˆæƒ…å ±
                    occurrence_frequency FLOAT,
                    pattern_stability FLOAT,
                    
                    -- æ›´æ–°ç®¡ç†
                    first_observed TIMESTAMP,
                    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_collection_session ON data_collection_history (session_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_extraction_session ON recommendation_extraction_data (session_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_behavior_user ON continuous_behavior_patterns (user_id)')
            
            conn.commit()
    
    def save_recommendation_extraction_data(self, 
                                          session_data: Dict,
                                          gemini_analysis_text: str,
                                          structured_recommendation: StructuredRecommendation) -> bool:
        """
        Geminiæ¨å¥¨æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•ä¿å­˜
        
        Args:
            session_data: ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
            gemini_analysis_text: Geminiåˆ†æã®ç”Ÿãƒ†ã‚­ã‚¹ãƒˆ
            structured_recommendation: æ§‹é€ åŒ–æ¨å¥¨ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            ä¿å­˜æˆåŠŸãƒ•ãƒ©ã‚°
        """
        try:
            session_id = session_data.get('session_id')
            user_id = session_data.get('user_id')
            
            if not session_id:
                logger.error("ã‚»ãƒƒã‚·ãƒ§ãƒ³IDãŒå¿…è¦ã§ã™")
                return False
            
            # å“è³ªãƒã‚§ãƒƒã‚¯
            quality_score = self._evaluate_recommendation_data_quality(
                gemini_analysis_text, structured_recommendation
            )
            
            if quality_score < 0.3:  # æœ€ä½å“è³ªé–¾å€¤
                logger.warning(f"æ¨å¥¨æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®å“è³ªãŒä½ã„ãŸã‚ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—: {quality_score:.2f}")
                return False
            
            with sqlite3.connect(self.analytics_db_path) as conn:
                cursor = conn.cursor()
                
                cursor.execute('''
                    INSERT INTO recommendation_extraction_data (
                        session_id, user_id, gemini_analysis_text,
                        extracted_recommendation, confidence_score, strength_level,
                        primary_reasons, secondary_reasons, reasoning_text,
                        extraction_metadata, language
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    session_id,
                    user_id,
                    gemini_analysis_text,
                    structured_recommendation.recommended_engine,
                    structured_recommendation.confidence_score,
                    structured_recommendation.strength_level.value,
                    json.dumps([r.value for r in structured_recommendation.primary_reasons]),
                    json.dumps([r.value for r in structured_recommendation.secondary_reasons]),
                    structured_recommendation.reasoning_text,
                    json.dumps(structured_recommendation.analysis_metadata),
                    structured_recommendation.language
                ))
                
                conn.commit()
            
            # åé›†å±¥æ­´ã®è¨˜éŒ²
            self._record_collection_event(
                session_id, user_id, 
                CollectionStatus.SUCCESS,
                DataQuality.GOOD,
                ['recommendation_extraction'],
                {'quality_score': quality_score}
            )
            
            logger.info(f"æ¨å¥¨æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: session={session_id}, å“è³ª={quality_score:.2f}")
            return True
            
        except Exception as e:
            logger.error(f"æ¨å¥¨æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
            self._record_collection_event(
                session_data.get('session_id', 'unknown'),
                session_data.get('user_id'),
                CollectionStatus.FAILED,
                DataQuality.INVALID,
                ['recommendation_extraction'],
                {'error': str(e)}
            )
            return False
    
    def _evaluate_recommendation_data_quality(self, 
                                            analysis_text: str,
                                            recommendation: StructuredRecommendation) -> float:
        """æ¨å¥¨æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®å“è³ªè©•ä¾¡"""
        quality_factors = []
        
        # 1. åˆ†æãƒ†ã‚­ã‚¹ãƒˆã®å®Œå…¨æ€§
        if analysis_text and len(analysis_text.strip()) > 20:
            quality_factors.append(0.9)
        elif analysis_text and len(analysis_text.strip()) > 10:
            quality_factors.append(0.6)
        else:
            quality_factors.append(0.2)
        
        # 2. æ¨å¥¨ã®ä¿¡é ¼åº¦
        quality_factors.append(recommendation.confidence_score)
        
        # 3. ç†ç”±ã®è©³ç´°åº¦
        reason_count = len(recommendation.primary_reasons) + len(recommendation.secondary_reasons)
        if reason_count >= 3:
            quality_factors.append(0.9)
        elif reason_count >= 1:
            quality_factors.append(0.7)
        else:
            quality_factors.append(0.3)
        
        # 4. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è±Šå¯Œã•
        metadata_richness = len(recommendation.analysis_metadata) / 10.0  # æ­£è¦åŒ–
        quality_factors.append(min(1.0, metadata_richness))
        
        return sum(quality_factors) / len(quality_factors)
    
    def record_divergence_events(self, divergence_data: DivergenceEvent) -> bool:
        """
        ä¹–é›¢ã‚¤ãƒ™ãƒ³ãƒˆã®è©³ç´°è¨˜éŒ²
        
        Args:
            divergence_data: ä¹–é›¢ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
            
        Returns:
            è¨˜éŒ²æˆåŠŸãƒ•ãƒ©ã‚°
        """
        try:
            # ä¹–é›¢ãƒ‡ãƒ¼ã‚¿ã®å“è³ªè©•ä¾¡
            quality_score = self._evaluate_divergence_data_quality(divergence_data)
            
            # å“è³ªã«å¿œã˜ãŸå‡¦ç†
            if quality_score >= 0.8:
                data_quality = DataQuality.EXCELLENT
            elif quality_score >= 0.6:
                data_quality = DataQuality.GOOD
            elif quality_score >= 0.4:
                data_quality = DataQuality.FAIR
            else:
                data_quality = DataQuality.POOR
            
            # åŸºæœ¬è¨˜éŒ²ã¯ divergence_detector ã§å®Ÿè¡Œæ¸ˆã¿ã®ãŸã‚ã€
            # ã“ã“ã§ã¯å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨æ‹¡å¼µæƒ…å ±ã‚’è¨˜éŒ²
            
            extended_metadata = {
                'quality_score': quality_score,
                'data_quality': data_quality.value,
                'learning_priority': self._calculate_learning_priority(divergence_data),
                'contextual_richness': self._evaluate_contextual_richness(divergence_data),
                'behavioral_complexity': self._evaluate_behavioral_complexity(divergence_data)
            }
            
            # æ‹¡å¼µãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’analytics_eventsã«è¨˜éŒ²
            self._save_extended_divergence_metadata(divergence_data, extended_metadata)
            
            # åé›†å±¥æ­´ã®è¨˜éŒ²
            self._record_collection_event(
                divergence_data.session_id,
                divergence_data.user_id,
                CollectionStatus.SUCCESS,
                data_quality,
                ['divergence_event', 'extended_metadata'],
                {'quality_score': quality_score}
            )
            
            logger.info(f"ä¹–é›¢ã‚¤ãƒ™ãƒ³ãƒˆè©³ç´°è¨˜éŒ²å®Œäº†: {divergence_data.event_id}, å“è³ª={quality_score:.2f}")
            return True
            
        except Exception as e:
            logger.error(f"ä¹–é›¢ã‚¤ãƒ™ãƒ³ãƒˆè¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def _evaluate_divergence_data_quality(self, divergence: DivergenceEvent) -> float:
        """ä¹–é›¢ãƒ‡ãƒ¼ã‚¿ã®å“è³ªè©•ä¾¡"""
        quality_factors = []
        
        # 1. åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨æ€§
        completeness_score = 0.0
        if divergence.gemini_recommendation:
            completeness_score += 0.2
        if divergence.user_choice:
            completeness_score += 0.2
        if divergence.satisfaction_score > 0:
            completeness_score += 0.2
        if divergence.context_data:
            completeness_score += 0.2
        if divergence.behavioral_indicators:
            completeness_score += 0.2
        
        quality_factors.append(completeness_score)
        
        # 2. å­¦ç¿’ä¾¡å€¤
        quality_factors.append(divergence.learning_value)
        
        # 3. ä¿¡é ¼åº¦
        quality_factors.append(divergence.gemini_confidence)
        
        # 4. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è±Šå¯Œã•
        context_richness = len(divergence.context_data) / 10.0
        quality_factors.append(min(1.0, context_richness))
        
        return sum(quality_factors) / len(quality_factors)
    
    def _calculate_learning_priority(self, divergence: DivergenceEvent) -> float:
        """å­¦ç¿’å„ªå…ˆåº¦ã®è¨ˆç®—"""
        priority = 0.0
        
        # é‡è¦åº¦ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
        importance_weights = {
            'critical': 1.0,
            'high': 0.8,
            'medium': 0.6,
            'low': 0.4,
            'noise': 0.1
        }
        
        priority += importance_weights.get(divergence.divergence_importance.value, 0.5)
        
        # å­¦ç¿’ä¾¡å€¤ã«ã‚ˆã‚‹èª¿æ•´
        priority = (priority + divergence.learning_value) / 2
        
        return min(1.0, priority)
    
    def _evaluate_contextual_richness(self, divergence: DivergenceEvent) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è±Šå¯Œã•è©•ä¾¡"""
        richness = 0.0
        context = divergence.context_data
        
        # å„è¦ç´ ã®å­˜åœ¨ã«ã‚ˆã‚‹åŠ ç‚¹
        if context.get('text_length', 0) > 100:
            richness += 0.2
        if context.get('has_technical_terms'):
            richness += 0.2
        if context.get('business_context'):
            richness += 0.2
        if context.get('cultural_context'):
            richness += 0.2
        if len(context.keys()) >= 5:
            richness += 0.2
        
        return richness
    
    def _evaluate_behavioral_complexity(self, divergence: DivergenceEvent) -> float:
        """è¡Œå‹•ã®è¤‡é›‘ã•è©•ä¾¡"""
        complexity = 0.0
        behaviors = divergence.behavioral_indicators
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¶™ç¶šæ™‚é–“
        duration = behaviors.get('session_duration', 0)
        if duration >= 180:  # 3åˆ†ä»¥ä¸Š
            complexity += 0.3
        elif duration >= 60:  # 1åˆ†ä»¥ä¸Š
            complexity += 0.2
        
        # ã‚³ãƒ”ãƒ¼è¡Œå‹•ã®å¤šæ§˜æ€§
        copy_behaviors = behaviors.get('recent_copy_behaviors', [])
        if len(copy_behaviors) >= 3:
            complexity += 0.3
        elif len(copy_behaviors) >= 2:
            complexity += 0.2
        
        # è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¨®é¡
        behavior_types = len(behaviors.get('session_behaviors', {}))
        if behavior_types >= 4:
            complexity += 0.4
        elif behavior_types >= 2:
            complexity += 0.2
        
        return min(1.0, complexity)
    
    def _save_extended_divergence_metadata(self, divergence: DivergenceEvent, metadata: Dict):
        """æ‹¡å¼µä¹–é›¢ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
        with sqlite3.connect(self.analytics_db_path) as conn:
            cursor = conn.cursor()
            
            # å°‚ç”¨ã‚¤ãƒ™ãƒ³ãƒˆã¨ã—ã¦è¨˜éŒ²
            event_id = f"divergence_meta_{divergence.event_id}"
            
            cursor.execute('''
                INSERT INTO analytics_events (
                    event_id, event_type, timestamp, session_id,
                    user_id, ip_address, user_agent, custom_data
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                event_id,
                'divergence_metadata',
                int(time.time() * 1000),
                divergence.session_id,
                divergence.user_id,
                '127.0.0.1',  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
                'DataCollectionEnhancement/1.0',
                json.dumps(metadata)
            ))
            
            conn.commit()
    
    def track_continuous_behavior_patterns(self, user_id: str) -> Dict[str, Any]:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¶™ç¶šè¿½è·¡
        
        Args:
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            
        Returns:
            ç¶™ç¶šè¿½è·¡çµæœ
        """
        if not user_id:
            return {'error': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒå¿…è¦ã§ã™'}
        
        try:
            # éå»30æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ
            patterns = self._analyze_user_behavior_patterns(user_id, days=30)
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®‰å®šæ€§ã‚’è©•ä¾¡
            stability_metrics = self._calculate_pattern_stability(user_id, patterns)
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é€²åŒ–ã‚’è¿½è·¡
            evolution_data = self._track_pattern_evolution(user_id, patterns)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            self._save_behavior_patterns(user_id, patterns, stability_metrics, evolution_data)
            
            result = {
                'user_id': user_id,
                'patterns_detected': len(patterns),
                'stability_score': stability_metrics.get('overall_stability', 0.0),
                'evolution_trend': evolution_data.get('trend', 'stable'),
                'tracking_quality': self._evaluate_tracking_quality(patterns),
                'last_updated': datetime.now().isoformat()
            }
            
            logger.info(f"ç¶™ç¶šè¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³è¿½è·¡å®Œäº†: user={user_id}, ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°={len(patterns)}")
            return result
            
        except Exception as e:
            logger.error(f"ç¶™ç¶šè¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³è¿½è·¡ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'error': str(e)}
    
    def _analyze_user_behavior_patterns(self, user_id: str, days: int = 30) -> List[Dict[str, Any]]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        patterns = []
        
        with sqlite3.connect(self.analytics_db_path) as conn:
            cursor = conn.cursor()
            
            # æ™‚ç³»åˆ—ã§ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            cursor.execute('''
                SELECT event_type, custom_data, timestamp,
                       DATE(timestamp/1000, 'unixepoch') as event_date
                FROM analytics_events
                WHERE user_id = ?
                  AND timestamp >= (strftime('%s', 'now', '-{} days') * 1000)
                ORDER BY timestamp
            '''.format(days), (user_id,))
            
            events_by_date = defaultdict(list)
            for event_type, custom_data, timestamp, event_date in cursor.fetchall():
                try:
                    data = json.loads(custom_data) if custom_data else {}
                    events_by_date[event_date].append({
                        'event_type': event_type,
                        'data': data,
                        'timestamp': timestamp
                    })
                except json.JSONDecodeError:
                    continue
            
            # æ—¥åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æŠ½å‡º
            for date, events in events_by_date.items():
                if len(events) >= 3:  # æœ€ä½3ã‚¤ãƒ™ãƒ³ãƒˆå¿…è¦
                    daily_pattern = self._extract_daily_pattern(events)
                    if daily_pattern:
                        daily_pattern['date'] = date
                        patterns.append(daily_pattern)
        
        return patterns
    
    def _extract_daily_pattern(self, events: List[Dict]) -> Optional[Dict[str, Any]]:
        """æ—¥åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æŠ½å‡º"""
        if len(events) < 3:
            return None
        
        # ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã®åˆ†å¸ƒ
        event_types = [e['event_type'] for e in events]
        type_counter = Counter(event_types)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¶™ç¶šæ™‚é–“ã®æ¨å®š
        timestamps = [e['timestamp'] for e in events]
        session_duration = (max(timestamps) - min(timestamps)) / 1000.0
        
        # ç¿»è¨³é–¢é€£ã‚¤ãƒ™ãƒ³ãƒˆã®æŠ½å‡º
        translation_events = [e for e in events if 'translation' in e['event_type']]
        
        pattern = {
            'pattern_type': 'daily_usage',
            'total_events': len(events),
            'event_distribution': dict(type_counter),
            'session_duration': session_duration,
            'translation_activity': len(translation_events),
            'activity_intensity': len(events) / (session_duration / 60) if session_duration > 0 else 0,
            'peak_activity_time': self._identify_peak_activity_time(events)
        }
        
        return pattern
    
    def _identify_peak_activity_time(self, events: List[Dict]) -> str:
        """ãƒ”ãƒ¼ã‚¯æ´»å‹•æ™‚é–“ã®ç‰¹å®š"""
        if not events:
            return 'unknown'
        
        # æ™‚é–“å¸¯åˆ¥ã®ã‚¤ãƒ™ãƒ³ãƒˆæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        hour_counter = Counter()
        for event in events:
            hour = datetime.fromtimestamp(event['timestamp'] / 1000).hour
            hour_counter[hour] += 1
        
        if hour_counter:
            peak_hour = hour_counter.most_common(1)[0][0]
            if 6 <= peak_hour < 12:
                return 'morning'
            elif 12 <= peak_hour < 18:
                return 'afternoon'
            elif 18 <= peak_hour < 22:
                return 'evening'
            else:
                return 'night'
        
        return 'unknown'
    
    def _calculate_pattern_stability(self, user_id: str, patterns: List[Dict]) -> Dict[str, float]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®‰å®šæ€§è¨ˆç®—"""
        if len(patterns) < 3:
            return {'overall_stability': 0.0}
        
        # æ´»å‹•æ™‚é–“ã®å®‰å®šæ€§
        durations = [p.get('session_duration', 0) for p in patterns]
        duration_stability = 1.0 - (statistics.stdev(durations) / statistics.mean(durations)) if durations and statistics.mean(durations) > 0 else 0.0
        duration_stability = max(0.0, min(1.0, duration_stability))
        
        # æ´»å‹•å¼·åº¦ã®å®‰å®šæ€§
        intensities = [p.get('activity_intensity', 0) for p in patterns]
        intensity_stability = 1.0 - (statistics.stdev(intensities) / statistics.mean(intensities)) if intensities and statistics.mean(intensities) > 0 else 0.0
        intensity_stability = max(0.0, min(1.0, intensity_stability))
        
        # æ™‚é–“å¸¯ã®ä¸€è²«æ€§
        peak_times = [p.get('peak_activity_time', 'unknown') for p in patterns]
        peak_time_counter = Counter(peak_times)
        time_consistency = peak_time_counter.most_common(1)[0][1] / len(peak_times) if peak_times else 0.0
        
        overall_stability = (duration_stability + intensity_stability + time_consistency) / 3
        
        return {
            'overall_stability': overall_stability,
            'duration_stability': duration_stability,
            'intensity_stability': intensity_stability,
            'time_consistency': time_consistency
        }
    
    def _track_pattern_evolution(self, user_id: str, patterns: List[Dict]) -> Dict[str, Any]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³é€²åŒ–ã®è¿½è·¡"""
        if len(patterns) < 5:
            return {'trend': 'insufficient_data'}
        
        # æ™‚ç³»åˆ—ã§ã®å¤‰åŒ–ã‚’åˆ†æ
        recent_patterns = patterns[-5:]  # ç›´è¿‘5ãƒ‘ã‚¿ãƒ¼ãƒ³
        older_patterns = patterns[:-5] if len(patterns) > 5 else []
        
        evolution = {
            'trend': 'stable',
            'changes_detected': [],
            'evolution_score': 0.0
        }
        
        if older_patterns:
            # æ´»å‹•é‡ã®å¤‰åŒ–
            recent_activity = statistics.mean([p.get('total_events', 0) for p in recent_patterns])
            older_activity = statistics.mean([p.get('total_events', 0) for p in older_patterns])
            
            if recent_activity > older_activity * 1.2:
                evolution['trend'] = 'increasing'
                evolution['changes_detected'].append('activity_increase')
            elif recent_activity < older_activity * 0.8:
                evolution['trend'] = 'decreasing'
                evolution['changes_detected'].append('activity_decrease')
            
            # å¤‰åŒ–åº¦åˆã„ã®è¨ˆç®—
            activity_change_ratio = abs(recent_activity - older_activity) / max(older_activity, 1)
            evolution['evolution_score'] = min(1.0, activity_change_ratio)
        
        return evolution
    
    def _save_behavior_patterns(self, user_id: str, patterns: List[Dict], 
                               stability: Dict, evolution: Dict):
        """è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¿å­˜"""
        with sqlite3.connect(self.analytics_db_path) as conn:
            cursor = conn.cursor()
            
            # æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‰Šé™¤ï¼ˆæœ€æ–°ãƒ‡ãƒ¼ã‚¿ã§æ›´æ–°ï¼‰
            cursor.execute('''
                DELETE FROM continuous_behavior_patterns
                WHERE user_id = ?
            ''', (user_id,))
            
            # æ–°ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¿å­˜
            for pattern in patterns:
                cursor.execute('''
                    INSERT INTO continuous_behavior_patterns (
                        user_id, pattern_type, pattern_data, confidence_level,
                        observation_window, pattern_evolution,
                        occurrence_frequency, pattern_stability,
                        first_observed
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    user_id,
                    pattern.get('pattern_type', 'unknown'),
                    json.dumps(pattern),
                    'medium',  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
                    '30days',
                    json.dumps(evolution),
                    pattern.get('activity_intensity', 0.0),
                    stability.get('overall_stability', 0.0),
                    pattern.get('date', datetime.now().isoformat())
                ))
            
            conn.commit()
    
    def _evaluate_tracking_quality(self, patterns: List[Dict]) -> str:
        """è¿½è·¡å“è³ªã®è©•ä¾¡"""
        if len(patterns) >= 10:
            return 'excellent'
        elif len(patterns) >= 5:
            return 'good'
        elif len(patterns) >= 2:
            return 'fair'
        else:
            return 'poor'
    
    def _record_collection_event(self, session_id: str, user_id: Optional[str],
                                status: CollectionStatus, quality: DataQuality,
                                data_types: List[str], metrics: Dict):
        """åé›†ã‚¤ãƒ™ãƒ³ãƒˆã®è¨˜éŒ²"""
        collection_id = hashlib.md5(
            f"{session_id}_{user_id}_{int(time.time() * 1000000)}_{len(data_types)}".encode()
        ).hexdigest()[:16]
        
        with sqlite3.connect(self.analytics_db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO data_collection_history (
                    collection_id, session_id, user_id, status, data_quality,
                    collected_data_types, quality_metrics, error_messages,
                    collection_metadata
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                collection_id,
                session_id,
                user_id,
                status.value,
                quality.value,
                json.dumps(data_types),
                json.dumps(metrics),
                json.dumps(metrics.get('errors', [])),
                json.dumps({'collection_timestamp': datetime.now().isoformat()})
            ))
            
            conn.commit()
    
    def get_collection_statistics(self, days: int = 30) -> Dict[str, Any]:
        """åé›†çµ±è¨ˆã®å–å¾—"""
        stats = {
            'collection_summary': {},
            'quality_distribution': {},
            'data_type_distribution': {},
            'error_analysis': {}
        }
        
        with sqlite3.connect(self.analytics_db_path) as conn:
            cursor = conn.cursor()
            
            # åŸºæœ¬çµ±è¨ˆ
            cursor.execute('''
                SELECT status, COUNT(*) as count
                FROM data_collection_history
                WHERE created_at >= datetime('now', '-{} days')
                GROUP BY status
            '''.format(days))
            
            stats['collection_summary'] = dict(cursor.fetchall())
            
            # å“è³ªåˆ†å¸ƒ
            cursor.execute('''
                SELECT data_quality, COUNT(*) as count
                FROM data_collection_history
                WHERE created_at >= datetime('now', '-{} days')
                GROUP BY data_quality
            '''.format(days))
            
            stats['quality_distribution'] = dict(cursor.fetchall())
        
        return stats


# ãƒ†ã‚¹ãƒˆç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°
if __name__ == "__main__":
    print("ğŸ¯ ãƒ‡ãƒ¼ã‚¿åé›†å¼·åŒ–ã‚·ã‚¹ãƒ†ãƒ  - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print("=" * 60)
    
    collector = DataCollectionEnhancement()
    
    # ãƒ†ã‚¹ãƒˆç”¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
    test_session = {
        'session_id': 'test_session_292',
        'user_id': 'test_user_292'
    }
    
    # ãƒ†ã‚¹ãƒˆç”¨æ¨å¥¨ãƒ‡ãƒ¼ã‚¿
    from advanced_gemini_analysis_engine import StructuredRecommendation, RecommendationStrength
    test_recommendation = StructuredRecommendation(
        recommended_engine='enhanced',
        confidence_score=0.85,
        strength_level=RecommendationStrength.STRONG,
        primary_reasons=[],
        secondary_reasons=[],
        reasoning_text='ãƒ†ã‚¹ãƒˆç†ç”±',
        language='ja'
    )
    
    # æ¨å¥¨æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ†ã‚¹ãƒˆ
    success = collector.save_recommendation_extraction_data(
        test_session,
        "ãƒ†ã‚¹ãƒˆç”¨Geminiåˆ†æãƒ†ã‚­ã‚¹ãƒˆ",
        test_recommendation
    )
    
    print(f"âœ… æ¨å¥¨æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ä¿å­˜: {'æˆåŠŸ' if success else 'å¤±æ•—'}")
    
    # ç¶™ç¶šè¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³è¿½è·¡ãƒ†ã‚¹ãƒˆ
    tracking_result = collector.track_continuous_behavior_patterns('test_user_292')
    print(f"âœ… ç¶™ç¶šè¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³è¿½è·¡:")
    print(f"  æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {tracking_result.get('patterns_detected', 0)}")
    print(f"  å®‰å®šæ€§ã‚¹ã‚³ã‚¢: {tracking_result.get('stability_score', 0.0):.3f}")
    
    # åé›†çµ±è¨ˆãƒ†ã‚¹ãƒˆ
    stats = collector.get_collection_statistics(7)
    print(f"âœ… åé›†çµ±è¨ˆ (7æ—¥é–“):")
    print(f"  åé›†ã‚µãƒãƒªãƒ¼: {stats['collection_summary']}")
    print(f"  å“è³ªåˆ†å¸ƒ: {stats['quality_distribution']}")
    
    print("\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº† - ãƒ‡ãƒ¼ã‚¿åé›†å¼·åŒ–ã‚·ã‚¹ãƒ†ãƒ æ­£å¸¸å‹•ä½œ")