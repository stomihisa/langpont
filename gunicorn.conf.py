# -*- coding: utf-8 -*-
"""
Gunicorn Configuration for LangPont
æœ¬ç•ªç’°å¢ƒç”¨Gunicornè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

ä½¿ç”¨æ–¹æ³•:
  gunicorn -c gunicorn.conf.py wsgi:app
"""

import os
import multiprocessing

# ã‚µãƒ¼ãƒãƒ¼è¨­å®š
bind = f"0.0.0.0:{os.getenv('PORT', '8080')}"
backlog = 2048

# ãƒ¯ãƒ¼ã‚«ãƒ¼è¨­å®š
workers = int(os.getenv('GUNICORN_WORKERS', multiprocessing.cpu_count() * 2 + 1))
worker_class = "sync"
worker_connections = 1000
timeout = 120  # OpenAI APIã®é•·ã„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã«å¯¾å¿œ
keepalive = 5
max_requests = 1000
max_requests_jitter = 100

# ãƒ—ãƒ­ã‚»ã‚¹ç®¡ç†
preload_app = True
daemon = False
pidfile = "/tmp/gunicorn.pid"
user = None
group = None
tmp_upload_dir = None

# ãƒ­ã‚°è¨­å®š
loglevel = os.getenv('LOG_LEVEL', 'info')
accesslog = os.getenv('ACCESS_LOG', '-')  # stdout
errorlog = os.getenv('ERROR_LOG', '-')   # stderr
access_log_format = '%h %l %u %t "%r" %s %b "%{Referer}i" "%{User-Agent}i" %D'

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
limit_request_line = 4096
limit_request_fields = 100
limit_request_field_size = 8190

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®š
preload_app = True
worker_tmp_dir = "/dev/shm"  # ãƒ¡ãƒ¢ãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰

# ç’°å¢ƒå¤‰æ•°è¨­å®š
def when_ready(server):
    server.log.info("ğŸš€ LangPont Gunicorn ã‚µãƒ¼ãƒãƒ¼èµ·å‹•å®Œäº†")
    server.log.info(f"ğŸŒ ãƒã‚¤ãƒ³ãƒ‰: {bind}")
    server.log.info(f"ğŸ‘¥ ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {workers}")
    server.log.info(f"â±ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {timeout}ç§’")

def worker_int(worker):
    worker.log.info("ğŸ‘‹ ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒåœæ­¢ã‚·ã‚°ãƒŠãƒ«ã‚’å—ä¿¡ã—ã¾ã—ãŸ")

def on_exit(server):
    server.log.info("ğŸ›‘ LangPont Gunicorn ã‚µãƒ¼ãƒãƒ¼åœæ­¢")

# æœ¬ç•ªç’°å¢ƒã§ã®è¿½åŠ è¨­å®š
if os.getenv('ENVIRONMENT') == 'production':
    # æœ¬ç•ªç’°å¢ƒã§ã¯å°‘ã—å³ã—ã‚ã®è¨­å®š
    timeout = 180
    keepalive = 2
    max_requests = 500
    worker_connections = 500
    
    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–
    limit_request_line = 2048
    limit_request_fields = 50