#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ¯ æˆ¦ç•¥çš„å¼·åŒ–: ç«¶åˆå„ªä½æ€§ãƒ»å‚å…¥éšœå£åˆ†æã‚·ã‚¹ãƒ†ãƒ 
=====================================================
ç›®çš„: LangPontå•†ç”¨åŒ–ã«ãŠã‘ã‚‹ç«¶åˆå„ªä½æ€§ã®å®šé‡çš„è©•ä¾¡
     - åé›†ãƒ‡ãƒ¼ã‚¿ã®ç‹¬è‡ªæ€§ãƒ»å¸Œå°‘æ€§ã‚¹ã‚³ã‚¢ç®—å‡º
     - ç«¶åˆã«ã‚ˆã‚‹æ¨¡å€£å›°é›£åº¦ã®æ¨å®š
     - å‚å…¥éšœå£ã®å¼·åº¦è¨ˆç®—
     - æˆ¦ç•¥çš„ãƒ‡ãƒ¼ã‚¿åé›†ã®æ”¹å–„ç‚¹ç‰¹å®š

ã€æˆ¦ç•¥çš„ä¾¡å€¤ã€‘
- ãƒ‡ãƒ¼ã‚¿ã®æ¨¡å€£å›°é›£åº¦æ¸¬å®šã«ã‚ˆã‚‹ç«¶åˆåˆ†æ
- è“„ç©ãƒ‡ãƒ¼ã‚¿ã®å•†ç”¨ä¾¡å€¤è©•ä¾¡
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŠ¹æœãƒ»å­¦ç¿’åŠ¹æœã®å®šé‡åŒ–
- å‚å…¥éšœå£æ§‹ç¯‰ã®å¼±ç‚¹åˆ†æã¨æ”¹å–„ææ¡ˆ
"""

import sqlite3
import json
import logging
import time
import statistics
import math
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from collections import defaultdict, Counter
from dataclasses import dataclass, field
from enum import Enum
import hashlib

# æˆ¦ç•¥çš„ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from personalization_data_collector import PersonalizationDataCollector, DataCommercialValue

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MoatStrength(Enum):
    """å‚å…¥éšœå£å¼·åº¦ãƒ¬ãƒ™ãƒ«"""
    UNBREACHABLE = "unbreachable"    # çªç ´ä¸å¯èƒ½ï¼ˆ10å¹´ä»¥ä¸Šã®å„ªä½æ€§ï¼‰
    VERY_STRONG = "very_strong"      # éå¸¸ã«å¼·å›ºï¼ˆ5-10å¹´ã®å„ªä½æ€§ï¼‰
    STRONG = "strong"                # å¼·å›ºï¼ˆ3-5å¹´ã®å„ªä½æ€§ï¼‰
    MODERATE = "moderate"            # ä¸­ç¨‹åº¦ï¼ˆ1-3å¹´ã®å„ªä½æ€§ï¼‰
    WEAK = "weak"                    # å¼±ã„ï¼ˆ1å¹´æœªæº€ã®å„ªä½æ€§ï¼‰
    NONE = "none"                    # éšœå£ãªã—

class CompetitiveFactor(Enum):
    """ç«¶åˆè¦å› ã‚¿ã‚¤ãƒ—"""
    DATA_VOLUME = "data_volume"                # ãƒ‡ãƒ¼ã‚¿ãƒœãƒªãƒ¥ãƒ¼ãƒ 
    DATA_QUALITY = "data_quality"              # ãƒ‡ãƒ¼ã‚¿å“è³ª
    COLLECTION_METHOD = "collection_method"    # åé›†æ‰‹æ³•
    USER_BEHAVIOR = "user_behavior"            # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•
    PERSONALIZATION = "personalization"       # å€‹äººåŒ–æŠ€è¡“
    NETWORK_EFFECTS = "network_effects"        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŠ¹æœ
    LEARNING_CURVE = "learning_curve"          # å­¦ç¿’æ›²ç·š
    BRAND_LOYALTY = "brand_loyalty"            # ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ¤ãƒ«ãƒ†ã‚£

class ReplicationDifficulty(Enum):
    """æ¨¡å€£å›°é›£åº¦ãƒ¬ãƒ™ãƒ«"""
    IMPOSSIBLE = "impossible"        # æ¨¡å€£ä¸å¯èƒ½
    EXTREMELY_HARD = "extremely_hard" # æ¥µã‚ã¦å›°é›£
    VERY_HARD = "very_hard"         # éå¸¸ã«å›°é›£
    HARD = "hard"                   # å›°é›£
    MODERATE = "moderate"           # ä¸­ç¨‹åº¦
    EASY = "easy"                   # å®¹æ˜“

@dataclass
class CompetitiveAdvantage:
    """ç«¶åˆå„ªä½æ€§ãƒ‡ãƒ¼ã‚¿"""
    advantage_id: str
    advantage_type: CompetitiveFactor
    strength_score: float  # 0.0-1.0
    moat_contribution: float  # å‚å…¥éšœå£ã¸ã®è²¢çŒ®åº¦
    replication_difficulty: ReplicationDifficulty
    time_to_replicate_months: int  # æ¨¡å€£ã«è¦ã™ã‚‹æœŸé–“ï¼ˆæœˆï¼‰
    cost_to_replicate_usd: float  # æ¨¡å€£ã«è¦ã™ã‚‹ã‚³ã‚¹ãƒˆï¼ˆUSDï¼‰
    strategic_importance: float  # æˆ¦ç•¥çš„é‡è¦åº¦
    evidence_data: Dict[str, Any]
    last_updated: str = field(default_factory=lambda: datetime.now().isoformat())

@dataclass
class MoatAnalysis:
    """å‚å…¥éšœå£åˆ†æçµæœ"""
    overall_moat_strength: MoatStrength
    moat_score: float  # 0.0-1.0
    competitive_advantages: List[CompetitiveAdvantage]
    vulnerability_points: List[str]
    improvement_opportunities: List[str]
    strategic_recommendations: List[str]
    analysis_metadata: Dict[str, Any]

class CompetitiveAdvantageAnalyzer:
    """ç«¶åˆå„ªä½æ€§ãƒ»å‚å…¥éšœå£åˆ†æã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self,
                 analytics_db_path: str = "langpont_analytics.db",
                 personalization_db_path: str = "langpont_personalization.db",
                 competitive_db_path: str = "langpont_competitive.db"):
        """åˆæœŸåŒ–"""
        self.analytics_db_path = analytics_db_path
        self.personalization_db_path = personalization_db_path
        self.competitive_db_path = competitive_db_path
        
        # å€‹äººåŒ–ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ ã®æ´»ç”¨
        self.personalization_collector = PersonalizationDataCollector(
            analytics_db_path=analytics_db_path,
            personalization_db_path=personalization_db_path
        )
        
        # ç«¶åˆåˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.competitive_factors_weights = {
            CompetitiveFactor.DATA_VOLUME: 0.15,
            CompetitiveFactor.DATA_QUALITY: 0.20,
            CompetitiveFactor.COLLECTION_METHOD: 0.15,
            CompetitiveFactor.USER_BEHAVIOR: 0.10,
            CompetitiveFactor.PERSONALIZATION: 0.15,
            CompetitiveFactor.NETWORK_EFFECTS: 0.10,
            CompetitiveFactor.LEARNING_CURVE: 0.10,
            CompetitiveFactor.BRAND_LOYALTY: 0.05
        }
        
        # æ¨¡å€£å›°é›£åº¦è©•ä¾¡åŸºæº–
        self.replication_thresholds = {
            ReplicationDifficulty.IMPOSSIBLE: 0.95,
            ReplicationDifficulty.EXTREMELY_HARD: 0.85,
            ReplicationDifficulty.VERY_HARD: 0.75,
            ReplicationDifficulty.HARD: 0.60,
            ReplicationDifficulty.MODERATE: 0.40,
            ReplicationDifficulty.EASY: 0.20
        }
        
        # ç«¶åˆåˆ†æãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–
        self._init_competitive_database()
        
        logger.info("ç«¶åˆå„ªä½æ€§åˆ†æã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def _init_competitive_database(self):
        """ç«¶åˆåˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
        with sqlite3.connect(self.competitive_db_path) as conn:
            cursor = conn.cursor()
            
            # ç«¶åˆå„ªä½æ€§ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS competitive_advantages (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    advantage_id VARCHAR(100) UNIQUE NOT NULL,
                    advantage_type VARCHAR(50) NOT NULL,
                    
                    -- å¼·åº¦ãƒ»è²¢çŒ®åº¦
                    strength_score FLOAT NOT NULL,
                    moat_contribution FLOAT NOT NULL,
                    replication_difficulty VARCHAR(20) NOT NULL,
                    
                    -- æ¨¡å€£ã‚³ã‚¹ãƒˆãƒ»æœŸé–“
                    time_to_replicate_months INTEGER,
                    cost_to_replicate_usd FLOAT,
                    strategic_importance FLOAT,
                    
                    -- è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿
                    evidence_data TEXT,
                    
                    -- æ™‚é–“ç®¡ç†
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # å‚å…¥éšœå£åˆ†æå±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS moat_analysis_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    analysis_id VARCHAR(100) UNIQUE NOT NULL,
                    
                    -- åˆ†æçµæœ
                    overall_moat_strength VARCHAR(20) NOT NULL,
                    moat_score FLOAT NOT NULL,
                    
                    -- åˆ†æãƒ‡ãƒ¼ã‚¿
                    competitive_advantages_count INTEGER,
                    vulnerability_points_count INTEGER,
                    improvement_opportunities_count INTEGER,
                    
                    -- è©³ç´°ãƒ‡ãƒ¼ã‚¿
                    analysis_details TEXT,
                    strategic_recommendations TEXT,
                    
                    analysis_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ç«¶åˆæ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS competitor_comparison (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    competitor_name VARCHAR(100),
                    comparison_factor VARCHAR(50),
                    
                    -- æ¯”è¼ƒã‚¹ã‚³ã‚¢
                    langpont_score FLOAT,
                    competitor_score FLOAT,
                    advantage_gap FLOAT,
                    
                    -- åˆ†æãƒ‡ãƒ¼ã‚¿
                    comparison_basis TEXT,
                    confidence_level FLOAT,
                    
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ãƒ‡ãƒ¼ã‚¿ç‹¬è‡ªæ€§è©•ä¾¡ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS data_uniqueness_evaluation (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    data_category VARCHAR(50),
                    
                    -- ç‹¬è‡ªæ€§æŒ‡æ¨™
                    uniqueness_score FLOAT,
                    rarity_index FLOAT,
                    commercial_value VARCHAR(20),
                    
                    -- æ¨¡å€£å›°é›£åº¦
                    replication_difficulty_score FLOAT,
                    estimated_replication_time_months INTEGER,
                    estimated_replication_cost_usd FLOAT,
                    
                    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                    evaluation_methodology TEXT,
                    supporting_data TEXT,
                    
                    evaluated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_advantage_type ON competitive_advantages (advantage_type)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_moat_analysis_date ON moat_analysis_history (analysis_timestamp)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_competitor_factor ON competitor_comparison (comparison_factor)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_data_category ON data_uniqueness_evaluation (data_category)')
            
            conn.commit()
    
    def measure_data_uniqueness(self, collected_data: Dict) -> Dict[str, Any]:
        """
        åé›†ãƒ‡ãƒ¼ã‚¿ã®ç‹¬è‡ªæ€§ãƒ»å¸Œå°‘æ€§ã‚¹ã‚³ã‚¢ç®—å‡º
        
        Args:
            collected_data: åé›†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            ãƒ‡ãƒ¼ã‚¿ç‹¬è‡ªæ€§åˆ†æçµæœ
        """
        try:
            uniqueness_analysis = {
                'overall_uniqueness_score': 0.0,
                'category_uniqueness': {},
                'rarity_indicators': {},
                'commercial_value_assessment': {},
                'competitive_differentiation': {}
            }
            
            # 1. å€‹äººåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ç‹¬è‡ªæ€§è©•ä¾¡
            if 'personalization_patterns' in collected_data:
                pattern_uniqueness = self._evaluate_pattern_uniqueness(
                    collected_data['personalization_patterns']
                )
                uniqueness_analysis['category_uniqueness']['personalization_patterns'] = pattern_uniqueness
            
            # 2. ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã®å¸Œå°‘æ€§è©•ä¾¡
            if 'user_behavior_data' in collected_data:
                behavior_rarity = self._evaluate_behavior_data_rarity(
                    collected_data['user_behavior_data']
                )
                uniqueness_analysis['rarity_indicators']['user_behavior'] = behavior_rarity
            
            # 3. è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ç‹¬è‡ªæ€§è©•ä¾¡
            if 'language_patterns' in collected_data:
                language_uniqueness = self._evaluate_language_pattern_uniqueness(
                    collected_data['language_patterns']
                )
                uniqueness_analysis['category_uniqueness']['language_patterns'] = language_uniqueness
            
            # 4. å…¨ä½“çš„ãªç‹¬è‡ªæ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
            uniqueness_analysis['overall_uniqueness_score'] = self._calculate_overall_uniqueness(
                uniqueness_analysis['category_uniqueness']
            )
            
            # 5. å•†ç”¨ä¾¡å€¤è©•ä¾¡
            uniqueness_analysis['commercial_value_assessment'] = self._assess_commercial_value_from_uniqueness(
                uniqueness_analysis
            )
            
            # 6. ç«¶åˆã¨ã®å·®åˆ¥åŒ–è¦ç´ å®šé‡åŒ–
            uniqueness_analysis['competitive_differentiation'] = self._quantify_competitive_differentiation(
                uniqueness_analysis
            )
            
            # 7. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜
            self._save_data_uniqueness_evaluation(uniqueness_analysis)
            
            logger.info(f"ãƒ‡ãƒ¼ã‚¿ç‹¬è‡ªæ€§è©•ä¾¡å®Œäº†: å…¨ä½“ã‚¹ã‚³ã‚¢={uniqueness_analysis['overall_uniqueness_score']:.3f}")
            
            return uniqueness_analysis
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ç‹¬è‡ªæ€§è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'error': str(e)}
    
    def estimate_replication_difficulty(self, pattern_data: Dict) -> Dict[str, Any]:
        """
        ç«¶åˆã«ã‚ˆã‚‹æ¨¡å€£å›°é›£åº¦ã®æ¨å®š
        
        Args:
            pattern_data: ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            æ¨¡å€£å›°é›£åº¦åˆ†æçµæœ
        """
        try:
            replication_analysis = {
                'overall_difficulty': ReplicationDifficulty.MODERATE,
                'difficulty_score': 0.0,
                'factor_analysis': {},
                'time_estimates': {},
                'cost_estimates': {},
                'risk_assessment': {}
            }
            
            # 1. ãƒ‡ãƒ¼ã‚¿åé›†æœŸé–“ã®å„ªä½æ€§è©•ä¾¡
            collection_advantage = self._evaluate_data_collection_time_advantage(pattern_data)
            replication_analysis['factor_analysis']['collection_time'] = collection_advantage
            
            # 2. éæ¥è§¦åé›†æ‰‹æ³•ã®æŠ€è¡“çš„éšœå£è©•ä¾¡
            technical_barrier = self._evaluate_technical_collection_barriers(pattern_data)
            replication_analysis['factor_analysis']['technical_barriers'] = technical_barrier
            
            # 3. ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¤‡é›‘æ€§è©•ä¾¡
            complexity_barrier = self._evaluate_user_behavior_complexity(pattern_data)
            replication_analysis['factor_analysis']['behavior_complexity'] = complexity_barrier
            
            # 4. å­¦ç¿’åŠ¹æœã«ã‚ˆã‚‹éšœå£è©•ä¾¡
            learning_barrier = self._evaluate_learning_effect_barriers(pattern_data)
            replication_analysis['factor_analysis']['learning_effects'] = learning_barrier
            
            # 5. å…¨ä½“çš„ãªå›°é›£åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
            replication_analysis['difficulty_score'] = self._calculate_replication_difficulty_score(
                replication_analysis['factor_analysis']
            )
            
            # 6. å›°é›£åº¦ãƒ¬ãƒ™ãƒ«ã®æ±ºå®š
            replication_analysis['overall_difficulty'] = self._determine_replication_difficulty_level(
                replication_analysis['difficulty_score']
            )
            
            # 7. æ¨¡å€£æ™‚é–“ãƒ»ã‚³ã‚¹ãƒˆæ¨å®š
            replication_analysis['time_estimates'] = self._estimate_replication_time(
                replication_analysis['factor_analysis']
            )
            replication_analysis['cost_estimates'] = self._estimate_replication_costs(
                replication_analysis['factor_analysis']
            )
            
            # 8. ãƒªã‚¹ã‚¯è©•ä¾¡
            replication_analysis['risk_assessment'] = self._assess_replication_risks(
                replication_analysis
            )
            
            logger.info(f"æ¨¡å€£å›°é›£åº¦æ¨å®šå®Œäº†: ãƒ¬ãƒ™ãƒ«={replication_analysis['overall_difficulty'].value}, "
                       f"ã‚¹ã‚³ã‚¢={replication_analysis['difficulty_score']:.3f}")
            
            return replication_analysis
            
        except Exception as e:
            logger.error(f"æ¨¡å€£å›°é›£åº¦æ¨å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'error': str(e)}
    
    def calculate_moat_strength(self, user_base_data: Dict) -> float:
        """
        å‚å…¥éšœå£ã®å¼·åº¦è¨ˆç®—
        
        Args:
            user_base_data: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            å‚å…¥éšœå£å¼·åº¦ã‚¹ã‚³ã‚¢ (0.0-1.0)
        """
        try:
            moat_components = {}
            
            # 1. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŠ¹æœã®æ¸¬å®š
            network_effect = self._measure_network_effects(user_base_data)
            moat_components['network_effects'] = network_effect
            
            # 2. ãƒ‡ãƒ¼ã‚¿è“„ç©ã«ã‚ˆã‚‹å­¦ç¿’åŠ¹æœæ¸¬å®š
            learning_effect = self._measure_data_learning_effects(user_base_data)
            moat_components['learning_effects'] = learning_effect
            
            # 3. å€‹äººåŒ–ç²¾åº¦ã®ç«¶åˆå„ªä½æ€§æ¸¬å®š
            personalization_advantage = self._measure_personalization_advantage(user_base_data)
            moat_components['personalization_advantage'] = personalization_advantage
            
            # 4. ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ‡ã‚Šæ›¿ãˆã‚³ã‚¹ãƒˆæ¸¬å®š
            switching_cost = self._measure_user_switching_costs(user_base_data)
            moat_components['switching_costs'] = switching_cost
            
            # 5. ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ¤ãƒ«ãƒ†ã‚£æ¸¬å®š
            brand_loyalty = self._measure_brand_loyalty(user_base_data)
            moat_components['brand_loyalty'] = brand_loyalty
            
            # 6. é‡ã¿ä»˜ãç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
            moat_strength = self._calculate_weighted_moat_strength(moat_components)
            
            logger.info(f"å‚å…¥éšœå£å¼·åº¦è¨ˆç®—å®Œäº†: {moat_strength:.3f}")
            
            return moat_strength
            
        except Exception as e:
            logger.error(f"å‚å…¥éšœå£å¼·åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return 0.0
    
    def identify_strategic_data_gaps(self, current_data: Dict) -> List[str]:
        """
        æˆ¦ç•¥çš„ãƒ‡ãƒ¼ã‚¿åé›†ã®æ”¹å–„ç‚¹ç‰¹å®š
        
        Args:
            current_data: ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            æ”¹å–„ç‚¹ãƒªã‚¹ãƒˆ
        """
        try:
            strategic_gaps = []
            
            # 1. ç«¶åˆå„ªä½æ€§å¼·åŒ–ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã®ç‰¹å®š
            competitive_gaps = self._identify_competitive_data_gaps(current_data)
            strategic_gaps.extend(competitive_gaps)
            
            # 2. å€‹äººåŒ–ç²¾åº¦å‘ä¸Šã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
            personalization_gaps = self._identify_personalization_bottlenecks(current_data)
            strategic_gaps.extend(personalization_gaps)
            
            # 3. å‚å…¥éšœå£æ§‹ç¯‰ã®å¼±ç‚¹åˆ†æ
            moat_gaps = self._identify_moat_building_weaknesses(current_data)
            strategic_gaps.extend(moat_gaps)
            
            # 4. ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Šã®æ©Ÿä¼šç‰¹å®š
            quality_gaps = self._identify_data_quality_opportunities(current_data)
            strategic_gaps.extend(quality_gaps)
            
            # 5. é‡è¤‡æ’é™¤ã¨å„ªå…ˆåº¦ä»˜ã‘
            strategic_gaps = self._prioritize_strategic_gaps(strategic_gaps)
            
            logger.info(f"æˆ¦ç•¥çš„ãƒ‡ãƒ¼ã‚¿ã‚®ãƒ£ãƒƒãƒ—ç‰¹å®šå®Œäº†: {len(strategic_gaps)}ä»¶ã®æ”¹å–„ç‚¹")
            
            return strategic_gaps
            
        except Exception as e:
            logger.error(f"æˆ¦ç•¥çš„ãƒ‡ãƒ¼ã‚¿ã‚®ãƒ£ãƒƒãƒ—ç‰¹å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def generate_comprehensive_moat_analysis(self, 
                                           collected_data: Dict,
                                           user_base_data: Dict) -> MoatAnalysis:
        """
        åŒ…æ‹¬çš„å‚å…¥éšœå£åˆ†æã®å®Ÿè¡Œ
        
        Args:
            collected_data: åé›†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
            user_base_data: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            åŒ…æ‹¬çš„å‚å…¥éšœå£åˆ†æçµæœ
        """
        try:
            # 1. ãƒ‡ãƒ¼ã‚¿ç‹¬è‡ªæ€§è©•ä¾¡
            uniqueness_analysis = self.measure_data_uniqueness(collected_data)
            
            # 2. æ¨¡å€£å›°é›£åº¦æ¨å®š
            replication_analysis = self.estimate_replication_difficulty(collected_data)
            
            # 3. å‚å…¥éšœå£å¼·åº¦è¨ˆç®—
            moat_strength_score = self.calculate_moat_strength(user_base_data)
            
            # 4. æˆ¦ç•¥çš„ã‚®ãƒ£ãƒƒãƒ—ç‰¹å®š
            strategic_gaps = self.identify_strategic_data_gaps(collected_data)
            
            # 5. ç«¶åˆå„ªä½æ€§è¦ç´ ã®ç‰¹å®š
            competitive_advantages = self._identify_competitive_advantages(
                uniqueness_analysis, replication_analysis, moat_strength_score
            )
            
            # 6. è„†å¼±æ€§ãƒã‚¤ãƒ³ãƒˆã®ç‰¹å®š
            vulnerability_points = self._identify_vulnerability_points(
                uniqueness_analysis, replication_analysis, strategic_gaps
            )
            
            # 7. æ”¹å–„æ©Ÿä¼šã®ç‰¹å®š
            improvement_opportunities = self._identify_improvement_opportunities(strategic_gaps)
            
            # 8. æˆ¦ç•¥çš„æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
            strategic_recommendations = self._generate_strategic_recommendations(
                competitive_advantages, vulnerability_points, improvement_opportunities
            )
            
            # 9. å…¨ä½“çš„ãªå‚å…¥éšœå£å¼·åº¦ã®æ±ºå®š
            overall_moat_strength = self._determine_overall_moat_strength(moat_strength_score)
            
            # 10. åŒ…æ‹¬çš„åˆ†æçµæœã®æ§‹ç¯‰
            moat_analysis = MoatAnalysis(
                overall_moat_strength=overall_moat_strength,
                moat_score=moat_strength_score,
                competitive_advantages=competitive_advantages,
                vulnerability_points=vulnerability_points,
                improvement_opportunities=improvement_opportunities,
                strategic_recommendations=strategic_recommendations,
                analysis_metadata={
                    'analysis_timestamp': datetime.now().isoformat(),
                    'data_volume': len(collected_data),
                    'user_base_size': user_base_data.get('total_users', 0),
                    'analysis_version': '1.0'
                }
            )
            
            # 11. åˆ†æçµæœã®ä¿å­˜
            self._save_moat_analysis(moat_analysis)
            
            logger.info(f"åŒ…æ‹¬çš„å‚å…¥éšœå£åˆ†æå®Œäº†: å¼·åº¦={overall_moat_strength.value}, "
                       f"ã‚¹ã‚³ã‚¢={moat_strength_score:.3f}")
            
            return moat_analysis
            
        except Exception as e:
            logger.error(f"åŒ…æ‹¬çš„å‚å…¥éšœå£åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return MoatAnalysis(
                overall_moat_strength=MoatStrength.WEAK,
                moat_score=0.0,
                competitive_advantages=[],
                vulnerability_points=[f"Analysis error: {str(e)}"],
                improvement_opportunities=[],
                strategic_recommendations=[],
                analysis_metadata={'error': str(e)}
            )
    
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤ï¼ˆç°¡ç•¥åŒ–å®Ÿè£…ï¼‰
    def _evaluate_pattern_uniqueness(self, patterns: List) -> Dict[str, float]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ç‹¬è‡ªæ€§ã®è©•ä¾¡"""
        return {
            'uniqueness_score': 0.8,
            'pattern_diversity': 0.7,
            'commercial_value': 0.9
        }
    
    def _evaluate_behavior_data_rarity(self, behavior_data: Dict) -> Dict[str, float]:
        """è¡Œå‹•ãƒ‡ãƒ¼ã‚¿å¸Œå°‘æ€§ã®è©•ä¾¡"""
        return {
            'rarity_index': 0.75,
            'collection_difficulty': 0.85,
            'market_availability': 0.3
        }
    
    def _evaluate_language_pattern_uniqueness(self, language_patterns: Dict) -> Dict[str, float]:
        """è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³ç‹¬è‡ªæ€§ã®è©•ä¾¡"""
        return {
            'linguistic_uniqueness': 0.85,
            'cultural_specificity': 0.8,
            'replication_complexity': 0.9
        }
    
    def _calculate_overall_uniqueness(self, category_uniqueness: Dict) -> float:
        """å…¨ä½“ç‹¬è‡ªæ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if not category_uniqueness:
            return 0.0
        
        scores = []
        for category, metrics in category_uniqueness.items():
            if isinstance(metrics, dict) and 'uniqueness_score' in metrics:
                scores.append(metrics['uniqueness_score'])
        
        return statistics.mean(scores) if scores else 0.0
    
    def _assess_commercial_value_from_uniqueness(self, uniqueness_data: Dict) -> Dict[str, Any]:
        """ç‹¬è‡ªæ€§ã‹ã‚‰å•†ç”¨ä¾¡å€¤ã‚’è©•ä¾¡"""
        overall_score = uniqueness_data.get('overall_uniqueness_score', 0)
        
        if overall_score >= 0.8:
            value_level = DataCommercialValue.EXTREMELY_HIGH
        elif overall_score >= 0.6:
            value_level = DataCommercialValue.HIGH
        elif overall_score >= 0.4:
            value_level = DataCommercialValue.MEDIUM
        else:
            value_level = DataCommercialValue.LOW
        
        return {
            'commercial_value_level': value_level,
            'estimated_market_value_usd': overall_score * 1000000,  # ä»®æƒ³çš„ãªè©•ä¾¡
            'competitive_advantage_duration_months': int(overall_score * 60)
        }
    
    def _quantify_competitive_differentiation(self, uniqueness_data: Dict) -> Dict[str, float]:
        """ç«¶åˆå·®åˆ¥åŒ–è¦ç´ ã®å®šé‡åŒ–"""
        return {
            'differentiation_strength': uniqueness_data.get('overall_uniqueness_score', 0) * 0.9,
            'market_position_advantage': 0.8,
            'brand_differentiation_potential': 0.75
        }
    
    # ãã®ä»–ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ã‚‚åŒæ§˜ã«ç°¡ç•¥å®Ÿè£…
    def _evaluate_data_collection_time_advantage(self, data: Dict) -> Dict[str, float]:
        return {'time_advantage_months': 24, 'first_mover_benefit': 0.8}
    
    def _evaluate_technical_collection_barriers(self, data: Dict) -> Dict[str, float]:
        return {'technical_complexity': 0.85, 'implementation_difficulty': 0.9}
    
    def _evaluate_user_behavior_complexity(self, data: Dict) -> Dict[str, float]:
        return {'behavior_complexity_score': 0.8, 'pattern_sophistication': 0.85}
    
    def _evaluate_learning_effect_barriers(self, data: Dict) -> Dict[str, float]:
        return {'learning_curve_steepness': 0.7, 'experience_advantage': 0.8}
    
    def _calculate_replication_difficulty_score(self, factors: Dict) -> float:
        """æ¨¡å€£å›°é›£åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        scores = []
        for factor_data in factors.values():
            if isinstance(factor_data, dict):
                for value in factor_data.values():
                    if isinstance(value, (int, float)):
                        scores.append(float(value))
        
        return statistics.mean(scores) if scores else 0.5
    
    def _determine_replication_difficulty_level(self, score: float) -> ReplicationDifficulty:
        """æ¨¡å€£å›°é›£åº¦ãƒ¬ãƒ™ãƒ«æ±ºå®š"""
        for difficulty, threshold in self.replication_thresholds.items():
            if score >= threshold:
                return difficulty
        return ReplicationDifficulty.EASY
    
    def _estimate_replication_time(self, factors: Dict) -> Dict[str, int]:
        """æ¨¡å€£æ™‚é–“æ¨å®š"""
        return {
            'minimum_months': 12,
            'likely_months': 24,
            'maximum_months': 48
        }
    
    def _estimate_replication_costs(self, factors: Dict) -> Dict[str, float]:
        """æ¨¡å€£ã‚³ã‚¹ãƒˆæ¨å®š"""
        return {
            'development_cost_usd': 500000,
            'data_collection_cost_usd': 1000000,
            'opportunity_cost_usd': 2000000
        }
    
    def _assess_replication_risks(self, analysis: Dict) -> Dict[str, float]:
        """æ¨¡å€£ãƒªã‚¹ã‚¯è©•ä¾¡"""
        return {
            'technical_risk': 0.7,
            'market_risk': 0.6,
            'competitive_response_risk': 0.8
        }
    
    def _measure_network_effects(self, user_data: Dict) -> float:
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŠ¹æœæ¸¬å®š"""
        return 0.6
    
    def _measure_data_learning_effects(self, user_data: Dict) -> float:
        """ãƒ‡ãƒ¼ã‚¿å­¦ç¿’åŠ¹æœæ¸¬å®š"""
        return 0.75
    
    def _measure_personalization_advantage(self, user_data: Dict) -> float:
        """å€‹äººåŒ–å„ªä½æ€§æ¸¬å®š"""
        return 0.8
    
    def _measure_user_switching_costs(self, user_data: Dict) -> float:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ‡ã‚Šæ›¿ãˆã‚³ã‚¹ãƒˆæ¸¬å®š"""
        return 0.5
    
    def _measure_brand_loyalty(self, user_data: Dict) -> float:
        """ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ¤ãƒ«ãƒ†ã‚£æ¸¬å®š"""
        return 0.4
    
    def _calculate_weighted_moat_strength(self, components: Dict) -> float:
        """é‡ã¿ä»˜ãå‚å…¥éšœå£å¼·åº¦è¨ˆç®—"""
        total_score = 0.0
        total_weight = 0.0
        
        weights = {
            'network_effects': 0.25,
            'learning_effects': 0.25,
            'personalization_advantage': 0.25,
            'switching_costs': 0.15,
            'brand_loyalty': 0.10
        }
        
        for component, score in components.items():
            if component in weights:
                total_score += score * weights[component]
                total_weight += weights[component]
        
        return total_score / total_weight if total_weight > 0 else 0.0
    
    def _identify_competitive_data_gaps(self, data: Dict) -> List[str]:
        """ç«¶åˆãƒ‡ãƒ¼ã‚¿ã‚®ãƒ£ãƒƒãƒ—ç‰¹å®š"""
        return [
            "éœ€è¦å¯æ›´å¤šè¨€èªè¦†è“‹çš„ç¿»è­¯æ•¸æ“š",
            "éœ€è¦è¡Œæ¥­ç‰¹å®šç¿»è­¯æ¨¡å¼æ•¸æ“š",
            "éœ€è¦å¯¦æ™‚ç”¨æˆ¶åé¥‹æ•¸æ“š"
        ]
    
    def _identify_personalization_bottlenecks(self, data: Dict) -> List[str]:
        """å€‹äººåŒ–ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š"""
        return [
            "ç”¨æˆ¶è¡Œç‚ºæ­·å²æ•¸æ“šä¸è¶³",
            "ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›æœ‰é™",
            "å€‹äººåŒ–å­¸ç¿’é€±æœŸéé•·"
        ]
    
    def _identify_moat_building_weaknesses(self, data: Dict) -> List[str]:
        """å‚å…¥éšœå£æ§‹ç¯‰å¼±ç‚¹ç‰¹å®š"""
        return [
            "ç”¨æˆ¶åˆ‡æ›æˆæœ¬è¼ƒä½",
            "ç¶²çµ¡æ•ˆæ‡‰å°šæœªå……åˆ†ç™¼æ®",
            "å“ç‰Œå¿ èª åº¦éœ€è¦æå‡"
        ]
    
    def _identify_data_quality_opportunities(self, data: Dict) -> List[str]:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Šæ©Ÿä¼šç‰¹å®š"""
        return [
            "æ•¸æ“šæ”¶é›†è¦†è“‹ç‡å¯æå‡",
            "æ•¸æ“šæ¨™è¨»è³ªé‡éœ€æ”¹é€²",
            "æ•¸æ“šé©—è­‰æµç¨‹éœ€å®Œå–„"
        ]
    
    def _prioritize_strategic_gaps(self, gaps: List[str]) -> List[str]:
        """æˆ¦ç•¥çš„ã‚®ãƒ£ãƒƒãƒ—å„ªå…ˆåº¦ä»˜ã‘"""
        return gaps[:5]  # ä¸Šä½5ã¤ã‚’è¿”ã™
    
    def _save_data_uniqueness_evaluation(self, analysis: Dict):
        """ãƒ‡ãƒ¼ã‚¿ç‹¬è‡ªæ€§è©•ä¾¡ã®ä¿å­˜"""
        pass
    
    def _save_moat_analysis(self, analysis: MoatAnalysis):
        """å‚å…¥éšœå£åˆ†æã®ä¿å­˜"""
        pass
    
    def _identify_competitive_advantages(self, uniqueness: Dict, replication: Dict, moat_score: float) -> List[CompetitiveAdvantage]:
        """ç«¶åˆå„ªä½æ€§ç‰¹å®š"""
        return []
    
    def _identify_vulnerability_points(self, uniqueness: Dict, replication: Dict, gaps: List) -> List[str]:
        """è„†å¼±æ€§ãƒã‚¤ãƒ³ãƒˆç‰¹å®š"""
        return ["ãƒ‡ãƒ¼ã‚¿åé›†æ‰‹æ³•ã®æ¨¡å€£å¯èƒ½æ€§", "æŠ€è¡“äººæã®ä¸è¶³"]
    
    def _identify_improvement_opportunities(self, gaps: List) -> List[str]:
        """æ”¹å–„æ©Ÿä¼šç‰¹å®š"""
        return ["AI model fine-tuning acceleration", "Multi-language expansion"]
    
    def _generate_strategic_recommendations(self, advantages: List, vulnerabilities: List, opportunities: List) -> List[str]:
        """æˆ¦ç•¥çš„æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        return [
            "å€‹äººåŒ–ãƒ‡ãƒ¼ã‚¿åé›†ã®åŠ é€Ÿ",
            "å¤šè¨€èªå¯¾å¿œã®æ‹¡å¤§",
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ‡ã‚Šæ›¿ãˆã‚³ã‚¹ãƒˆå‘ä¸Šç­–ã®å®Ÿæ–½"
        ]
    
    def _determine_overall_moat_strength(self, score: float) -> MoatStrength:
        """å…¨ä½“å‚å…¥éšœå£å¼·åº¦æ±ºå®š"""
        if score >= 0.9:
            return MoatStrength.UNBREACHABLE
        elif score >= 0.8:
            return MoatStrength.VERY_STRONG
        elif score >= 0.6:
            return MoatStrength.STRONG
        elif score >= 0.4:
            return MoatStrength.MODERATE
        elif score >= 0.2:
            return MoatStrength.WEAK
        else:
            return MoatStrength.NONE


# ãƒ†ã‚¹ãƒˆç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°
if __name__ == "__main__":
    print("ğŸ¯ ç«¶åˆå„ªä½æ€§ãƒ»å‚å…¥éšœå£åˆ†æã‚·ã‚¹ãƒ†ãƒ  - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print("=" * 60)
    
    analyzer = CompetitiveAdvantageAnalyzer()
    
    # ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿
    test_collected_data = {
        'personalization_patterns': [
            {'pattern_type': 'thinking_to_language', 'uniqueness': 0.8},
            {'pattern_type': 'cultural_adaptation', 'uniqueness': 0.9}
        ],
        'user_behavior_data': {
            'total_sessions': 1000,
            'unique_patterns': 150
        },
        'language_patterns': {
            'unique_expressions': 200,
            'cultural_specificity': 0.85
        }
    }
    
    test_user_base_data = {
        'total_users': 500,
        'active_users': 350,
        'retention_rate': 0.75
    }
    
    # 1. ãƒ‡ãƒ¼ã‚¿ç‹¬è‡ªæ€§è©•ä¾¡ãƒ†ã‚¹ãƒˆ
    uniqueness_result = analyzer.measure_data_uniqueness(test_collected_data)
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ç‹¬è‡ªæ€§è©•ä¾¡:")
    print(f"  å…¨ä½“ç‹¬è‡ªæ€§ã‚¹ã‚³ã‚¢: {uniqueness_result.get('overall_uniqueness_score', 0):.3f}")
    print(f"  ã‚«ãƒ†ã‚´ãƒªæ•°: {len(uniqueness_result.get('category_uniqueness', {}))}")
    
    # 2. æ¨¡å€£å›°é›£åº¦æ¨å®šãƒ†ã‚¹ãƒˆ
    replication_result = analyzer.estimate_replication_difficulty(test_collected_data)
    print(f"\nğŸ”’ æ¨¡å€£å›°é›£åº¦æ¨å®š:")
    print(f"  å›°é›£åº¦ãƒ¬ãƒ™ãƒ«: {replication_result.get('overall_difficulty', 'unknown')}")
    print(f"  å›°é›£åº¦ã‚¹ã‚³ã‚¢: {replication_result.get('difficulty_score', 0):.3f}")
    
    # 3. å‚å…¥éšœå£å¼·åº¦è¨ˆç®—ãƒ†ã‚¹ãƒˆ
    moat_strength = analyzer.calculate_moat_strength(test_user_base_data)
    print(f"\nğŸ° å‚å…¥éšœå£å¼·åº¦è¨ˆç®—:")
    print(f"  å¼·åº¦ã‚¹ã‚³ã‚¢: {moat_strength:.3f}")
    
    # 4. æˆ¦ç•¥çš„ã‚®ãƒ£ãƒƒãƒ—ç‰¹å®šãƒ†ã‚¹ãƒˆ
    strategic_gaps = analyzer.identify_strategic_data_gaps(test_collected_data)
    print(f"\nğŸ“‹ æˆ¦ç•¥çš„ã‚®ãƒ£ãƒƒãƒ—ç‰¹å®š:")
    print(f"  ç‰¹å®šã•ã‚ŒãŸã‚®ãƒ£ãƒƒãƒ—æ•°: {len(strategic_gaps)}")
    for i, gap in enumerate(strategic_gaps[:3], 1):
        print(f"    {i}. {gap}")
    
    # 5. åŒ…æ‹¬çš„åˆ†æãƒ†ã‚¹ãƒˆ
    comprehensive_analysis = analyzer.generate_comprehensive_moat_analysis(
        test_collected_data, test_user_base_data
    )
    print(f"\nğŸ¯ åŒ…æ‹¬çš„å‚å…¥éšœå£åˆ†æ:")
    print(f"  å…¨ä½“çš„ãªå‚å…¥éšœå£å¼·åº¦: {comprehensive_analysis.overall_moat_strength.value}")
    print(f"  å‚å…¥éšœå£ã‚¹ã‚³ã‚¢: {comprehensive_analysis.moat_score:.3f}")
    print(f"  è„†å¼±æ€§ãƒã‚¤ãƒ³ãƒˆæ•°: {len(comprehensive_analysis.vulnerability_points)}")
    print(f"  æ”¹å–„æ©Ÿä¼šæ•°: {len(comprehensive_analysis.improvement_opportunities)}")
    print(f"  æˆ¦ç•¥çš„æ¨å¥¨äº‹é …æ•°: {len(comprehensive_analysis.strategic_recommendations)}")
    
    print("\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº† - ç«¶åˆå„ªä½æ€§åˆ†æã‚·ã‚¹ãƒ†ãƒ æ­£å¸¸å‹•ä½œ")