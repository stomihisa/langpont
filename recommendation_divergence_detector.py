#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ¯ Task 2.9.2: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ä¹–é›¢æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ å¼·åŒ–
=====================================================
ç›®çš„: Task 2.9.1.5ã§ç¢ºç«‹ã—ãŸåŒ…æ‹¬çš„è¡Œå‹•è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ ã‚’åŸºç›¤ã¨ã—ã¦ã€
     Geminiæ¨å¥¨vså®Ÿé¸æŠã®ä¹–é›¢ã‚’é«˜ç²¾åº¦ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œçŸ¥ã—ã€
     å€‹äººåŒ–ç¿»è¨³AIã®ãŸã‚ã®è²´é‡ãªãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã™ã‚‹

ã€Task 2.9.1.5åŸºç›¤æ´»ç”¨ã€‘
- GeminiRecommendationAnalyzerã®æ‹¡å¼µ
- EnhancedSatisfactionEstimatorã¨ã®é€£æº
- éæ¥è§¦ãƒ‡ãƒ¼ã‚¿åé›†åŸå‰‡ã®ç¶™æ‰¿
"""

import sqlite3
import json
import logging
import time
import statistics
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from collections import defaultdict, Counter
from dataclasses import dataclass, field
from enum import Enum
import hashlib

# Task 2.9.1.5åŸºç›¤ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from gemini_recommendation_analyzer import GeminiRecommendationAnalyzer
from enhanced_satisfaction_estimator import EnhancedSatisfactionEstimator
from advanced_gemini_analysis_engine import AdvancedGeminiAnalysisEngine, StructuredRecommendation

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DivergenceImportance(Enum):
    """ä¹–é›¢é‡è¦åº¦ãƒ¬ãƒ™ãƒ«"""
    CRITICAL = "critical"       # æ¥µã‚ã¦é‡è¦ï¼ˆå­¦ç¿’ä¾¡å€¤ãŒéå¸¸ã«é«˜ã„ï¼‰
    HIGH = "high"              # é«˜é‡è¦åº¦ï¼ˆå€‹äººåŒ–ã«å¤§ããå½±éŸ¿ï¼‰
    MEDIUM = "medium"          # ä¸­é‡è¦åº¦ï¼ˆä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    LOW = "low"               # ä½é‡è¦åº¦ï¼ˆå¶ç™ºçš„ãªé¸æŠï¼‰
    NOISE = "noise"           # ãƒã‚¤ã‚ºï¼ˆåˆ†æå¯¾è±¡å¤–ï¼‰

class DivergenceCategory(Enum):
    """ä¹–é›¢ã‚«ãƒ†ã‚´ãƒª"""
    STYLE_PREFERENCE = "style_preference"           # ã‚¹ã‚¿ã‚¤ãƒ«é¸å¥½
    ACCURACY_PRIORITY = "accuracy_priority"         # ç²¾åº¦é‡è¦–
    FORMALITY_CHOICE = "formality_choice"          # ä¸å¯§åº¦é¸æŠ
    CULTURAL_ADAPTATION = "cultural_adaptation"     # æ–‡åŒ–çš„é©å¿œ
    DOMAIN_EXPERTISE = "domain_expertise"          # å°‚é–€åˆ†é‡çŸ¥è­˜
    PERSONAL_HABIT = "personal_habit"              # å€‹äººçš„ç¿’æ…£
    CONTEXT_SPECIFIC = "context_specific"          # æ–‡è„ˆç‰¹åŒ–
    EXPERIMENTAL = "experimental"                  # å®Ÿé¨“çš„é¸æŠ

@dataclass
class DivergenceEvent:
    """ä¹–é›¢ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿"""
    event_id: str
    session_id: str
    user_id: Optional[str]
    gemini_recommendation: str
    user_choice: str
    gemini_confidence: float
    divergence_importance: DivergenceImportance
    divergence_category: DivergenceCategory
    satisfaction_score: float
    context_data: Dict[str, Any]
    behavioral_indicators: Dict[str, Any]
    learning_value: float
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

@dataclass
class DivergenceTrend:
    """ä¹–é›¢ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿"""
    time_period: str
    total_divergences: int
    divergence_rate: float
    category_distribution: Dict[DivergenceCategory, int]
    importance_distribution: Dict[DivergenceImportance, int]
    learning_value_score: float
    user_patterns: Dict[str, Any]

class EnhancedRecommendationDivergenceDetector:
    """Task 2.9.1.5åŸºç›¤ã‚’æ´»ç”¨ã—ãŸé«˜åº¦ä¹–é›¢æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, 
                 analytics_db_path: str = "langpont_analytics.db",
                 divergence_db_path: str = "langpont_divergence.db"):
        """åˆæœŸåŒ–"""
        self.analytics_db_path = analytics_db_path
        self.divergence_db_path = divergence_db_path
        
        # Task 2.9.1.5åŸºç›¤ã‚·ã‚¹ãƒ†ãƒ ã®æ´»ç”¨
        self.base_analyzer = GeminiRecommendationAnalyzer()
        self.satisfaction_estimator = EnhancedSatisfactionEstimator(analytics_db_path)
        self.advanced_analyzer = AdvancedGeminiAnalysisEngine()
        
        # ä¹–é›¢æ¤œçŸ¥ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–
        self._init_divergence_database()
        
        # å­¦ç¿’ä¾¡å€¤è©•ä¾¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.learning_value_weights = {
            'confidence_gap': 0.3,      # æ¨å¥¨ä¿¡é ¼åº¦ã¨å®Ÿé¸æŠã®ä¹–é›¢
            'satisfaction_impact': 0.25, # æº€è¶³åº¦ã¸ã®å½±éŸ¿
            'pattern_rarity': 0.2,       # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¸Œå°‘æ€§
            'context_richness': 0.15,    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è±Šå¯Œã•
            'behavioral_consistency': 0.1 # è¡Œå‹•ã®ä¸€è²«æ€§
        }
        
        logger.info("å¼·åŒ–ç‰ˆä¹–é›¢æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def _init_divergence_database(self):
        """ä¹–é›¢æ¤œçŸ¥ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
        with sqlite3.connect(self.divergence_db_path) as conn:
            cursor = conn.cursor()
            
            # ä¹–é›¢ã‚¤ãƒ™ãƒ³ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS divergence_events (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    event_id VARCHAR(100) UNIQUE NOT NULL,
                    session_id VARCHAR(100) NOT NULL,
                    user_id VARCHAR(100),
                    
                    -- ä¹–é›¢åŸºæœ¬æƒ…å ±
                    gemini_recommendation VARCHAR(50) NOT NULL,
                    user_choice VARCHAR(50) NOT NULL,
                    gemini_confidence FLOAT,
                    
                    -- åˆ†é¡æƒ…å ±
                    divergence_importance VARCHAR(20),
                    divergence_category VARCHAR(50),
                    
                    -- è©•ä¾¡æŒ‡æ¨™
                    satisfaction_score FLOAT,
                    learning_value FLOAT,
                    
                    -- ãƒ‡ãƒ¼ã‚¿è©³ç´°
                    context_data TEXT,
                    behavioral_indicators TEXT,
                    
                    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    
                    -- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”¨
                    date_only DATE GENERATED ALWAYS AS (DATE(created_at)) STORED
                )
            ''')
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS divergence_patterns (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id VARCHAR(100),
                    pattern_type VARCHAR(50),
                    pattern_data TEXT,
                    occurrence_count INTEGER DEFAULT 1,
                    learning_score FLOAT,
                    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS divergence_trends (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    time_period VARCHAR(20),
                    trend_data TEXT,
                    calculated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_divergence_session ON divergence_events (session_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_divergence_user ON divergence_events (user_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_divergence_importance ON divergence_events (divergence_importance)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_divergence_category ON divergence_events (divergence_category)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_divergence_date ON divergence_events (date_only)')
            
            conn.commit()
    
    def detect_real_time_divergence(self, 
                                  gemini_analysis_text: str,
                                  gemini_recommendation: str, 
                                  user_choice: str,
                                  session_id: str,
                                  user_id: Optional[str] = None,
                                  context_data: Optional[Dict] = None) -> DivergenceEvent:
        """
        ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ä¹–é›¢ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œçŸ¥
        
        Args:
            gemini_analysis_text: Geminiåˆ†æã®ç”Ÿãƒ†ã‚­ã‚¹ãƒˆ
            gemini_recommendation: Geminiæ¨å¥¨ã‚¨ãƒ³ã‚¸ãƒ³
            user_choice: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å®Ÿé¸æŠ
            session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            context_data: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
            
        Returns:
            ä¹–é›¢ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
        """
        try:
            # ã‚¤ãƒ™ãƒ³ãƒˆIDã®ç”Ÿæˆ
            event_id = self._generate_event_id(session_id, user_choice)
            
            # é«˜åº¦Geminiåˆ†æã«ã‚ˆã‚‹æ¨å¥¨è©³ç´°å–å¾—
            structured_recommendation = self.advanced_analyzer.extract_structured_recommendations(
                gemini_analysis_text
            )
            
            # æº€è¶³åº¦ã‚¹ã‚³ã‚¢ã®å–å¾—
            satisfaction_result = self.satisfaction_estimator.calculate_satisfaction(session_id, user_id)
            satisfaction_score = satisfaction_result.get('satisfaction_score', 0.0)
            
            # è¡Œå‹•æŒ‡æ¨™ã®åé›†
            behavioral_indicators = self._collect_behavioral_indicators(session_id, user_id)
            
            # ä¹–é›¢é‡è¦åº¦ã®åˆ¤å®š
            importance = self.classify_divergence_importance({
                'gemini_confidence': structured_recommendation.confidence_score,
                'satisfaction_score': satisfaction_score,
                'behavioral_indicators': behavioral_indicators,
                'context_data': context_data or {}
            })
            
            # ä¹–é›¢ã‚«ãƒ†ã‚´ãƒªã®åˆ†é¡
            category = self._classify_divergence_category(
                gemini_recommendation, 
                user_choice, 
                structured_recommendation,
                context_data or {}
            )
            
            # å­¦ç¿’ä¾¡å€¤ã®ç®—å‡º
            learning_value = self._calculate_learning_value(
                structured_recommendation.confidence_score,
                satisfaction_score,
                behavioral_indicators,
                context_data or {}
            )
            
            # ä¹–é›¢ã‚¤ãƒ™ãƒ³ãƒˆã®ä½œæˆ
            divergence_event = DivergenceEvent(
                event_id=event_id,
                session_id=session_id,
                user_id=user_id,
                gemini_recommendation=gemini_recommendation,
                user_choice=user_choice,
                gemini_confidence=structured_recommendation.confidence_score,
                divergence_importance=importance,
                divergence_category=category,
                satisfaction_score=satisfaction_score,
                context_data=context_data or {},
                behavioral_indicators=behavioral_indicators,
                learning_value=learning_value
            )
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜
            self._save_divergence_event(divergence_event)
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ã®æ›´æ–°
            self._update_pattern_learning(divergence_event)
            
            logger.info(f"ä¹–é›¢æ¤œçŸ¥å®Œäº†: {user_choice} vs {gemini_recommendation} "
                       f"(é‡è¦åº¦: {importance.value}, å­¦ç¿’ä¾¡å€¤: {learning_value:.3f})")
            
            return divergence_event
            
        except Exception as e:
            logger.error(f"ä¹–é›¢æ¤œçŸ¥ã‚¨ãƒ©ãƒ¼: {str(e)}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return DivergenceEvent(
                event_id=f"error_{int(time.time())}",
                session_id=session_id,
                user_id=user_id,
                gemini_recommendation=gemini_recommendation,
                user_choice=user_choice,
                gemini_confidence=0.0,
                divergence_importance=DivergenceImportance.NOISE,
                divergence_category=DivergenceCategory.EXPERIMENTAL,
                satisfaction_score=0.0,
                context_data={'error': str(e)},
                behavioral_indicators={},
                learning_value=0.0
            )
    
    def _generate_event_id(self, session_id: str, user_choice: str) -> str:
        """ä¹–é›¢ã‚¤ãƒ™ãƒ³ãƒˆIDã®ç”Ÿæˆ"""
        data = f"{session_id}_{user_choice}_{int(time.time() * 1000)}"
        return hashlib.md5(data.encode()).hexdigest()[:16]
    
    def _collect_behavioral_indicators(self, session_id: str, user_id: Optional[str]) -> Dict[str, Any]:
        """è¡Œå‹•æŒ‡æ¨™ã®åé›†"""
        indicators = {}
        
        try:
            with sqlite3.connect(self.analytics_db_path) as conn:
                cursor = conn.cursor()
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¡Œå‹•ã®åˆ†æ
                cursor.execute('''
                    SELECT event_type, COUNT(*) as count,
                           AVG(timestamp) as avg_timestamp
                    FROM analytics_events
                    WHERE session_id = ?
                    GROUP BY event_type
                ''', (session_id,))
                
                session_behaviors = {}
                for event_type, count, avg_timestamp in cursor.fetchall():
                    session_behaviors[event_type] = {
                        'count': count,
                        'avg_timestamp': avg_timestamp
                    }
                
                indicators['session_behaviors'] = session_behaviors
                
                # ã‚³ãƒ”ãƒ¼è¡Œå‹•ã®è©³ç´°åˆ†æ
                cursor.execute('''
                    SELECT custom_data
                    FROM analytics_events
                    WHERE session_id = ? AND event_type = 'translation_copy'
                    ORDER BY timestamp DESC
                    LIMIT 5
                ''', (session_id,))
                
                copy_behaviors = []
                for row in cursor.fetchall():
                    try:
                        copy_data = json.loads(row[0]) if row[0] else {}
                        copy_behaviors.append(copy_data)
                    except json.JSONDecodeError:
                        continue
                
                indicators['recent_copy_behaviors'] = copy_behaviors
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¶™ç¶šæ™‚é–“
                cursor.execute('''
                    SELECT MIN(timestamp) as start_time,
                           MAX(timestamp) as end_time
                    FROM analytics_events
                    WHERE session_id = ?
                ''', (session_id,))
                
                time_data = cursor.fetchone()
                if time_data and time_data[0] and time_data[1]:
                    duration = (time_data[1] - time_data[0]) / 1000.0
                    indicators['session_duration'] = duration
                
        except Exception as e:
            logger.error(f"è¡Œå‹•æŒ‡æ¨™åé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            indicators['error'] = str(e)
        
        return indicators
    
    def classify_divergence_importance(self, divergence_data: Dict[str, Any]) -> DivergenceImportance:
        """
        ä¹–é›¢åˆ†é¡ãƒ»é‡è¦åº¦åˆ¤å®š
        
        Args:
            divergence_data: ä¹–é›¢åˆ¤å®šç”¨ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            ä¹–é›¢é‡è¦åº¦
        """
        score = 0.0
        
        # 1. Geminiæ¨å¥¨ã®ä¿¡é ¼åº¦ï¼ˆé«˜ä¿¡é ¼åº¦ã§ã®ä¹–é›¢ã¯é‡è¦ï¼‰
        gemini_confidence = divergence_data.get('gemini_confidence', 0.0)
        if gemini_confidence >= 0.8:
            score += 3.0
        elif gemini_confidence >= 0.6:
            score += 2.0
        elif gemini_confidence >= 0.4:
            score += 1.0
        
        # 2. æº€è¶³åº¦ã‚¹ã‚³ã‚¢ï¼ˆé«˜æº€è¶³åº¦ã§ã®ä¹–é›¢ã¯ä¾¡å€¤ãŒé«˜ã„ï¼‰
        satisfaction_score = divergence_data.get('satisfaction_score', 0.0)
        if satisfaction_score >= 80:
            score += 2.5
        elif satisfaction_score >= 60:
            score += 1.5
        elif satisfaction_score >= 40:
            score += 0.5
        
        # 3. è¡Œå‹•ã®ä¸€è²«æ€§
        behavioral_indicators = divergence_data.get('behavioral_indicators', {})
        copy_behaviors = behavioral_indicators.get('recent_copy_behaviors', [])
        
        if len(copy_behaviors) >= 2:
            # è¤‡æ•°ã®ã‚³ãƒ”ãƒ¼è¡Œå‹•ãŒã‚ã‚‹ = æ…é‡ãªé¸æŠ
            score += 1.5
        
        session_duration = behavioral_indicators.get('session_duration', 0)
        if session_duration >= 120:  # 2åˆ†ä»¥ä¸Šã®æ¤œè¨æ™‚é–“
            score += 1.0
        
        # 4. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è±Šå¯Œã•
        context_data = divergence_data.get('context_data', {})
        if context_data.get('text_length', 0) > 200:
            score += 1.0
        if context_data.get('has_technical_terms'):
            score += 1.0
        if context_data.get('cultural_context'):
            score += 1.0
        
        # é‡è¦åº¦ã®åˆ†é¡
        if score >= 7.0:
            return DivergenceImportance.CRITICAL
        elif score >= 5.0:
            return DivergenceImportance.HIGH
        elif score >= 3.0:
            return DivergenceImportance.MEDIUM
        elif score >= 1.0:
            return DivergenceImportance.LOW
        else:
            return DivergenceImportance.NOISE
    
    def _classify_divergence_category(self, 
                                    gemini_recommendation: str,
                                    user_choice: str,
                                    structured_recommendation: StructuredRecommendation,
                                    context_data: Dict) -> DivergenceCategory:
        """ä¹–é›¢ã‚«ãƒ†ã‚´ãƒªã®åˆ†é¡"""
        
        # æ¨å¥¨ç†ç”±ã«åŸºã¥ãåˆ†é¡
        primary_reasons = [r.value for r in structured_recommendation.primary_reasons]
        
        # ã‚¹ã‚¿ã‚¤ãƒ«é–¢é€£ã®ç†ç”±ãŒå¤šã„å ´åˆ
        style_reasons = ['style', 'tone', 'formality']
        if any(reason in primary_reasons for reason in style_reasons):
            return DivergenceCategory.STYLE_PREFERENCE
        
        # ç²¾åº¦é–¢é€£ã®ç†ç”±ãŒå¤šã„å ´åˆ
        accuracy_reasons = ['accuracy', 'clarity', 'terminology']
        if any(reason in primary_reasons for reason in accuracy_reasons):
            return DivergenceCategory.ACCURACY_PRIORITY
        
        # æ–‡åŒ–ãƒ»æ–‡è„ˆé–¢é€£
        cultural_reasons = ['cultural_fit', 'context_fit']
        if any(reason in primary_reasons for reason in cultural_reasons):
            return DivergenceCategory.CULTURAL_ADAPTATION
        
        # æ–‡ç« é•·ã«åŸºã¥ãåˆ†é¡
        text_length = context_data.get('text_length', 0)
        if text_length > 500:
            return DivergenceCategory.DOMAIN_EXPERTISE
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ†é¡
        return DivergenceCategory.PERSONAL_HABIT
    
    def _calculate_learning_value(self, 
                                gemini_confidence: float,
                                satisfaction_score: float,
                                behavioral_indicators: Dict,
                                context_data: Dict) -> float:
        """å­¦ç¿’ä¾¡å€¤ã®ç®—å‡º"""
        
        learning_value = 0.0
        
        # 1. ä¿¡é ¼åº¦ã‚®ãƒ£ãƒƒãƒ—ï¼ˆé«˜ä¿¡é ¼åº¦æ¨å¥¨ã‹ã‚‰ã®ä¹–é›¢ã¯ä¾¡å€¤ãŒé«˜ã„ï¼‰
        confidence_gap = gemini_confidence * self.learning_value_weights['confidence_gap']
        learning_value += confidence_gap
        
        # 2. æº€è¶³åº¦ã¸ã®å½±éŸ¿
        satisfaction_impact = (satisfaction_score / 100.0) * self.learning_value_weights['satisfaction_impact']
        learning_value += satisfaction_impact
        
        # 3. ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¸Œå°‘æ€§ï¼ˆéå»ã®é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå°‘ãªã„ã»ã©ä¾¡å€¤ãŒé«˜ã„ï¼‰
        rarity_score = self._calculate_pattern_rarity(context_data)
        learning_value += rarity_score * self.learning_value_weights['pattern_rarity']
        
        # 4. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è±Šå¯Œã•
        context_richness = self._evaluate_context_richness(context_data)
        learning_value += context_richness * self.learning_value_weights['context_richness']
        
        # 5. è¡Œå‹•ã®ä¸€è²«æ€§
        behavioral_consistency = self._evaluate_behavioral_consistency(behavioral_indicators)
        learning_value += behavioral_consistency * self.learning_value_weights['behavioral_consistency']
        
        # 0.0-1.0ã®ç¯„å›²ã«æ­£è¦åŒ–
        return max(0.0, min(1.0, learning_value))
    
    def _calculate_pattern_rarity(self, context_data: Dict) -> float:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¸Œå°‘æ€§ç®—å‡º"""
        # ç°¡æ˜“å®Ÿè£…ï¼šå°†æ¥çš„ã«ã¯MLãƒ™ãƒ¼ã‚¹ã®å¸Œå°‘æ€§åˆ¤å®š
        text_length = context_data.get('text_length', 0)
        
        if text_length > 1000:
            return 0.9  # é•·æ–‡ã¯å¸Œå°‘
        elif text_length > 500:
            return 0.7
        elif text_length > 200:
            return 0.5
        else:
            return 0.3
    
    def _evaluate_context_richness(self, context_data: Dict) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è±Šå¯Œã•è©•ä¾¡"""
        richness = 0.0
        
        if context_data.get('has_technical_terms'):
            richness += 0.3
        if context_data.get('cultural_context'):
            richness += 0.3
        if context_data.get('business_context'):
            richness += 0.2
        if context_data.get('text_length', 0) > 300:
            richness += 0.2
        
        return min(1.0, richness)
    
    def _evaluate_behavioral_consistency(self, behavioral_indicators: Dict) -> float:
        """è¡Œå‹•ã®ä¸€è²«æ€§è©•ä¾¡"""
        consistency = 0.0
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¶™ç¶šæ™‚é–“ï¼ˆã˜ã£ãã‚Šæ¤œè¨ã—ãŸè¨¼æ‹ ï¼‰
        duration = behavioral_indicators.get('session_duration', 0)
        if duration >= 180:  # 3åˆ†ä»¥ä¸Š
            consistency += 0.4
        elif duration >= 60:  # 1åˆ†ä»¥ä¸Š
            consistency += 0.2
        
        # è¤‡æ•°ã®ã‚³ãƒ”ãƒ¼è¡Œå‹•ï¼ˆæ¯”è¼ƒæ¤œè¨ã®è¨¼æ‹ ï¼‰
        copy_behaviors = behavioral_indicators.get('recent_copy_behaviors', [])
        if len(copy_behaviors) >= 3:
            consistency += 0.3
        elif len(copy_behaviors) >= 2:
            consistency += 0.2
        
        # ç•°ãªã‚‹ã‚³ãƒ”ãƒ¼æ–¹æ³•ã®ä½¿ç”¨ï¼ˆä¸Šç´šãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¨¼æ‹ ï¼‰
        copy_methods = set()
        for behavior in copy_behaviors:
            method = behavior.get('copy_method')
            if method:
                copy_methods.add(method)
        
        if len(copy_methods) >= 2:
            consistency += 0.3
        
        return min(1.0, consistency)
    
    def identify_valuable_divergence_patterns(self, 
                                            user_id: Optional[str] = None,
                                            days: int = 30) -> List[Dict[str, Any]]:
        """
        è²´é‡ãªæ¨å¥¨é•ã„ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•ç‰¹å®š
        
        Args:
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼IDï¼ˆNoneã®å ´åˆã¯å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‰
            days: åˆ†ææœŸé–“ï¼ˆæ—¥æ•°ï¼‰
            
        Returns:
            è²´é‡ãªä¹–é›¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒªã‚¹ãƒˆ
        """
        valuable_patterns = []
        
        with sqlite3.connect(self.divergence_db_path) as conn:
            cursor = conn.cursor()
            
            query = '''
                SELECT event_id, session_id, user_id,
                       gemini_recommendation, user_choice,
                       divergence_importance, divergence_category,
                       learning_value, satisfaction_score,
                       context_data, behavioral_indicators,
                       created_at
                FROM divergence_events
                WHERE created_at >= datetime('now', '-{} days')
                  AND learning_value >= 0.7
            '''.format(days)
            
            if user_id:
                query += " AND user_id = ?"
                cursor.execute(query, (user_id,))
            else:
                cursor.execute(query)
            
            for row in cursor.fetchall():
                try:
                    context_data = json.loads(row[9]) if row[9] else {}
                    behavioral_indicators = json.loads(row[10]) if row[10] else {}
                    
                    pattern = {
                        'event_id': row[0],
                        'session_id': row[1],
                        'user_id': row[2],
                        'divergence': {
                            'gemini_recommendation': row[3],
                            'user_choice': row[4]
                        },
                        'classification': {
                            'importance': row[5],
                            'category': row[6]
                        },
                        'scores': {
                            'learning_value': row[7],
                            'satisfaction_score': row[8]
                        },
                        'context_data': context_data,
                        'behavioral_indicators': behavioral_indicators,
                        'created_at': row[11]
                    }
                    
                    valuable_patterns.append(pattern)
                    
                except json.JSONDecodeError:
                    continue
        
        # å­¦ç¿’ä¾¡å€¤ã§ã‚½ãƒ¼ãƒˆ
        valuable_patterns.sort(key=lambda x: x['scores']['learning_value'], reverse=True)
        
        logger.info(f"è²´é‡ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å®šå®Œäº†: {len(valuable_patterns)}ä»¶")
        return valuable_patterns
    
    def analyze_divergence_trends(self, time_window: str = "30days") -> DivergenceTrend:
        """
        çµ±è¨ˆåˆ†æãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º
        
        Args:
            time_window: åˆ†ææœŸé–“ ("7days", "30days", "90days")
            
        Returns:
            ä¹–é›¢ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æçµæœ
        """
        days = {"7days": 7, "30days": 30, "90days": 90}.get(time_window, 30)
        
        with sqlite3.connect(self.divergence_db_path) as conn:
            cursor = conn.cursor()
            
            # åŸºæœ¬çµ±è¨ˆã®å–å¾—
            cursor.execute('''
                SELECT COUNT(*) as total_divergences,
                       AVG(learning_value) as avg_learning_value,
                       AVG(satisfaction_score) as avg_satisfaction
                FROM divergence_events
                WHERE created_at >= datetime('now', '-{} days')
            '''.format(days))
            
            basic_stats = cursor.fetchone()
            total_divergences = basic_stats[0] if basic_stats else 0
            
            # å…¨ç¿»è¨³æ•°ã®å–å¾—ï¼ˆä¹–é›¢ç‡ç®—å‡ºç”¨ï¼‰
            with sqlite3.connect(self.analytics_db_path) as analytics_conn:
                analytics_cursor = analytics_conn.cursor()
                analytics_cursor.execute('''
                    SELECT COUNT(*)
                    FROM analytics_events
                    WHERE event_type = 'translation_copy'
                      AND timestamp >= (strftime('%s', 'now', '-{} days') * 1000)
                '''.format(days))
                
                result = analytics_cursor.fetchone()
                total_translations = result[0] if result else 1
            
            divergence_rate = total_divergences / max(1, total_translations)
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ
            cursor.execute('''
                SELECT divergence_category, COUNT(*)
                FROM divergence_events
                WHERE created_at >= datetime('now', '-{} days')
                GROUP BY divergence_category
            '''.format(days))
            
            category_distribution = {}
            for category, count in cursor.fetchall():
                try:
                    category_distribution[DivergenceCategory(category)] = count
                except ValueError:
                    category_distribution[DivergenceCategory.EXPERIMENTAL] = count
            
            # é‡è¦åº¦åˆ†å¸ƒ
            cursor.execute('''
                SELECT divergence_importance, COUNT(*)
                FROM divergence_events
                WHERE created_at >= datetime('now', '-{} days')
                GROUP BY divergence_importance
            '''.format(days))
            
            importance_distribution = {}
            for importance, count in cursor.fetchall():
                try:
                    importance_distribution[DivergenceImportance(importance)] = count
                except ValueError:
                    importance_distribution[DivergenceImportance.NOISE] = count
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            cursor.execute('''
                SELECT user_id, COUNT(*) as divergence_count,
                       AVG(learning_value) as avg_learning_value
                FROM divergence_events
                WHERE created_at >= datetime('now', '-{} days')
                  AND user_id IS NOT NULL
                GROUP BY user_id
                ORDER BY divergence_count DESC
                LIMIT 10
            '''.format(days))
            
            user_patterns = {}
            for user_id, count, avg_learning in cursor.fetchall():
                user_patterns[user_id] = {
                    'divergence_count': count,
                    'avg_learning_value': avg_learning
                }
            
            # å­¦ç¿’ä¾¡å€¤ç·åˆã‚¹ã‚³ã‚¢
            learning_value_score = basic_stats[1] if basic_stats and basic_stats[1] else 0.0
        
        trend = DivergenceTrend(
            time_period=time_window,
            total_divergences=total_divergences,
            divergence_rate=divergence_rate,
            category_distribution=category_distribution,
            importance_distribution=importance_distribution,
            learning_value_score=learning_value_score,
            user_patterns=user_patterns
        )
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        self._save_trend_analysis(trend)
        
        logger.info(f"ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æå®Œäº†: {time_window}, ä¹–é›¢ç‡={divergence_rate:.3f}")
        return trend
    
    def _save_divergence_event(self, event: DivergenceEvent):
        """ä¹–é›¢ã‚¤ãƒ™ãƒ³ãƒˆã®ä¿å­˜"""
        with sqlite3.connect(self.divergence_db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO divergence_events (
                    event_id, session_id, user_id,
                    gemini_recommendation, user_choice, gemini_confidence,
                    divergence_importance, divergence_category,
                    satisfaction_score, learning_value,
                    context_data, behavioral_indicators
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                event.event_id,
                event.session_id,
                event.user_id,
                event.gemini_recommendation,
                event.user_choice,
                event.gemini_confidence,
                event.divergence_importance.value,
                event.divergence_category.value,
                event.satisfaction_score,
                event.learning_value,
                json.dumps(event.context_data),
                json.dumps(event.behavioral_indicators)
            ))
            
            conn.commit()
    
    def _update_pattern_learning(self, event: DivergenceEvent):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ã®æ›´æ–°"""
        if not event.user_id:
            return
        
        with sqlite3.connect(self.divergence_db_path) as conn:
            cursor = conn.cursor()
            
            # æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¢ºèª
            pattern_key = f"{event.gemini_recommendation}_{event.user_choice}_{event.divergence_category.value}"
            
            cursor.execute('''
                SELECT id, occurrence_count, learning_score
                FROM divergence_patterns
                WHERE user_id = ? AND pattern_type = ?
            ''', (event.user_id, pattern_key))
            
            existing = cursor.fetchone()
            
            if existing:
                # æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ›´æ–°
                new_count = existing[1] + 1
                new_score = (existing[2] + event.learning_value) / 2  # å¹³å‡
                
                cursor.execute('''
                    UPDATE divergence_patterns
                    SET occurrence_count = ?, learning_score = ?,
                        last_updated = CURRENT_TIMESTAMP
                    WHERE id = ?
                ''', (new_count, new_score, existing[0]))
            else:
                # æ–°è¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä½œæˆ
                cursor.execute('''
                    INSERT INTO divergence_patterns (
                        user_id, pattern_type, pattern_data,
                        occurrence_count, learning_score
                    ) VALUES (?, ?, ?, ?, ?)
                ''', (
                    event.user_id,
                    pattern_key,
                    json.dumps({
                        'gemini_recommendation': event.gemini_recommendation,
                        'user_choice': event.user_choice,
                        'category': event.divergence_category.value
                    }),
                    1,
                    event.learning_value
                ))
            
            conn.commit()
    
    def _save_trend_analysis(self, trend: DivergenceTrend):
        """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æçµæœã®ä¿å­˜"""
        with sqlite3.connect(self.divergence_db_path) as conn:
            cursor = conn.cursor()
            
            trend_data = {
                'total_divergences': trend.total_divergences,
                'divergence_rate': trend.divergence_rate,
                'category_distribution': {k.value: v for k, v in trend.category_distribution.items()},
                'importance_distribution': {k.value: v for k, v in trend.importance_distribution.items()},
                'learning_value_score': trend.learning_value_score,
                'user_patterns': trend.user_patterns
            }
            
            cursor.execute('''
                INSERT INTO divergence_trends (time_period, trend_data)
                VALUES (?, ?)
            ''', (trend.time_period, json.dumps(trend_data)))
            
            conn.commit()


# ãƒ†ã‚¹ãƒˆç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°
if __name__ == "__main__":
    print("ğŸ¯ å¼·åŒ–ç‰ˆä¹–é›¢æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ  - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print("=" * 60)
    
    detector = EnhancedRecommendationDivergenceDetector()
    
    # ãƒ†ã‚¹ãƒˆç”¨ä¹–é›¢ã‚¤ãƒ™ãƒ³ãƒˆ
    test_analysis = """
    3ã¤ã®ç¿»è¨³ã‚’è©³ç´°ã«åˆ†æã—ãŸçµæœã€Enhancedç¿»è¨³ãŒæœ€ã‚‚é©åˆ‡ã§ã™ã€‚
    æ–‡è„ˆã¸ã®é©åˆæ€§ã¨è‡ªç„¶ã•ã®è¦³ç‚¹ã‹ã‚‰å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚
    """
    
    divergence = detector.detect_real_time_divergence(
        gemini_analysis_text=test_analysis,
        gemini_recommendation="enhanced",
        user_choice="gemini",
        session_id="test_session_292",
        user_id="test_user",
        context_data={
            'text_length': 200,
            'has_technical_terms': True,
            'business_context': True
        }
    )
    
    print(f"âœ… ä¹–é›¢æ¤œçŸ¥çµæœ:")
    print(f"  é‡è¦åº¦: {divergence.divergence_importance.value}")
    print(f"  ã‚«ãƒ†ã‚´ãƒª: {divergence.divergence_category.value}")
    print(f"  å­¦ç¿’ä¾¡å€¤: {divergence.learning_value:.3f}")
    print(f"  æº€è¶³åº¦: {divergence.satisfaction_score}")
    
    # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
    trend = detector.analyze_divergence_trends("30days")
    print(f"\nğŸ“ˆ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ:")
    print(f"  ç·ä¹–é›¢æ•°: {trend.total_divergences}")
    print(f"  ä¹–é›¢ç‡: {trend.divergence_rate:.3f}")
    print(f"  å­¦ç¿’ä¾¡å€¤ã‚¹ã‚³ã‚¢: {trend.learning_value_score:.3f}")
    
    print("\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº† - å¼·åŒ–ç‰ˆä¹–é›¢æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ æ­£å¸¸å‹•ä½œ")