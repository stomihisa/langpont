#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üöÄ Task 2.9.2 Phase B-2: È´òÂ∫¶„Å™ÂàÜÊûêÊ©üËÉΩ„Å®„É™„Ç¢„É´„Çø„Ç§„É†Áõ£Ë¶ñ
================================================================
ÁõÆÁöÑ: Task 2.9.2„Ç∑„Çπ„ÉÜ„É†„ÅÆË©≥Á¥∞Áõ£Ë¶ñ„Å®„Éá„Éº„ÇøÂèéÈõÜÂº∑Âåñ
Ê©üËÉΩ: Êé®Â•®ÊäΩÂá∫Á≤æÂ∫¶ÂàÜÊûê„ÄÅAPIÁõ£Ë¶ñ„ÄÅÂÄã‰∫∫Âåñ„Éá„Éº„ÇøÂèéÈõÜ
"""

import logging
import json
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict, Counter
import time
import os
import threading
from dataclasses import dataclass, field

# „É≠„Ç∞Ë®≠ÂÆö
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class Task292Metrics:
    """Task 2.9.2 „É°„Éà„É™„ÇØ„Çπ"""
    total_extractions: int = 0
    successful_extractions: int = 0
    failed_extractions: int = 0
    llm_successes: int = 0
    llm_failures: int = 0
    api_errors: int = 0
    average_confidence: float = 0.0
    extraction_methods: Dict[str, int] = field(default_factory=dict)
    recommendations: Dict[str, int] = field(default_factory=dict)
    language_distribution: Dict[str, int] = field(default_factory=dict)
    error_patterns: Dict[str, int] = field(default_factory=dict)
    
class AdvancedDashboardAnalytics:
    """È´òÂ∫¶„Å™ÂàÜÊûêÊ©üËÉΩ„Å®„É™„Ç¢„É´„Çø„Ç§„É†Áõ£Ë¶ñ„Ç∑„Çπ„ÉÜ„É†"""
    
    def __init__(self, db_path: str = "task292_analytics.db"):
        """ÂàùÊúüÂåñ"""
        self.db_path = db_path
        self.lock = threading.Lock()
        
        # „É™„Ç¢„É´„Çø„Ç§„É†„É°„Éà„É™„ÇØ„Çπ
        self.current_metrics = Task292Metrics()
        
        # API‰ΩøÁî®Áµ±Ë®à
        self.api_stats = {
            'openai': {
                'total_calls': 0,
                'successful_calls': 0,
                'failed_calls': 0,
                'total_duration_ms': 0,
                'average_duration_ms': 0,
                'error_types': defaultdict(int)
            },
            'gemini': {
                'total_calls': 0,
                'successful_calls': 0,
                'failed_calls': 0,
                'total_duration_ms': 0,
                'average_duration_ms': 0,
                'error_types': defaultdict(int)
            }
        }
        
        # „Ç¢„É©„Éº„ÉàË®≠ÂÆö
        self.alert_thresholds = {
            'error_rate': 0.2,  # 20%‰ª•‰∏ä„ÅÆ„Ç®„É©„ÉºÁéá„Åß„Ç¢„É©„Éº„Éà
            'api_response_time': 3000,  # 3Áßí‰ª•‰∏ä„Åß„Ç¢„É©„Éº„Éà
            'extraction_failure_rate': 0.3,  # 30%‰ª•‰∏ä„ÅÆÂ§±ÊïóÁéá„Åß„Ç¢„É©„Éº„Éà
            'low_confidence_rate': 0.5  # 50%‰ª•‰∏ä„Åå‰Ωé‰ø°È†ºÂ∫¶„Åß„Ç¢„É©„Éº„Éà
        }
        
        # „Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Ç¢„É©„Éº„Éà
        self.active_alerts = []
        
        # ÂÄã‰∫∫Âåñ„Éá„Éº„ÇøÂèéÈõÜ
        self.personalization_data = defaultdict(lambda: {
            'translation_preferences': Counter(),
            'language_pairs': Counter(),
            'rejection_patterns': [],
            'style_preferences': {},
            'interaction_count': 0
        })
        
        self._init_database()
        logger.info("üöÄ Phase B-2: È´òÂ∫¶„Å™ÂàÜÊûê„Ç∑„Çπ„ÉÜ„É†ÂàùÊúüÂåñÂÆå‰∫Ü")
    
    def _init_database(self):
        """„Éá„Éº„Çø„Éô„Éº„ÇπÂàùÊúüÂåñ"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # Task 2.9.2 Êé®Â•®ÊäΩÂá∫Ë©≥Á¥∞„É≠„Ç∞
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS task292_extraction_logs (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp TEXT NOT NULL,
                        session_id TEXT,
                        user_id TEXT,
                        input_text TEXT,
                        analysis_language TEXT,
                        extraction_method TEXT,
                        recommendation TEXT,
                        confidence REAL,
                        processing_time_ms INTEGER,
                        success BOOLEAN,
                        error_details TEXT,
                        llm_response TEXT,
                        metadata TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # APIÁõ£Ë¶ñ„É≠„Ç∞
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS api_monitoring (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp TEXT NOT NULL,
                        api_provider TEXT NOT NULL,
                        endpoint TEXT,
                        method TEXT,
                        status_code INTEGER,
                        response_time_ms INTEGER,
                        success BOOLEAN,
                        error_type TEXT,
                        error_message TEXT,
                        request_size INTEGER,
                        response_size INTEGER,
                        metadata TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # ÂÄã‰∫∫Âåñ„Éá„Éº„ÇøÂèéÈõÜ
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS personalization_data (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        user_id TEXT NOT NULL,
                        timestamp TEXT NOT NULL,
                        event_type TEXT,
                        gemini_recommendation TEXT,
                        user_choice TEXT,
                        rejection_reason TEXT,
                        language_pair TEXT,
                        translation_context TEXT,
                        style_attributes TEXT,
                        confidence_score REAL,
                        metadata TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # „Ç¢„É©„Éº„ÉàÂ±•Ê≠¥
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS alert_history (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp TEXT NOT NULL,
                        alert_type TEXT NOT NULL,
                        severity TEXT NOT NULL,
                        message TEXT,
                        metric_value REAL,
                        threshold_value REAL,
                        resolved BOOLEAN DEFAULT 0,
                        resolved_at TEXT,
                        metadata TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # „Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ‰ΩúÊàê
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_task292_timestamp ON task292_extraction_logs(timestamp)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_task292_user ON task292_extraction_logs(user_id)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_api_timestamp ON api_monitoring(timestamp)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_personalization_user ON personalization_data(user_id)")
                
                conn.commit()
                logger.info("üìä Task 2.9.2 ÂàÜÊûê„Éá„Éº„Çø„Éô„Éº„ÇπÂàùÊúüÂåñÂÆå‰∫Ü")
                
        except Exception as e:
            logger.error(f"‚ùå „Éá„Éº„Çø„Éô„Éº„ÇπÂàùÊúüÂåñ„Ç®„É©„Éº: {str(e)}")
    
    def log_task292_extraction(self, session_id: str, user_id: str, input_text: str,
                              analysis_language: str, method: str, recommendation: Optional[str],
                              confidence: float, processing_time_ms: int, success: bool,
                              error_details: str = "", llm_response: str = "",
                              metadata: Dict[str, Any] = None):
        """Task 2.9.2 Êé®Â•®ÊäΩÂá∫„ÅÆË©≥Á¥∞„É≠„Ç∞Ë®òÈå≤"""
        try:
            timestamp = datetime.now().isoformat()
            
            # „É™„Ç¢„É´„Çø„Ç§„É†„É°„Éà„É™„ÇØ„ÇπÊõ¥Êñ∞
            with self.lock:
                self.current_metrics.total_extractions += 1
                if success:
                    self.current_metrics.successful_extractions += 1
                    if method.startswith("llm"):
                        self.current_metrics.llm_successes += 1
                else:
                    self.current_metrics.failed_extractions += 1
                    if method.startswith("llm"):
                        self.current_metrics.llm_failures += 1
                    if "api" in error_details.lower():
                        self.current_metrics.api_errors += 1
                
                # „É°„ÇΩ„ÉÉ„ÉâÁµ±Ë®à
                self.current_metrics.extraction_methods[method] = \
                    self.current_metrics.extraction_methods.get(method, 0) + 1
                
                # Êé®Â•®Áµ±Ë®à
                if recommendation:
                    self.current_metrics.recommendations[recommendation] = \
                        self.current_metrics.recommendations.get(recommendation, 0) + 1
                
                # Ë®ÄË™ûÂàÜÂ∏É
                self.current_metrics.language_distribution[analysis_language] = \
                    self.current_metrics.language_distribution.get(analysis_language, 0) + 1
                
                # „Ç®„É©„Éº„Éë„Çø„Éº„É≥
                if not success and error_details:
                    error_key = error_details.split(':')[0] if ':' in error_details else error_details
                    self.current_metrics.error_patterns[error_key] = \
                        self.current_metrics.error_patterns.get(error_key, 0) + 1
                
                # Âπ≥Âùá‰ø°È†ºÂ∫¶Êõ¥Êñ∞
                if success and confidence > 0:
                    total_conf = self.current_metrics.average_confidence * (self.current_metrics.successful_extractions - 1)
                    self.current_metrics.average_confidence = (total_conf + confidence) / self.current_metrics.successful_extractions
            
            # „Éá„Éº„Çø„Éô„Éº„Çπ„Å´‰øùÂ≠ò
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO task292_extraction_logs 
                    (timestamp, session_id, user_id, input_text, analysis_language,
                     extraction_method, recommendation, confidence, processing_time_ms,
                     success, error_details, llm_response, metadata)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    timestamp, session_id, user_id, input_text[:500], analysis_language,
                    method, recommendation, confidence, processing_time_ms,
                    success, error_details, llm_response[:1000] if llm_response else "",
                    json.dumps(metadata) if metadata else None
                ))
                conn.commit()
            
            # „Ç¢„É©„Éº„Éà„ÉÅ„Çß„ÉÉ„ÇØ
            self._check_extraction_alerts()
            
            logger.info(f"üìä Task 2.9.2 ÊäΩÂá∫„É≠„Ç∞Ë®òÈå≤: method={method}, success={success}, confidence={confidence}")
            
        except Exception as e:
            logger.error(f"‚ùå Task 2.9.2 „É≠„Ç∞Ë®òÈå≤„Ç®„É©„Éº: {str(e)}")
    
    def log_api_call(self, api_provider: str, endpoint: str, method: str,
                     status_code: int, response_time_ms: int, success: bool,
                     error_type: str = "", error_message: str = "",
                     request_size: int = 0, response_size: int = 0,
                     metadata: Dict[str, Any] = None):
        """APIÂëº„Å≥Âá∫„Åó„ÅÆÁõ£Ë¶ñ„É≠„Ç∞Ë®òÈå≤"""
        try:
            timestamp = datetime.now().isoformat()
            
            # APIÁµ±Ë®àÊõ¥Êñ∞
            with self.lock:
                api_stat = self.api_stats[api_provider]
                api_stat['total_calls'] += 1
                
                if success:
                    api_stat['successful_calls'] += 1
                else:
                    api_stat['failed_calls'] += 1
                    if error_type:
                        api_stat['error_types'][error_type] += 1
                
                # Âπ≥ÂùáÂøúÁ≠îÊôÇÈñìÊõ¥Êñ∞
                api_stat['total_duration_ms'] += response_time_ms
                api_stat['average_duration_ms'] = api_stat['total_duration_ms'] / api_stat['total_calls']
            
            # „Éá„Éº„Çø„Éô„Éº„Çπ„Å´‰øùÂ≠ò
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO api_monitoring
                    (timestamp, api_provider, endpoint, method, status_code,
                     response_time_ms, success, error_type, error_message,
                     request_size, response_size, metadata)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    timestamp, api_provider, endpoint, method, status_code,
                    response_time_ms, success, error_type, error_message,
                    request_size, response_size,
                    json.dumps(metadata) if metadata else None
                ))
                conn.commit()
            
            # API„Ç¢„É©„Éº„Éà„ÉÅ„Çß„ÉÉ„ÇØ
            self._check_api_alerts(api_provider, response_time_ms)
            
        except Exception as e:
            logger.error(f"‚ùå APIÁõ£Ë¶ñ„É≠„Ç∞„Ç®„É©„Éº: {str(e)}")
    
    def log_personalization_event(self, user_id: str, event_type: str,
                                 gemini_recommendation: Optional[str],
                                 user_choice: Optional[str],
                                 language_pair: str,
                                 translation_context: str = "",
                                 style_attributes: Dict[str, Any] = None,
                                 confidence_score: float = 0.0,
                                 rejection_reason: str = "",
                                 metadata: Dict[str, Any] = None):
        """ÂÄã‰∫∫Âåñ„Éá„Éº„ÇøÂèéÈõÜ„Ç§„Éô„É≥„ÉàË®òÈå≤"""
        try:
            timestamp = datetime.now().isoformat()
            
            # „É°„É¢„É™ÂÜÖÂÄã‰∫∫Âåñ„Éá„Éº„ÇøÊõ¥Êñ∞
            with self.lock:
                user_data = self.personalization_data[user_id]
                user_data['interaction_count'] += 1
                
                # ÁøªË®≥ÈÅ∏Â•ΩË®òÈå≤
                if user_choice:
                    user_data['translation_preferences'][user_choice] += 1
                
                # Ë®ÄË™û„Éö„Ç¢Ë®òÈå≤
                user_data['language_pairs'][language_pair] += 1
                
                # ÊãíÂê¶„Éë„Çø„Éº„É≥Ë®òÈå≤
                if event_type == 'rejection' and rejection_reason:
                    user_data['rejection_patterns'].append({
                        'timestamp': timestamp,
                        'recommendation': gemini_recommendation,
                        'reason': rejection_reason
                    })
                
                # „Çπ„Çø„Ç§„É´ÈÅ∏Â•ΩÊõ¥Êñ∞
                if style_attributes:
                    for attr, value in style_attributes.items():
                        if attr not in user_data['style_preferences']:
                            user_data['style_preferences'][attr] = Counter()
                        user_data['style_preferences'][attr][value] += 1
            
            # „Éá„Éº„Çø„Éô„Éº„Çπ„Å´‰øùÂ≠ò
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO personalization_data
                    (user_id, timestamp, event_type, gemini_recommendation,
                     user_choice, rejection_reason, language_pair,
                     translation_context, style_attributes, confidence_score, metadata)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    user_id, timestamp, event_type, gemini_recommendation,
                    user_choice, rejection_reason, language_pair,
                    translation_context[:500],
                    json.dumps(style_attributes) if style_attributes else None,
                    confidence_score,
                    json.dumps(metadata) if metadata else None
                ))
                conn.commit()
            
            logger.info(f"üìä ÂÄã‰∫∫Âåñ„Ç§„Éô„É≥„ÉàË®òÈå≤: user={user_id}, type={event_type}")
            
        except Exception as e:
            logger.error(f"‚ùå ÂÄã‰∫∫Âåñ„Éá„Éº„ÇøË®òÈå≤„Ç®„É©„Éº: {str(e)}")
    
    def _check_extraction_alerts(self):
        """Êé®Â•®ÊäΩÂá∫Èñ¢ÈÄ£„ÅÆ„Ç¢„É©„Éº„Éà„ÉÅ„Çß„ÉÉ„ÇØ"""
        try:
            with self.lock:
                # „Ç®„É©„ÉºÁéá„ÉÅ„Çß„ÉÉ„ÇØ
                if self.current_metrics.total_extractions > 10:
                    error_rate = self.current_metrics.failed_extractions / self.current_metrics.total_extractions
                    if error_rate > self.alert_thresholds['extraction_failure_rate']:
                        self._create_alert(
                            alert_type='high_extraction_failure_rate',
                            severity='WARNING',
                            message=f'Êé®Â•®ÊäΩÂá∫Â§±ÊïóÁéá„ÅåÈñæÂÄ§„ÇíË∂ÖÈÅé: {error_rate:.2%}',
                            metric_value=error_rate,
                            threshold_value=self.alert_thresholds['extraction_failure_rate']
                        )
                
                # ‰Ωé‰ø°È†ºÂ∫¶„ÉÅ„Çß„ÉÉ„ÇØ
                if self.current_metrics.successful_extractions > 10:
                    if self.current_metrics.average_confidence < 0.5:
                        self._create_alert(
                            alert_type='low_confidence_extractions',
                            severity='INFO',
                            message=f'Âπ≥Âùá‰ø°È†ºÂ∫¶„Åå‰Ωé‰∏ã: {self.current_metrics.average_confidence:.2f}',
                            metric_value=self.current_metrics.average_confidence,
                            threshold_value=0.5
                        )
                        
        except Exception as e:
            logger.error(f"‚ùå „Ç¢„É©„Éº„Éà„ÉÅ„Çß„ÉÉ„ÇØ„Ç®„É©„Éº: {str(e)}")
    
    def _check_api_alerts(self, api_provider: str, response_time_ms: int):
        """APIÈñ¢ÈÄ£„ÅÆ„Ç¢„É©„Éº„Éà„ÉÅ„Çß„ÉÉ„ÇØ"""
        try:
            # ÂøúÁ≠îÊôÇÈñì„ÉÅ„Çß„ÉÉ„ÇØ
            if response_time_ms > self.alert_thresholds['api_response_time']:
                self._create_alert(
                    alert_type='slow_api_response',
                    severity='WARNING',
                    message=f'{api_provider} APIÂøúÁ≠îÊôÇÈñì„ÅåÈñæÂÄ§„ÇíË∂ÖÈÅé: {response_time_ms}ms',
                    metric_value=response_time_ms,
                    threshold_value=self.alert_thresholds['api_response_time']
                )
            
            # „Ç®„É©„ÉºÁéá„ÉÅ„Çß„ÉÉ„ÇØ
            with self.lock:
                api_stat = self.api_stats[api_provider]
                if api_stat['total_calls'] > 10:
                    error_rate = api_stat['failed_calls'] / api_stat['total_calls']
                    if error_rate > self.alert_thresholds['error_rate']:
                        self._create_alert(
                            alert_type='high_api_error_rate',
                            severity='CRITICAL',
                            message=f'{api_provider} API„Ç®„É©„ÉºÁéá„ÅåÈñæÂÄ§„ÇíË∂ÖÈÅé: {error_rate:.2%}',
                            metric_value=error_rate,
                            threshold_value=self.alert_thresholds['error_rate']
                        )
                        
        except Exception as e:
            logger.error(f"‚ùå API„Ç¢„É©„Éº„Éà„ÉÅ„Çß„ÉÉ„ÇØ„Ç®„É©„Éº: {str(e)}")
    
    def _create_alert(self, alert_type: str, severity: str, message: str,
                     metric_value: float, threshold_value: float,
                     metadata: Dict[str, Any] = None):
        """„Ç¢„É©„Éº„Éà‰ΩúÊàê"""
        try:
            timestamp = datetime.now().isoformat()
            
            # „Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Ç¢„É©„Éº„Éà„Å´ËøΩÂä†
            alert = {
                'timestamp': timestamp,
                'alert_type': alert_type,
                'severity': severity,
                'message': message,
                'metric_value': metric_value,
                'threshold_value': threshold_value,
                'resolved': False
            }
            
            with self.lock:
                self.active_alerts.append(alert)
                # ÊúÄÊñ∞20‰ª∂„ÅÆ„Åø‰øùÊåÅ
                if len(self.active_alerts) > 20:
                    self.active_alerts = self.active_alerts[-20:]
            
            # „Éá„Éº„Çø„Éô„Éº„Çπ„Å´‰øùÂ≠ò
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO alert_history
                    (timestamp, alert_type, severity, message,
                     metric_value, threshold_value, metadata)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    timestamp, alert_type, severity, message,
                    metric_value, threshold_value,
                    json.dumps(metadata) if metadata else None
                ))
                conn.commit()
            
            logger.warning(f"üö® „Ç¢„É©„Éº„ÉàÁô∫Áîü: {severity} - {message}")
            
        except Exception as e:
            logger.error(f"‚ùå „Ç¢„É©„Éº„Éà‰ΩúÊàê„Ç®„É©„Éº: {str(e)}")
    
    def get_task292_metrics(self) -> Dict[str, Any]:
        """Task 2.9.2 „É°„Éà„É™„ÇØ„ÇπÂèñÂæó"""
        with self.lock:
            return {
                'total_extractions': self.current_metrics.total_extractions,
                'successful_extractions': self.current_metrics.successful_extractions,
                'failed_extractions': self.current_metrics.failed_extractions,
                'success_rate': (self.current_metrics.successful_extractions / 
                               self.current_metrics.total_extractions * 100) 
                               if self.current_metrics.total_extractions > 0 else 0,
                'average_confidence': self.current_metrics.average_confidence,
                'llm_success_rate': (self.current_metrics.llm_successes / 
                                   (self.current_metrics.llm_successes + self.current_metrics.llm_failures) * 100)
                                   if (self.current_metrics.llm_successes + self.current_metrics.llm_failures) > 0 else 0,
                'extraction_methods': dict(self.current_metrics.extraction_methods),
                'recommendations': dict(self.current_metrics.recommendations),
                'language_distribution': dict(self.current_metrics.language_distribution),
                'error_patterns': dict(self.current_metrics.error_patterns),
                'api_errors': self.current_metrics.api_errors
            }
    
    def get_api_statistics(self) -> Dict[str, Any]:
        """APIÁµ±Ë®àÂèñÂæó"""
        with self.lock:
            return {
                'openai': {
                    'total_calls': self.api_stats['openai']['total_calls'],
                    'success_rate': (self.api_stats['openai']['successful_calls'] /
                                   self.api_stats['openai']['total_calls'] * 100)
                                   if self.api_stats['openai']['total_calls'] > 0 else 0,
                    'average_response_time': self.api_stats['openai']['average_duration_ms'],
                    'error_types': dict(self.api_stats['openai']['error_types'])
                },
                'gemini': {
                    'total_calls': self.api_stats['gemini']['total_calls'],
                    'success_rate': (self.api_stats['gemini']['successful_calls'] /
                                   self.api_stats['gemini']['total_calls'] * 100)
                                   if self.api_stats['gemini']['total_calls'] > 0 else 0,
                    'average_response_time': self.api_stats['gemini']['average_duration_ms'],
                    'error_types': dict(self.api_stats['gemini']['error_types'])
                }
            }
    
    def get_active_alerts(self) -> List[Dict[str, Any]]:
        """„Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Ç¢„É©„Éº„ÉàÂèñÂæó"""
        with self.lock:
            return list(self.active_alerts)
    
    def get_personalization_insights(self, user_id: str) -> Dict[str, Any]:
        """„É¶„Éº„Ç∂„ÉºÂÄã‰∫∫Âåñ„Ç§„É≥„Çµ„Ç§„ÉàÂèñÂæó"""
        with self.lock:
            user_data = self.personalization_data.get(user_id, {})
            
            if not user_data or user_data.get('interaction_count', 0) == 0:
                return {'available': False}
            
            # ÊúÄ„ÇÇÂ•Ω„Åæ„Çå„ÇãÁøªË®≥„Ç®„É≥„Ç∏„É≥
            preferred_engine = None
            if user_data.get('translation_preferences'):
                preferred_engine = user_data['translation_preferences'].most_common(1)[0]
            
            # ÊúÄ„ÇÇ‰ΩøÁî®„Åï„Çå„ÇãË®ÄË™û„Éö„Ç¢
            top_language_pair = None
            if user_data.get('language_pairs'):
                top_language_pair = user_data['language_pairs'].most_common(1)[0]
            
            return {
                'available': True,
                'interaction_count': user_data.get('interaction_count', 0),
                'preferred_engine': preferred_engine,
                'top_language_pair': top_language_pair,
                'style_preferences': dict(user_data.get('style_preferences', {})),
                'rejection_patterns': len(user_data.get('rejection_patterns', []))
            }
    
    def get_extraction_history(self, hours: int = 24, limit: int = 100) -> List[Dict[str, Any]]:
        """Êé®Â•®ÊäΩÂá∫Â±•Ê≠¥ÂèñÂæó"""
        try:
            since_time = (datetime.now() - timedelta(hours=hours)).isoformat()
            
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT timestamp, user_id, analysis_language, extraction_method,
                           recommendation, confidence, processing_time_ms, success,
                           error_details
                    FROM task292_extraction_logs
                    WHERE timestamp > ?
                    ORDER BY timestamp DESC
                    LIMIT ?
                """, (since_time, limit))
                
                columns = [desc[0] for desc in cursor.description]
                return [dict(zip(columns, row)) for row in cursor.fetchall()]
                
        except Exception as e:
            logger.error(f"‚ùå ÊäΩÂá∫Â±•Ê≠¥ÂèñÂæó„Ç®„É©„Éº: {str(e)}")
            return []
    
    def cleanup_old_data(self, days_to_keep: int = 30):
        """Âè§„ÅÑ„Éá„Éº„Çø„ÅÆ„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó"""
        try:
            cutoff_time = (datetime.now() - timedelta(days=days_to_keep)).isoformat()
            
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                tables = ['task292_extraction_logs', 'api_monitoring', 
                         'personalization_data', 'alert_history']
                
                for table in tables:
                    cursor.execute(f"DELETE FROM {table} WHERE timestamp < ?", (cutoff_time,))
                
                conn.commit()
                logger.info(f"üßπ {days_to_keep}Êó•‰ª•Ââç„ÅÆ„Éá„Éº„Çø„Çí„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó„Åó„Åæ„Åó„Åü")
                
        except Exception as e:
            logger.error(f"‚ùå „Éá„Éº„Çø„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó„Ç®„É©„Éº: {str(e)}")

# „Ç∞„É≠„Éº„Éê„É´„Ç§„É≥„Çπ„Çø„É≥„Çπ
advanced_analytics = AdvancedDashboardAnalytics()

# „ÉÜ„Çπ„ÉàÈñ¢Êï∞
def test_advanced_analytics():
    """È´òÂ∫¶„Å™ÂàÜÊûê„Ç∑„Çπ„ÉÜ„É†„ÅÆ„ÉÜ„Çπ„Éà"""
    print("üß™ Phase B-2: È´òÂ∫¶„Å™ÂàÜÊûê„Ç∑„Çπ„ÉÜ„É†„ÉÜ„Çπ„ÉàÈñãÂßã")
    print("=" * 60)
    
    # „ÉÜ„Çπ„Éà„Éá„Éº„ÇøÁîüÊàê
    test_user_id = "test_user_001"
    test_session_id = "test_session_001"
    
    # 1. Task 2.9.2 Êé®Â•®ÊäΩÂá∫„É≠„Ç∞„ÉÜ„Çπ„Éà
    print("\nüìù Test 1: Task 2.9.2 Êé®Â•®ÊäΩÂá∫„É≠„Ç∞")
    advanced_analytics.log_task292_extraction(
        session_id=test_session_id,
        user_id=test_user_id,
        input_text="„Åì„Çå„ÅØ„ÉÜ„Çπ„ÉàÁøªË®≥„ÉÜ„Ç≠„Çπ„Éà„Åß„Åô„ÄÇ",
        analysis_language="Japanese",
        method="llm_chatgpt_a8",
        recommendation="enhanced",
        confidence=0.95,
        processing_time_ms=1200,
        success=True,
        metadata={'test': True}
    )
    print("‚úÖ ÊàêÂäü„Ç±„Éº„Çπ„É≠„Ç∞Ë®òÈå≤")
    
    # Â§±Êïó„Ç±„Éº„Çπ
    advanced_analytics.log_task292_extraction(
        session_id=test_session_id,
        user_id=test_user_id,
        input_text="„Ç®„É©„Éº„ÉÜ„Çπ„Éà„Ç±„Éº„Çπ",
        analysis_language="Japanese",
        method="llm_chatgpt_a8",
        recommendation=None,
        confidence=0.0,
        processing_time_ms=5000,
        success=False,
        error_details="api_timeout: Request timeout after 5s"
    )
    print("‚úÖ Â§±Êïó„Ç±„Éº„Çπ„É≠„Ç∞Ë®òÈå≤")
    
    # 2. APIÁõ£Ë¶ñ„É≠„Ç∞„ÉÜ„Çπ„Éà
    print("\nüìù Test 2: APIÁõ£Ë¶ñ„É≠„Ç∞")
    advanced_analytics.log_api_call(
        api_provider="openai",
        endpoint="/v1/chat/completions",
        method="POST",
        status_code=200,
        response_time_ms=850,
        success=True,
        request_size=1024,
        response_size=2048
    )
    
    # ÈÅÖ„ÅÑAPI„É¨„Çπ„Éù„É≥„ÇπÔºà„Ç¢„É©„Éº„ÉàÁô∫ÁîüÔºâ
    advanced_analytics.log_api_call(
        api_provider="gemini",
        endpoint="/v1beta/models/gemini-1.5-pro:generateContent",
        method="POST",
        status_code=200,
        response_time_ms=4500,  # „Ç¢„É©„Éº„ÉàÈñæÂÄ§Ë∂ÖÈÅé
        success=True
    )
    print("‚úÖ APIÁõ£Ë¶ñ„É≠„Ç∞Ë®òÈå≤Ôºà„Ç¢„É©„Éº„ÉàÁô∫ÁîüÂê´„ÇÄÔºâ")
    
    # 3. ÂÄã‰∫∫Âåñ„Éá„Éº„ÇøÂèéÈõÜ„ÉÜ„Çπ„Éà
    print("\nüìù Test 3: ÂÄã‰∫∫Âåñ„Éá„Éº„ÇøÂèéÈõÜ")
    advanced_analytics.log_personalization_event(
        user_id=test_user_id,
        event_type="translation_selection",
        gemini_recommendation="chatgpt",
        user_choice="enhanced",
        language_pair="ja-en",
        translation_context="business_email",
        style_attributes={'formality': 'formal', 'tone': 'professional'},
        confidence_score=0.85
    )
    
    advanced_analytics.log_personalization_event(
        user_id=test_user_id,
        event_type="rejection",
        gemini_recommendation="gemini",
        user_choice="chatgpt",
        language_pair="ja-en",
        rejection_reason="too_casual",
        confidence_score=0.92
    )
    print("‚úÖ ÂÄã‰∫∫Âåñ„Ç§„Éô„É≥„ÉàË®òÈå≤")
    
    # 4. „É°„Éà„É™„ÇØ„ÇπÂèñÂæó„ÉÜ„Çπ„Éà
    print("\nüìä Test 4: „É°„Éà„É™„ÇØ„ÇπÂèñÂæó")
    task292_metrics = advanced_analytics.get_task292_metrics()
    print(f"Task 2.9.2 „É°„Éà„É™„ÇØ„Çπ:")
    print(f"  - Á∑èÊäΩÂá∫Êï∞: {task292_metrics['total_extractions']}")
    print(f"  - ÊàêÂäüÁéá: {task292_metrics['success_rate']:.1f}%")
    print(f"  - Âπ≥Âùá‰ø°È†ºÂ∫¶: {task292_metrics['average_confidence']:.2f}")
    print(f"  - LLMÊàêÂäüÁéá: {task292_metrics['llm_success_rate']:.1f}%")
    
    # 5. APIÁµ±Ë®àÂèñÂæó„ÉÜ„Çπ„Éà
    print("\nüìä Test 5: APIÁµ±Ë®à")
    api_stats = advanced_analytics.get_api_statistics()
    for provider, stats in api_stats.items():
        print(f"{provider}:")
        print(f"  - Á∑èÂëº„Å≥Âá∫„Åó: {stats['total_calls']}")
        print(f"  - ÊàêÂäüÁéá: {stats['success_rate']:.1f}%")
        print(f"  - Âπ≥ÂùáÂøúÁ≠îÊôÇÈñì: {stats['average_response_time']:.0f}ms")
    
    # 6. „Ç¢„É©„Éº„ÉàÁ¢∫Ë™ç
    print("\nüö® Test 6: „Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Ç¢„É©„Éº„Éà")
    alerts = advanced_analytics.get_active_alerts()
    print(f"„Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Ç¢„É©„Éº„ÉàÊï∞: {len(alerts)}")
    for alert in alerts:
        print(f"  - [{alert['severity']}] {alert['message']}")
    
    # 7. ÂÄã‰∫∫Âåñ„Ç§„É≥„Çµ„Ç§„ÉàÂèñÂæó
    print("\nüí° Test 7: ÂÄã‰∫∫Âåñ„Ç§„É≥„Çµ„Ç§„Éà")
    insights = advanced_analytics.get_personalization_insights(test_user_id)
    if insights['available']:
        print(f"„É¶„Éº„Ç∂„Éº {test_user_id} „ÅÆ„Ç§„É≥„Çµ„Ç§„Éà:")
        print(f"  - „Ç§„É≥„Çø„É©„ÇØ„Ç∑„Éß„É≥Êï∞: {insights['interaction_count']}")
        print(f"  - Â•Ω„Åø„ÅÆ„Ç®„É≥„Ç∏„É≥: {insights['preferred_engine']}")
        print(f"  - „Çà„Åè‰Ωø„ÅÜË®ÄË™û„Éö„Ç¢: {insights['top_language_pair']}")
    
    print("\n‚úÖ Phase B-2: È´òÂ∫¶„Å™ÂàÜÊûê„Ç∑„Çπ„ÉÜ„É†„ÉÜ„Çπ„ÉàÂÆå‰∫Ü")
    print("=" * 60)

if __name__ == "__main__":
    test_advanced_analytics()