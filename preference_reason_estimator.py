#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ¯ Task 2.9.2: ä¹–é›¢ç†ç”±è‡ªå‹•æ¨å®šã‚·ã‚¹ãƒ†ãƒ 
=====================================================
ç›®çš„: å€‹äººåŒ–é¸å¥½ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ä¹–é›¢ç†ç”±ã®æ¨å®šã‚¨ãƒ³ã‚¸ãƒ³ã«ã‚ˆã‚Šã€
     ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸€äººã²ã¨ã‚Šã®ç¿»è¨³é¸å¥½ã‚’æ·±ãç†è§£ã—ã€
     å€‹äººåŒ–ç¿»è¨³AIæ§‹ç¯‰ã®ãŸã‚ã®é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹

ã€Task 2.9.1.5åŸºç›¤æ´»ç”¨ã€‘
- EnhancedSatisfactionEstimatorã¨ã®é€£æº
- éæ¥è§¦ãƒ‡ãƒ¼ã‚¿åé›†åŸå‰‡ã®ç¶™æ‰¿
- çœŸã®å€‹äººåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã®æ·±åŒ–
"""

import sqlite3
import json
import logging
import time
import statistics
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from collections import defaultdict, Counter
from dataclasses import dataclass, field
from enum import Enum
import hashlib
import math

# Task 2.9.1.5åŸºç›¤ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from enhanced_satisfaction_estimator import EnhancedSatisfactionEstimator
from recommendation_divergence_detector import (
    EnhancedRecommendationDivergenceDetector,
    DivergenceEvent,
    DivergenceCategory,
    DivergenceImportance
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PreferencePattern(Enum):
    """é¸å¥½ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—"""
    CONSISTENT_ENGINE = "consistent_engine"           # ç‰¹å®šã‚¨ãƒ³ã‚¸ãƒ³ä¸€è²«é¸å¥½
    CONTEXT_ADAPTIVE = "context_adaptive"            # æ–‡è„ˆé©å¿œå‹é¸å¥½
    QUALITY_MAXIMIZER = "quality_maximizer"          # å“è³ªæœ€å¤§åŒ–å‹
    STYLE_FOCUSED = "style_focused"                  # ã‚¹ã‚¿ã‚¤ãƒ«é‡è¦–å‹
    EFFICIENCY_ORIENTED = "efficiency_oriented"      # åŠ¹ç‡é‡è¦–å‹
    EXPERIMENTAL = "experimental"                    # å®Ÿé¨“çš„é¸å¥½
    DOMAIN_SPECIALIST = "domain_specialist"          # å°‚é–€åˆ†é‡ç‰¹åŒ–å‹
    CULTURAL_SENSITIVE = "cultural_sensitive"        # æ–‡åŒ–æ„Ÿå¿œå‹

class LearningConfidence(Enum):
    """å­¦ç¿’ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«"""
    HIGH = "high"           # é«˜ä¿¡é ¼åº¦ï¼ˆ10å›ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ï¼‰
    MEDIUM = "medium"       # ä¸­ä¿¡é ¼åº¦ï¼ˆ5-9å›ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
    LOW = "low"            # ä½ä¿¡é ¼åº¦ï¼ˆ2-4å›ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
    INSUFFICIENT = "insufficient"  # ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼ˆ1å›ä»¥ä¸‹ï¼‰

@dataclass
class PreferenceProfile:
    """å€‹äººåŒ–é¸å¥½ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«"""
    user_id: str
    
    # åŸºæœ¬é¸å¥½ãƒ‘ã‚¿ãƒ¼ãƒ³
    dominant_pattern: PreferencePattern
    secondary_patterns: List[PreferencePattern]
    confidence_level: LearningConfidence
    
    # ã‚¨ãƒ³ã‚¸ãƒ³é¸å¥½
    engine_preferences: Dict[str, float]  # ã‚¨ãƒ³ã‚¸ãƒ³å -> é¸å¥½åº¦ã‚¹ã‚³ã‚¢
    
    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¥é¸å¥½
    context_preferences: Dict[str, Dict[str, float]]
    
    # å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    total_observations: int
    prediction_accuracy: float
    last_updated: str
    
    # åˆ†æçµæœ
    preference_insights: List[str]
    improvement_suggestions: List[str]
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    profile_metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ReasonEstimation:
    """ä¹–é›¢ç†ç”±æ¨å®šçµæœ"""
    estimated_reasons: List[str]
    confidence_scores: Dict[str, float]
    supporting_evidence: Dict[str, List[str]]
    prediction_accuracy: float
    estimation_metadata: Dict[str, Any]

class PreferenceReasonEstimator:
    """å€‹äººåŒ–é¸å¥½ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ä¹–é›¢ç†ç”±ã®æ¨å®šã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, 
                 analytics_db_path: str = "langpont_analytics.db",
                 divergence_db_path: str = "langpont_divergence.db",
                 preference_db_path: str = "langpont_preferences.db"):
        """åˆæœŸåŒ–"""
        self.analytics_db_path = analytics_db_path
        self.divergence_db_path = divergence_db_path
        self.preference_db_path = preference_db_path
        
        # Task 2.9.1.5åŸºç›¤ã‚·ã‚¹ãƒ†ãƒ ã®æ´»ç”¨
        self.satisfaction_estimator = EnhancedSatisfactionEstimator(analytics_db_path)
        self.divergence_detector = EnhancedRecommendationDivergenceDetector(
            analytics_db_path, divergence_db_path
        )
        
        # é¸å¥½å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–
        self._init_preference_database()
        
        # æ¨å®šãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.reason_estimation_weights = {
            'historical_pattern': 0.35,    # éå»ã®é¸å¥½ãƒ‘ã‚¿ãƒ¼ãƒ³
            'context_similarity': 0.25,    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦
            'satisfaction_correlation': 0.20, # æº€è¶³åº¦ç›¸é–¢
            'behavioral_consistency': 0.15, # è¡Œå‹•ä¸€è²«æ€§
            'temporal_trend': 0.05         # æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰
        }
        
        # é¸å¥½ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¤å®šé–¾å€¤
        self.pattern_thresholds = {
            'consistent_engine': 0.7,      # 70%ä»¥ä¸ŠåŒä¸€ã‚¨ãƒ³ã‚¸ãƒ³é¸æŠ
            'context_adaptive': 0.6,       # 60%ä»¥ä¸Šæ–‡è„ˆé©å¿œ
            'quality_maximizer': 0.8,      # 80%ä»¥ä¸Šé«˜æº€è¶³åº¦é¸æŠ
            'style_focused': 0.5,          # 50%ä»¥ä¸Šã‚¹ã‚¿ã‚¤ãƒ«ç†ç”±
        }
        
        logger.info("é¸å¥½ç†ç”±æ¨å®šã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
    
    def _init_preference_database(self):
        """é¸å¥½å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
        with sqlite3.connect(self.preference_db_path) as conn:
            cursor = conn.cursor()
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼é¸å¥½ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS user_preference_profiles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id VARCHAR(100) UNIQUE NOT NULL,
                    
                    -- é¸å¥½ãƒ‘ã‚¿ãƒ¼ãƒ³
                    dominant_pattern VARCHAR(50),
                    secondary_patterns TEXT,
                    confidence_level VARCHAR(20),
                    
                    -- é¸å¥½ãƒ‡ãƒ¼ã‚¿
                    engine_preferences TEXT,
                    context_preferences TEXT,
                    
                    -- å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                    total_observations INTEGER DEFAULT 0,
                    prediction_accuracy FLOAT DEFAULT 0.0,
                    
                    -- åˆ†æçµæœ
                    preference_insights TEXT,
                    improvement_suggestions TEXT,
                    
                    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                    profile_metadata TEXT,
                    
                    -- æ™‚é–“ç®¡ç†
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ç†ç”±æ¨å®šå±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS reason_estimations (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id VARCHAR(100),
                    session_id VARCHAR(100),
                    
                    -- æ¨å®šçµæœ
                    estimated_reasons TEXT,
                    confidence_scores TEXT,
                    supporting_evidence TEXT,
                    prediction_accuracy FLOAT,
                    
                    -- å®Ÿéš›ã®çµæœï¼ˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”¨ï¼‰
                    actual_choice VARCHAR(50),
                    actual_satisfaction FLOAT,
                    
                    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                    estimation_metadata TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # å­¦ç¿’ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS learning_performance (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id VARCHAR(100),
                    
                    -- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
                    accuracy_trend TEXT,
                    pattern_stability FLOAT,
                    prediction_confidence FLOAT,
                    
                    -- å­¦ç¿’çµ±è¨ˆ
                    total_predictions INTEGER DEFAULT 0,
                    correct_predictions INTEGER DEFAULT 0,
                    
                    -- è¨ˆç®—æœŸé–“
                    calculation_period VARCHAR(20),
                    calculated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_preference_user ON user_preference_profiles (user_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_estimation_user ON reason_estimations (user_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_estimation_session ON reason_estimations (session_id)')
            
            conn.commit()
    
    def analyze_behavior_preference_correlation(self, user_data: Dict) -> Dict[str, Any]:
        """
        è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ç›¸é–¢åˆ†æ
        
        Args:
            user_data: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ãƒ»é¸æŠãƒ‡ãƒ¼ã‚¿
            
        Returns:
            è¡Œå‹•ã¨é¸å¥½ã®ç›¸é–¢åˆ†æçµæœ
        """
        correlations = {
            'engine_behavior_correlation': {},
            'satisfaction_behavior_correlation': {},
            'context_behavior_correlation': {},
            'temporal_patterns': {},
            'consistency_metrics': {}
        }
        
        try:
            user_id = user_data.get('user_id')
            if not user_id:
                return correlations
            
            # éå»ã®ä¹–é›¢ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
            divergence_history = self._get_user_divergence_history(user_id, days=90)
            
            if len(divergence_history) < 3:
                correlations['insufficient_data'] = True
                return correlations
            
            # 1. ã‚¨ãƒ³ã‚¸ãƒ³é¸æŠã¨è¡Œå‹•ã®ç›¸é–¢
            correlations['engine_behavior_correlation'] = self._analyze_engine_behavior_correlation(
                divergence_history
            )
            
            # 2. æº€è¶³åº¦ã¨è¡Œå‹•ã®ç›¸é–¢
            correlations['satisfaction_behavior_correlation'] = self._analyze_satisfaction_behavior_correlation(
                divergence_history
            )
            
            # 3. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨è¡Œå‹•ã®ç›¸é–¢
            correlations['context_behavior_correlation'] = self._analyze_context_behavior_correlation(
                divergence_history
            )
            
            # 4. æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
            correlations['temporal_patterns'] = self._analyze_temporal_patterns(
                divergence_history
            )
            
            # 5. ä¸€è²«æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            correlations['consistency_metrics'] = self._calculate_consistency_metrics(
                divergence_history
            )
            
            logger.info(f"è¡Œå‹•é¸å¥½ç›¸é–¢åˆ†æå®Œäº†: user={user_id}, ãƒ‡ãƒ¼ã‚¿æ•°={len(divergence_history)}")
            
        except Exception as e:
            logger.error(f"è¡Œå‹•é¸å¥½ç›¸é–¢åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            correlations['error'] = str(e)
        
        return correlations
    
    def _get_user_divergence_history(self, user_id: str, days: int = 90) -> List[Dict[str, Any]]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¹–é›¢å±¥æ­´å–å¾—"""
        history = []
        
        with sqlite3.connect(self.divergence_db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT session_id, gemini_recommendation, user_choice,
                       gemini_confidence, divergence_importance, divergence_category,
                       satisfaction_score, learning_value,
                       context_data, behavioral_indicators,
                       created_at
                FROM divergence_events
                WHERE user_id = ? 
                  AND created_at >= datetime('now', '-{} days')
                ORDER BY created_at DESC
            '''.format(days), (user_id,))
            
            for row in cursor.fetchall():
                try:
                    context_data = json.loads(row[8]) if row[8] else {}
                    behavioral_indicators = json.loads(row[9]) if row[9] else {}
                    
                    history.append({
                        'session_id': row[0],
                        'gemini_recommendation': row[1],
                        'user_choice': row[2],
                        'gemini_confidence': row[3],
                        'divergence_importance': row[4],
                        'divergence_category': row[5],
                        'satisfaction_score': row[6],
                        'learning_value': row[7],
                        'context_data': context_data,
                        'behavioral_indicators': behavioral_indicators,
                        'created_at': row[10]
                    })
                except json.JSONDecodeError:
                    continue
        
        return history
    
    def _analyze_engine_behavior_correlation(self, history: List[Dict]) -> Dict[str, float]:
        """ã‚¨ãƒ³ã‚¸ãƒ³é¸æŠã¨è¡Œå‹•ã®ç›¸é–¢åˆ†æ"""
        correlations = {}
        
        # ã‚¨ãƒ³ã‚¸ãƒ³åˆ¥ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
        engine_behaviors = defaultdict(list)
        
        for event in history:
            choice = event['user_choice']
            behavioral_indicators = event.get('behavioral_indicators', {})
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¶™ç¶šæ™‚é–“
            session_duration = behavioral_indicators.get('session_duration', 0)
            # ã‚³ãƒ”ãƒ¼è¡Œå‹•å›æ•°
            copy_count = len(behavioral_indicators.get('recent_copy_behaviors', []))
            
            engine_behaviors[choice].append({
                'session_duration': session_duration,
                'copy_count': copy_count,
                'satisfaction': event.get('satisfaction_score', 0)
            })
        
        # å„ã‚¨ãƒ³ã‚¸ãƒ³ã®å¹³å‡è¡Œå‹•æŒ‡æ¨™ã‚’è¨ˆç®—
        for engine, behaviors in engine_behaviors.items():
            if len(behaviors) >= 2:
                avg_duration = statistics.mean([b['session_duration'] for b in behaviors])
                avg_copy_count = statistics.mean([b['copy_count'] for b in behaviors])
                avg_satisfaction = statistics.mean([b['satisfaction'] for b in behaviors])
                
                correlations[f'{engine}_avg_duration'] = avg_duration
                correlations[f'{engine}_avg_copy_count'] = avg_copy_count
                correlations[f'{engine}_avg_satisfaction'] = avg_satisfaction
        
        return correlations
    
    def _analyze_satisfaction_behavior_correlation(self, history: List[Dict]) -> Dict[str, float]:
        """æº€è¶³åº¦ã¨è¡Œå‹•ã®ç›¸é–¢åˆ†æ"""
        if len(history) < 3:
            return {}
        
        satisfactions = [event.get('satisfaction_score', 0) for event in history]
        durations = []
        copy_counts = []
        
        for event in history:
            behavioral_indicators = event.get('behavioral_indicators', {})
            durations.append(behavioral_indicators.get('session_duration', 0))
            copy_counts.append(len(behavioral_indicators.get('recent_copy_behaviors', [])))
        
        correlations = {}
        
        # ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ä¿‚æ•°ã®è¿‘ä¼¼è¨ˆç®—
        if len(satisfactions) == len(durations) and len(satisfactions) > 1:
            # æº€è¶³åº¦ã¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¶™ç¶šæ™‚é–“ã®ç›¸é–¢
            try:
                corr_duration = self._calculate_correlation(satisfactions, durations)
                correlations['satisfaction_duration_correlation'] = corr_duration
            except:
                correlations['satisfaction_duration_correlation'] = 0.0
            
            # æº€è¶³åº¦ã¨ã‚³ãƒ”ãƒ¼å›æ•°ã®ç›¸é–¢
            try:
                corr_copy = self._calculate_correlation(satisfactions, copy_counts)
                correlations['satisfaction_copy_correlation'] = corr_copy
            except:
                correlations['satisfaction_copy_correlation'] = 0.0
        
        return correlations
    
    def _calculate_correlation(self, x: List[float], y: List[float]) -> float:
        """ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ä¿‚æ•°ã®è¨ˆç®—"""
        if len(x) != len(y) or len(x) < 2:
            return 0.0
        
        try:
            n = len(x)
            sum_x = sum(x)
            sum_y = sum(y)
            sum_xy = sum(x[i] * y[i] for i in range(n))
            sum_x2 = sum(xi * xi for xi in x)
            sum_y2 = sum(yi * yi for yi in y)
            
            numerator = n * sum_xy - sum_x * sum_y
            denominator = math.sqrt((n * sum_x2 - sum_x * sum_x) * (n * sum_y2 - sum_y * sum_y))
            
            if denominator == 0:
                return 0.0
            
            return numerator / denominator
        except:
            return 0.0
    
    def _analyze_context_behavior_correlation(self, history: List[Dict]) -> Dict[str, Any]:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨è¡Œå‹•ã®ç›¸é–¢åˆ†æ"""
        context_patterns = defaultdict(list)
        
        for event in history:
            context_data = event.get('context_data', {})
            
            # æ–‡ç« é•·ã«ã‚ˆã‚‹åˆ†é¡
            text_length = context_data.get('text_length', 0)
            length_category = self._categorize_text_length(text_length)
            
            # æŠ€è¡“æ–‡æ›¸ã‹ã©ã†ã‹
            has_technical = context_data.get('has_technical_terms', False)
            tech_category = 'technical' if has_technical else 'general'
            
            context_patterns[length_category].append(event['user_choice'])
            context_patterns[tech_category].append(event['user_choice'])
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        correlations = {}
        for context, choices in context_patterns.items():
            if len(choices) >= 2:
                choice_counter = Counter(choices)
                most_common = choice_counter.most_common(1)[0]
                preference_rate = most_common[1] / len(choices)
                
                correlations[f'{context}_preference'] = {
                    'preferred_engine': most_common[0],
                    'preference_rate': preference_rate,
                    'sample_size': len(choices)
                }
        
        return correlations
    
    def _categorize_text_length(self, length: int) -> str:
        """æ–‡ç« é•·ã®ã‚«ãƒ†ã‚´ãƒ©ã‚¤ã‚º"""
        if length < 100:
            return 'short'
        elif length < 300:
            return 'medium'
        elif length < 600:
            return 'long'
        else:
            return 'very_long'
    
    def _analyze_temporal_patterns(self, history: List[Dict]) -> Dict[str, Any]:
        """æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        if len(history) < 5:
            return {}
        
        # æ™‚ç³»åˆ—ã§ã®é¸æŠãƒ‘ã‚¿ãƒ¼ãƒ³
        recent_choices = [event['user_choice'] for event in history[:10]]  # ç›´è¿‘10å›
        all_choices = [event['user_choice'] for event in history]
        
        # æœ€è¿‘ã®é¸æŠå‚¾å‘
        recent_counter = Counter(recent_choices)
        all_counter = Counter(all_choices)
        
        patterns = {
            'recent_preference': recent_counter.most_common(1)[0] if recent_choices else None,
            'overall_preference': all_counter.most_common(1)[0] if all_choices else None,
            'trend_stability': self._calculate_trend_stability(recent_choices, all_choices)
        }
        
        return patterns
    
    def _calculate_trend_stability(self, recent: List[str], overall: List[str]) -> float:
        """ãƒˆãƒ¬ãƒ³ãƒ‰ã®å®‰å®šæ€§è¨ˆç®—"""
        if not recent or not overall:
            return 0.0
        
        recent_counter = Counter(recent)
        overall_counter = Counter(overall)
        
        # æœ€é »å‡ºã‚¨ãƒ³ã‚¸ãƒ³ãŒä¸€è‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        recent_top = recent_counter.most_common(1)[0][0] if recent_counter else None
        overall_top = overall_counter.most_common(1)[0][0] if overall_counter else None
        
        if recent_top == overall_top:
            return 0.8  # é«˜ã„å®‰å®šæ€§
        else:
            return 0.3  # ä½ã„å®‰å®šæ€§
    
    def _calculate_consistency_metrics(self, history: List[Dict]) -> Dict[str, float]:
        """ä¸€è²«æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""
        if len(history) < 3:
            return {}
        
        # ã‚¨ãƒ³ã‚¸ãƒ³é¸æŠã®ä¸€è²«æ€§
        choices = [event['user_choice'] for event in history]
        choice_counter = Counter(choices)
        max_count = choice_counter.most_common(1)[0][1]
        choice_consistency = max_count / len(choices)
        
        # æº€è¶³åº¦ã®å®‰å®šæ€§
        satisfactions = [event.get('satisfaction_score', 0) for event in history]
        satisfaction_std = statistics.stdev(satisfactions) if len(satisfactions) > 1 else 0
        satisfaction_consistency = max(0, 1 - (satisfaction_std / 100))  # æ­£è¦åŒ–
        
        return {
            'choice_consistency': choice_consistency,
            'satisfaction_consistency': satisfaction_consistency,
            'overall_consistency': (choice_consistency + satisfaction_consistency) / 2
        }
    
    def estimate_divergence_reasons(self, divergence_event: Dict) -> ReasonEstimation:
        """
        ä¹–é›¢ç†ç”±ã®è‡ªå‹•æ¨å®š
        
        Args:
            divergence_event: ä¹–é›¢ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
            
        Returns:
            ç†ç”±æ¨å®šçµæœ
        """
        try:
            user_id = divergence_event.get('user_id')
            if not user_id:
                return ReasonEstimation(
                    estimated_reasons=['insufficient_user_data'],
                    confidence_scores={'insufficient_user_data': 1.0},
                    supporting_evidence={},
                    prediction_accuracy=0.0,
                    estimation_metadata={'error': 'No user ID provided'}
                )
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            user_data = {'user_id': user_id}
            correlations = self.analyze_behavior_preference_correlation(user_data)
            
            # æ¨å®šãƒ­ã‚¸ãƒƒã‚¯
            estimated_reasons = []
            confidence_scores = {}
            supporting_evidence = defaultdict(list)
            
            # 1. éå»ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹æ¨å®š
            pattern_reasons = self._estimate_from_historical_pattern(user_id, divergence_event)
            estimated_reasons.extend(pattern_reasons['reasons'])
            confidence_scores.update(pattern_reasons['confidences'])
            supporting_evidence.update(pattern_reasons['evidence'])
            
            # 2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹æ¨å®š
            context_reasons = self._estimate_from_context_similarity(divergence_event, correlations)
            estimated_reasons.extend(context_reasons['reasons'])
            confidence_scores.update(context_reasons['confidences'])
            supporting_evidence.update(context_reasons['evidence'])
            
            # 3. æº€è¶³åº¦ç›¸é–¢ãƒ™ãƒ¼ã‚¹æ¨å®š
            satisfaction_reasons = self._estimate_from_satisfaction_correlation(
                divergence_event, correlations
            )
            estimated_reasons.extend(satisfaction_reasons['reasons'])
            confidence_scores.update(satisfaction_reasons['confidences'])
            supporting_evidence.update(satisfaction_reasons['evidence'])
            
            # é‡è¤‡æ’é™¤ã¨é‡ã¿ä»˜ã‘
            unique_reasons = list(set(estimated_reasons))
            final_confidence_scores = {
                reason: confidence_scores.get(reason, 0.0) for reason in unique_reasons
            }
            
            # äºˆæ¸¬ç²¾åº¦ã®æ¨å®šï¼ˆéå»ã®æ€§èƒ½ã‹ã‚‰ï¼‰
            prediction_accuracy = self._estimate_prediction_accuracy(user_id)
            
            result = ReasonEstimation(
                estimated_reasons=unique_reasons,
                confidence_scores=final_confidence_scores,
                supporting_evidence=dict(supporting_evidence),
                prediction_accuracy=prediction_accuracy,
                estimation_metadata={
                    'estimation_timestamp': datetime.now().isoformat(),
                    'user_id': user_id,
                    'total_evidence_points': len(supporting_evidence)
                }
            )
            
            # æ¨å®šçµæœã®ä¿å­˜
            self._save_reason_estimation(result, divergence_event)
            
            logger.info(f"ä¹–é›¢ç†ç”±æ¨å®šå®Œäº†: user={user_id}, ç†ç”±æ•°={len(unique_reasons)}")
            return result
            
        except Exception as e:
            logger.error(f"ä¹–é›¢ç†ç”±æ¨å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ReasonEstimation(
                estimated_reasons=['estimation_error'],
                confidence_scores={'estimation_error': 0.0},
                supporting_evidence={},
                prediction_accuracy=0.0,
                estimation_metadata={'error': str(e)}
            )
    
    def _estimate_from_historical_pattern(self, user_id: str, event: Dict) -> Dict[str, Any]:
        """éå»ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ã®æ¨å®š"""
        reasons = []
        confidences = {}
        evidence = defaultdict(list)
        
        # éå»ã®é¡ä¼¼ä¹–é›¢ã‚’æ¤œç´¢
        history = self._get_user_divergence_history(user_id, days=60)
        
        # åŒã˜ã‚¨ãƒ³ã‚¸ãƒ³é¸æŠãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œç´¢
        similar_choices = [
            h for h in history 
            if h['user_choice'] == event.get('user_choice') and
               h['gemini_recommendation'] == event.get('gemini_recommendation')
        ]
        
        if similar_choices:
            # éå»ã®ç†ç”±ã‚’åˆ†æ
            categories = [h.get('divergence_category') for h in similar_choices]
            category_counter = Counter(categories)
            
            if category_counter:
                most_common_category = category_counter.most_common(1)[0]
                reasons.append(f"historical_pattern_{most_common_category[0]}")
                confidences[f"historical_pattern_{most_common_category[0]}"] = (
                    most_common_category[1] / len(similar_choices)
                ) * self.reason_estimation_weights['historical_pattern']
                
                evidence[f"historical_pattern_{most_common_category[0]}"].append(
                    f"éå»{len(similar_choices)}å›ã®é¡ä¼¼é¸æŠã§{most_common_category[1]}å›åŒæ§˜ã®ç†ç”±"
                )
        
        return {
            'reasons': reasons,
            'confidences': confidences,
            'evidence': evidence
        }
    
    def _estimate_from_context_similarity(self, event: Dict, correlations: Dict) -> Dict[str, Any]:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã‹ã‚‰ã®æ¨å®š"""
        reasons = []
        confidences = {}
        evidence = defaultdict(list)
        
        context_data = event.get('context_data', {})
        
        # æ–‡ç« é•·ã«ã‚ˆã‚‹æ¨å®š
        text_length = context_data.get('text_length', 0)
        length_category = self._categorize_text_length(text_length)
        
        context_correlations = correlations.get('context_behavior_correlation', {})
        length_preference = context_correlations.get(f'{length_category}_preference')
        
        if length_preference and length_preference['preferred_engine'] == event.get('user_choice'):
            reasons.append(f"context_length_preference_{length_category}")
            confidences[f"context_length_preference_{length_category}"] = (
                length_preference['preference_rate'] * 
                self.reason_estimation_weights['context_similarity']
            )
            evidence[f"context_length_preference_{length_category}"].append(
                f"{length_category}æ–‡æ›¸ã§{length_preference['preference_rate']:.1%}ã®é¸å¥½ç‡"
            )
        
        # æŠ€è¡“æ–‡æ›¸ã«ã‚ˆã‚‹æ¨å®š
        if context_data.get('has_technical_terms'):
            tech_preference = context_correlations.get('technical_preference')
            if tech_preference and tech_preference['preferred_engine'] == event.get('user_choice'):
                reasons.append("technical_document_preference")
                confidences["technical_document_preference"] = (
                    tech_preference['preference_rate'] * 
                    self.reason_estimation_weights['context_similarity']
                )
                evidence["technical_document_preference"].append(
                    f"æŠ€è¡“æ–‡æ›¸ã§{tech_preference['preference_rate']:.1%}ã®é¸å¥½ç‡"
                )
        
        return {
            'reasons': reasons,
            'confidences': confidences,
            'evidence': evidence
        }
    
    def _estimate_from_satisfaction_correlation(self, event: Dict, correlations: Dict) -> Dict[str, Any]:
        """æº€è¶³åº¦ç›¸é–¢ã‹ã‚‰ã®æ¨å®š"""
        reasons = []
        confidences = {}
        evidence = defaultdict(list)
        
        satisfaction_score = event.get('satisfaction_score', 0)
        satisfaction_correlations = correlations.get('satisfaction_behavior_correlation', {})
        
        # é«˜æº€è¶³åº¦ã§ã®é¸æŠãƒ‘ã‚¿ãƒ¼ãƒ³
        if satisfaction_score >= 80:
            engine_correlations = correlations.get('engine_behavior_correlation', {})
            user_choice = event.get('user_choice')
            
            # ãã®ã‚¨ãƒ³ã‚¸ãƒ³ã§ã®å¹³å‡æº€è¶³åº¦ã‚’ãƒã‚§ãƒƒã‚¯
            avg_satisfaction_key = f'{user_choice}_avg_satisfaction'
            if avg_satisfaction_key in engine_correlations:
                avg_satisfaction = engine_correlations[avg_satisfaction_key]
                if avg_satisfaction >= 75:
                    reasons.append("high_satisfaction_engine_preference")
                    confidences["high_satisfaction_engine_preference"] = (
                        (avg_satisfaction / 100) * 
                        self.reason_estimation_weights['satisfaction_correlation']
                    )
                    evidence["high_satisfaction_engine_preference"].append(
                        f"{user_choice}ã§å¹³å‡æº€è¶³åº¦{avg_satisfaction:.1f}ç‚¹"
                    )
        
        return {
            'reasons': reasons,
            'confidences': confidences,
            'evidence': evidence
        }
    
    def _estimate_prediction_accuracy(self, user_id: str) -> float:
        """äºˆæ¸¬ç²¾åº¦ã®æ¨å®š"""
        try:
            with sqlite3.connect(self.preference_db_path) as conn:
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT AVG(prediction_accuracy)
                    FROM reason_estimations
                    WHERE user_id = ?
                      AND created_at >= datetime('now', '-30 days')
                ''', (user_id,))
                
                result = cursor.fetchone()
                if result and result[0]:
                    return result[0]
                else:
                    return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        except:
            return 0.5
    
    def learn_personalization_patterns(self, user_sessions: List[Dict]) -> PreferenceProfile:
        """
        å€‹äººåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
        
        Args:
            user_sessions: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            å­¦ç¿’ã•ã‚ŒãŸé¸å¥½ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        """
        if not user_sessions:
            raise ValueError("ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™")
        
        user_id = user_sessions[0].get('user_id')
        if not user_id:
            raise ValueError("ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒå¿…è¦ã§ã™")
        
        # éå»ã®ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆ
        user_data = {'user_id': user_id}
        correlations = self.analyze_behavior_preference_correlation(user_data)
        
        # ã‚¨ãƒ³ã‚¸ãƒ³é¸å¥½ã®å­¦ç¿’
        engine_preferences = self._learn_engine_preferences(user_sessions, correlations)
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¥é¸å¥½ã®å­¦ç¿’
        context_preferences = self._learn_context_preferences(user_sessions, correlations)
        
        # æ”¯é…çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å®š
        dominant_pattern = self._identify_dominant_pattern(engine_preferences, context_preferences)
        
        # å‰¯æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å®š
        secondary_patterns = self._identify_secondary_patterns(engine_preferences, context_preferences)
        
        # ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«ã®æ±ºå®š
        confidence_level = self._determine_confidence_level(len(user_sessions))
        
        # äºˆæ¸¬ç²¾åº¦ã®è¨ˆç®—
        prediction_accuracy = self._calculate_prediction_accuracy(user_id)
        
        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆã¨æ”¹å–„ææ¡ˆã®ç”Ÿæˆ
        insights = self._generate_preference_insights(engine_preferences, context_preferences)
        suggestions = self._generate_improvement_suggestions(engine_preferences, context_preferences)
        
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
        profile = PreferenceProfile(
            user_id=user_id,
            dominant_pattern=dominant_pattern,
            secondary_patterns=secondary_patterns,
            confidence_level=confidence_level,
            engine_preferences=engine_preferences,
            context_preferences=context_preferences,
            total_observations=len(user_sessions),
            prediction_accuracy=prediction_accuracy,
            last_updated=datetime.now().isoformat(),
            preference_insights=insights,
            improvement_suggestions=suggestions,
            profile_metadata={
                'learning_timestamp': datetime.now().isoformat(),
                'data_sources': ['user_sessions', 'divergence_history'],
                'confidence_factors': {
                    'data_volume': len(user_sessions),
                    'pattern_consistency': correlations.get('consistency_metrics', {}).get('overall_consistency', 0)
                }
            }
        )
        
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
        self._save_preference_profile(profile)
        
        logger.info(f"å€‹äººåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’å®Œäº†: user={user_id}, pattern={dominant_pattern.value}")
        return profile
    
    def _learn_engine_preferences(self, sessions: List[Dict], correlations: Dict) -> Dict[str, float]:
        """ã‚¨ãƒ³ã‚¸ãƒ³é¸å¥½ã®å­¦ç¿’"""
        engine_scores = defaultdict(float)
        engine_counts = defaultdict(int)
        
        for session in sessions:
            choice = session.get('user_choice')
            satisfaction = session.get('satisfaction_score', 0)
            
            if choice:
                engine_scores[choice] += satisfaction
                engine_counts[choice] += 1
        
        # å¹³å‡æº€è¶³åº¦ãƒ™ãƒ¼ã‚¹ã®é¸å¥½ã‚¹ã‚³ã‚¢
        preferences = {}
        for engine in engine_scores:
            if engine_counts[engine] > 0:
                avg_satisfaction = engine_scores[engine] / engine_counts[engine]
                # 0.0-1.0ã®ç¯„å›²ã«æ­£è¦åŒ–
                preferences[engine] = avg_satisfaction / 100.0
        
        return preferences
    
    def _learn_context_preferences(self, sessions: List[Dict], correlations: Dict) -> Dict[str, Dict[str, float]]:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¥é¸å¥½ã®å­¦ç¿’"""
        context_preferences = defaultdict(lambda: defaultdict(float))
        context_counts = defaultdict(lambda: defaultdict(int))
        
        for session in sessions:
            choice = session.get('user_choice')
            satisfaction = session.get('satisfaction_score', 0)
            context_data = session.get('context_data', {})
            
            if choice:
                # æ–‡ç« é•·ã‚«ãƒ†ã‚´ãƒª
                text_length = context_data.get('text_length', 0)
                length_category = self._categorize_text_length(text_length)
                
                context_preferences[length_category][choice] += satisfaction
                context_counts[length_category][choice] += 1
                
                # æŠ€è¡“æ–‡æ›¸ã‚«ãƒ†ã‚´ãƒª
                tech_category = 'technical' if context_data.get('has_technical_terms') else 'general'
                context_preferences[tech_category][choice] += satisfaction
                context_counts[tech_category][choice] += 1
        
        # å¹³å‡åŒ–
        final_preferences = {}
        for context, engines in context_preferences.items():
            final_preferences[context] = {}
            for engine, total_satisfaction in engines.items():
                count = context_counts[context][engine]
                if count > 0:
                    final_preferences[context][engine] = total_satisfaction / count / 100.0
        
        return final_preferences
    
    def _identify_dominant_pattern(self, engine_prefs: Dict, context_prefs: Dict) -> PreferencePattern:
        """æ”¯é…çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å®š"""
        # ã‚¨ãƒ³ã‚¸ãƒ³ä¸€è²«æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        if engine_prefs:
            max_engine_score = max(engine_prefs.values())
            if max_engine_score >= self.pattern_thresholds['consistent_engine']:
                return PreferencePattern.CONSISTENT_ENGINE
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé©å¿œæ€§ã‚’ãƒã‚§ãƒƒã‚¯
        if len(context_prefs) >= 2:
            return PreferencePattern.CONTEXT_ADAPTIVE
        
        # å“è³ªæœ€å¤§åŒ–ã‚’ãƒã‚§ãƒƒã‚¯
        if engine_prefs:
            avg_score = sum(engine_prefs.values()) / len(engine_prefs)
            if avg_score >= self.pattern_thresholds['quality_maximizer']:
                return PreferencePattern.QUALITY_MAXIMIZER
        
        return PreferencePattern.EXPERIMENTAL
    
    def _identify_secondary_patterns(self, engine_prefs: Dict, context_prefs: Dict) -> List[PreferencePattern]:
        """å‰¯æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å®š"""
        patterns = []
        
        # åŠ¹ç‡é‡è¦–ã®åˆ¤å®šï¼ˆé«˜é€Ÿé¸æŠï¼‰
        if engine_prefs and len(engine_prefs) <= 2:
            patterns.append(PreferencePattern.EFFICIENCY_ORIENTED)
        
        # å°‚é–€åˆ†é‡ç‰¹åŒ–ã®åˆ¤å®š
        if 'technical' in context_prefs:
            patterns.append(PreferencePattern.DOMAIN_SPECIALIST)
        
        return patterns[:2]  # æœ€å¤§2ã¤ã¾ã§
    
    def _determine_confidence_level(self, observation_count: int) -> LearningConfidence:
        """ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«ã®æ±ºå®š"""
        if observation_count >= 10:
            return LearningConfidence.HIGH
        elif observation_count >= 5:
            return LearningConfidence.MEDIUM
        elif observation_count >= 2:
            return LearningConfidence.LOW
        else:
            return LearningConfidence.INSUFFICIENT
    
    def _calculate_prediction_accuracy(self, user_id: str) -> float:
        """äºˆæ¸¬ç²¾åº¦ã®è¨ˆç®—"""
        # ç°¡æ˜“å®Ÿè£…: å®Ÿéš›ã«ã¯éå»ã®äºˆæ¸¬ã¨å®Ÿç¸¾ã®æ¯”è¼ƒãŒå¿…è¦
        return 0.7  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
    
    def _generate_preference_insights(self, engine_prefs: Dict, context_prefs: Dict) -> List[str]:
        """é¸å¥½ã‚¤ãƒ³ã‚µã‚¤ãƒˆã®ç”Ÿæˆ"""
        insights = []
        
        if engine_prefs:
            favorite_engine = max(engine_prefs, key=engine_prefs.get)
            favorite_score = engine_prefs[favorite_engine]
            insights.append(f"{favorite_engine}ã‚¨ãƒ³ã‚¸ãƒ³ã‚’å¼·ãé¸å¥½ï¼ˆã‚¹ã‚³ã‚¢: {favorite_score:.2f}ï¼‰")
        
        if 'technical' in context_prefs and 'general' in context_prefs:
            insights.append("æŠ€è¡“æ–‡æ›¸ã¨ä¸€èˆ¬æ–‡æ›¸ã§ç•°ãªã‚‹é¸å¥½ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¤ºã™")
        
        return insights
    
    def _generate_improvement_suggestions(self, engine_prefs: Dict, context_prefs: Dict) -> List[str]:
        """æ”¹å–„ææ¡ˆã®ç”Ÿæˆ"""
        suggestions = []
        
        if engine_prefs and len(engine_prefs) < 2:
            suggestions.append("ä»–ã®ç¿»è¨³ã‚¨ãƒ³ã‚¸ãƒ³ã‚‚è©¦ã™ã“ã¨ã§ã‚ˆã‚Šé©åˆ‡ãªç¿»è¨³ã‚’ç™ºè¦‹ã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        
        if not context_prefs:
            suggestions.append("ã‚ˆã‚Šå¤šæ§˜ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã®åˆ©ç”¨ã«ã‚ˆã‚Šã€é¸å¥½ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæ˜ç¢ºã«ãªã‚Šã¾ã™")
        
        return suggestions
    
    def improve_prediction_accuracy(self, feedback_data: List[Dict]) -> float:
        """
        äºˆæ¸¬ç²¾åº¦å‘ä¸Šæ©Ÿèƒ½
        
        Args:
            feedback_data: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            æ”¹å–„å¾Œã®äºˆæ¸¬ç²¾åº¦
        """
        if not feedback_data:
            return 0.0
        
        correct_predictions = 0
        total_predictions = len(feedback_data)
        
        for feedback in feedback_data:
            predicted_choice = feedback.get('predicted_choice')
            actual_choice = feedback.get('actual_choice')
            
            if predicted_choice == actual_choice:
                correct_predictions += 1
        
        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0.0
        
        # å­¦ç¿’ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æ›´æ–°
        user_id = feedback_data[0].get('user_id') if feedback_data else None
        if user_id:
            self._update_learning_performance(user_id, accuracy, total_predictions, correct_predictions)
        
        logger.info(f"äºˆæ¸¬ç²¾åº¦å‘ä¸Š: user={user_id}, ç²¾åº¦={accuracy:.3f}")
        return accuracy
    
    def _save_reason_estimation(self, estimation: ReasonEstimation, event: Dict):
        """ç†ç”±æ¨å®šçµæœã®ä¿å­˜"""
        with sqlite3.connect(self.preference_db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO reason_estimations (
                    user_id, session_id, estimated_reasons, confidence_scores,
                    supporting_evidence, prediction_accuracy, actual_choice,
                    actual_satisfaction, estimation_metadata
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                event.get('user_id'),
                event.get('session_id'),
                json.dumps(estimation.estimated_reasons),
                json.dumps(estimation.confidence_scores),
                json.dumps(estimation.supporting_evidence),
                estimation.prediction_accuracy,
                event.get('user_choice'),
                event.get('satisfaction_score'),
                json.dumps(estimation.estimation_metadata)
            ))
            
            conn.commit()
    
    def _save_preference_profile(self, profile: PreferenceProfile):
        """é¸å¥½ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜"""
        with sqlite3.connect(self.preference_db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO user_preference_profiles (
                    user_id, dominant_pattern, secondary_patterns, confidence_level,
                    engine_preferences, context_preferences, total_observations,
                    prediction_accuracy, preference_insights, improvement_suggestions,
                    profile_metadata, last_updated
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                profile.user_id,
                profile.dominant_pattern.value,
                json.dumps([p.value for p in profile.secondary_patterns]),
                profile.confidence_level.value,
                json.dumps(profile.engine_preferences),
                json.dumps(profile.context_preferences),
                profile.total_observations,
                profile.prediction_accuracy,
                json.dumps(profile.preference_insights),
                json.dumps(profile.improvement_suggestions),
                json.dumps(profile.profile_metadata),
                profile.last_updated
            ))
            
            conn.commit()
    
    def _update_learning_performance(self, user_id: str, accuracy: float, 
                                   total: int, correct: int):
        """å­¦ç¿’ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æ›´æ–°"""
        with sqlite3.connect(self.preference_db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO learning_performance (
                    user_id, accuracy_trend, pattern_stability,
                    prediction_confidence, total_predictions, correct_predictions,
                    calculation_period
                ) VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                user_id,
                json.dumps([accuracy]),  # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ‹¡å¼µäºˆå®š
                0.7,  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
                accuracy,
                total,
                correct,
                '30days'
            ))
            
            conn.commit()


# ãƒ†ã‚¹ãƒˆç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°
if __name__ == "__main__":
    print("ğŸ¯ é¸å¥½ç†ç”±æ¨å®šã‚¨ãƒ³ã‚¸ãƒ³ - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print("=" * 60)
    
    estimator = PreferenceReasonEstimator()
    
    # ãƒ†ã‚¹ãƒˆç”¨ä¹–é›¢ã‚¤ãƒ™ãƒ³ãƒˆ
    test_event = {
        'user_id': 'test_user_292',
        'session_id': 'test_session_292',
        'gemini_recommendation': 'enhanced',
        'user_choice': 'gemini',
        'satisfaction_score': 85.0,
        'context_data': {
            'text_length': 300,
            'has_technical_terms': True
        }
    }
    
    # ç†ç”±æ¨å®šãƒ†ã‚¹ãƒˆ
    estimation = estimator.estimate_divergence_reasons(test_event)
    
    print(f"âœ… ç†ç”±æ¨å®šçµæœ:")
    print(f"  æ¨å®šç†ç”±: {estimation.estimated_reasons}")
    print(f"  ä¿¡é ¼åº¦: {estimation.confidence_scores}")
    print(f"  äºˆæ¸¬ç²¾åº¦: {estimation.prediction_accuracy:.3f}")
    
    # è¡Œå‹•ç›¸é–¢åˆ†æãƒ†ã‚¹ãƒˆ
    correlations = estimator.analyze_behavior_preference_correlation({'user_id': 'test_user_292'})
    print(f"\nğŸ“Š è¡Œå‹•ç›¸é–¢åˆ†æ:")
    print(f"  ã‚¨ãƒ³ã‚¸ãƒ³è¡Œå‹•ç›¸é–¢: {len(correlations.get('engine_behavior_correlation', {}))}")
    print(f"  æº€è¶³åº¦è¡Œå‹•ç›¸é–¢: {len(correlations.get('satisfaction_behavior_correlation', {}))}")
    
    print("\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº† - é¸å¥½ç†ç”±æ¨å®šã‚¨ãƒ³ã‚¸ãƒ³æ­£å¸¸å‹•ä½œ")