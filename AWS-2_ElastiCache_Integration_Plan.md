# â˜ï¸ AWS-2: ElastiCache çµ±åˆè¨ˆç”»

**è¨ˆç”»ä½œæˆæ—¥**: 2025å¹´7æœˆ25æ—¥  
**å¯¾è±¡**: LangPont ElastiCache Redisçµ±åˆè¨­è¨ˆ  
**è¨­è¨ˆè€…**: Claude Code  

## ğŸ¯ çµ±åˆç›®æ¨™

### å¯ç”¨æ€§ç›®æ¨™
- **Multi-AZå¯¾å¿œ**: 99.9%ç¨¼åƒç‡ä¿è¨¼
- **è‡ªå‹•ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼**: < 30ç§’
- **ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–**: Criticalé …ç›®100%ä¿è­·

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™
- **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·**: < 1msï¼ˆsame AZå†…ï¼‰
- **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: 10,000 ops/secå¯¾å¿œ
- **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æœ€é©åŒ–**: VPCå†…ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆé€šä¿¡

---

## ğŸ—ï¸ 1. ElastiCacheæ¥ç¶šæ§‹æˆè¨­è¨ˆ

### 1.1 ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ§‹æˆè¨­è¨ˆ

#### Redis Cluster Modeï¼ˆæ¨å¥¨æ§‹æˆï¼‰
```yaml
# ElastiCache Redis Clusterè¨­å®š
ElastiCacheReplicationGroup:
  ReplicationGroupId: langpont-session-cluster
  Description: "LangPont Session Store - Redis Cluster"
  
  # ã‚¨ãƒ³ã‚¸ãƒ³è¨­å®š
  Engine: redis
  EngineVersion: "7.0"
  ParameterGroupName: langpont-redis-params
  
  # ãƒãƒ¼ãƒ‰è¨­å®š
  NodeType: cache.t3.medium
  NumCacheClusters: 3           # 3ãƒãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
  MultiAZEnabled: true          # Multi-AZè‡ªå‹•é…ç½®
  
  # é«˜å¯ç”¨æ€§è¨­å®š
  AutomaticFailoverEnabled: true
  DataTieringEnabled: false     # ãƒ¡ãƒ¢ãƒªå°‚ç”¨ï¼ˆé«˜é€Ÿï¼‰
  
  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
  AuthToken: !Ref RedisAuthToken
  TransitEncryptionEnabled: true
  AtRestEncryptionEnabled: true
  
  # ã‚µãƒ–ãƒãƒƒãƒˆãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
  CacheSubnetGroupName: !Ref RedisSubnetGroup
  SecurityGroupIds:
    - !Ref RedisSecurityGroup
  
  # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®š
  SnapshotRetentionLimit: 7
  SnapshotWindow: "03:00-05:00"  # UTCï¼ˆæ—¥æœ¬æ™‚é–“12-14æ™‚ï¼‰
  
  # ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹è¨­å®š
  PreferredMaintenanceWindow: "sun:05:00-sun:07:00"
  
  # ãƒ­ã‚°è¨­å®š
  LogDeliveryConfigurations:
    - DestinationType: cloudwatch-logs
      DestinationDetails:
        LogGroup: /aws/elasticache/langpont-session
      LogFormat: json
      LogType: slow-log
```

#### ä»£æ›¿æ§‹æˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«æ§‹æˆï¼‰
```yaml
# Single-Nodeæ§‹æˆï¼ˆé–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆç”¨ï¼‰
ElastiCacheCluster:
  ClusterName: langpont-session-dev
  Engine: redis
  CacheNodeType: cache.t3.micro
  NumCacheNodes: 1
  AZMode: single-az
  
  # é–‹ç™ºç”¨è¨­å®š
  AuthToken: !Ref DevRedisAuthToken
  TransitEncryptionEnabled: false  # é–‹ç™ºç’°å¢ƒã¯ç„¡åŠ¹
  AtRestEncryptionEnabled: false
```

### 1.2 ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç®¡ç†

#### Primary/Replicaæ§‹æˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
```python
# AWS ElastiCache ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®š
ELASTICACHE_ENDPOINTS = {
    'production': {
        # Configuration Endpointï¼ˆæ¨å¥¨ï¼‰
        'config_endpoint': 'langpont-session-cluster.abc123.cache.amazonaws.com:6379',
        
        # Primary Endpointï¼ˆæ›¸ãè¾¼ã¿ï¼‰
        'primary_endpoint': 'langpont-session-cluster-001.abc123.cache.amazonaws.com:6379',
        
        # Reader Endpointï¼ˆèª­ã¿å–ã‚Šï¼‰
        'reader_endpoint': 'langpont-session-cluster-ro.abc123.cache.amazonaws.com:6379',
    },
    
    'staging': {
        'config_endpoint': 'langpont-session-staging.def456.cache.amazonaws.com:6379',
    },
    
    'development': {
        'single_endpoint': 'langpont-session-dev.ghi789.cache.amazonaws.com:6379',
    }
}

# ç’°å¢ƒåˆ¥æ¥ç¶šè¨­å®š
class ElastiCacheConnectionManager:
    def __init__(self, environment='production'):
        self.environment = environment
        self.endpoints = ELASTICACHE_ENDPOINTS[environment]
        
    def get_primary_client(self):
        """æ›¸ãè¾¼ã¿ç”¨Redisæ¥ç¶š"""
        import redis
        
        if self.environment == 'production':
            return redis.Redis(
                host=self.endpoints['primary_endpoint'].split(':')[0],
                port=int(self.endpoints['primary_endpoint'].split(':')[1]),
                password=os.getenv('ELASTICACHE_AUTH_TOKEN'),
                ssl=True,
                ssl_cert_reqs='required',
                ssl_check_hostname=False,
                decode_responses=True,
                
                # æ¥ç¶šãƒ—ãƒ¼ãƒ«æœ€é©åŒ–
                connection_pool_class=redis.BlockingConnectionPool,
                max_connections=50,
                retry_on_timeout=True,
                socket_connect_timeout=5,
                socket_timeout=10,
            )
        else:
            # é–‹ç™ºãƒ»ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒ
            endpoint = self.endpoints.get('config_endpoint', self.endpoints.get('single_endpoint'))
            host, port = endpoint.split(':')
            
            return redis.Redis(
                host=host,
                port=int(port),
                password=os.getenv('ELASTICACHE_AUTH_TOKEN'),
                decode_responses=True,
                max_connections=20,
            )
    
    def get_replica_client(self):
        """èª­ã¿å–ã‚Šç”¨Redisæ¥ç¶š"""
        if self.environment == 'production' and 'reader_endpoint' in self.endpoints:
            import redis
            return redis.Redis(
                host=self.endpoints['reader_endpoint'].split(':')[0],
                port=int(self.endpoints['reader_endpoint'].split(':')[1]),
                password=os.getenv('ELASTICACHE_AUTH_TOKEN'),
                ssl=True,
                decode_responses=True,
                max_connections=30,
            )
        else:
            # èª­ã¿å–ã‚Šå°‚ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒãªã„å ´åˆã¯Primaryã‚’ä½¿ç”¨
            return self.get_primary_client()
```

### 1.3 VPCãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­è¨ˆ

#### ã‚µãƒ–ãƒãƒƒãƒˆé…ç½®
```yaml
# Rediså°‚ç”¨ã‚µãƒ–ãƒãƒƒãƒˆã‚°ãƒ«ãƒ¼ãƒ—
RedisSubnetGroup:
  CacheSubnetGroupName: langpont-redis-subnet-group
  Description: "Subnet group for LangPont Redis Cache"
  SubnetIds:
    - !Ref PrivateSubnet1  # ap-northeast-1a
    - !Ref PrivateSubnet2  # ap-northeast-1c  
    - !Ref PrivateSubnet3  # ap-northeast-1d

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚°ãƒ«ãƒ¼ãƒ—
RedisSecurityGroup:
  GroupName: langpont-redis-sg
  GroupDescription: "Security group for LangPont Redis ElastiCache"
  VpcId: !Ref VPC
  
  SecurityGroupIngress:
    # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹
    - IpProtocol: tcp
      FromPort: 6379
      ToPort: 6379
      SourceSecurityGroupId: !Ref ApplicationSecurityGroup
      Description: "Redis access from application servers"
    
    # ç®¡ç†è€…ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆè¸ã¿å°ã‚µãƒ¼ãƒãƒ¼çµŒç”±ï¼‰
    - IpProtocol: tcp
      FromPort: 6379
      ToPort: 6379
      SourceSecurityGroupId: !Ref BastionSecurityGroup
      Description: "Redis access from bastion hosts"
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…é€šä¿¡
    - IpProtocol: tcp
      FromPort: 6379
      ToPort: 6379
      SourceSecurityGroupId: !Self
      Description: "Redis cluster internal communication"
```

---

## ğŸ”„ 2. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥

### 2.1 Rediséšœå®³æ™‚ã®å‹•ä½œä»•æ§˜

#### æ®µéšçš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­è¨ˆ
```python
class GracefulDegradationStrategy:
    """
    Rediséšœå®³æ™‚ã®Graceful Degradationå®Ÿè£…
    """
    
    def __init__(self):
        self.fallback_levels = {
            'level_0': 'normal_operation',      # æ­£å¸¸å‹•ä½œ
            'level_1': 'replica_fallback',     # ãƒ¬ãƒ—ãƒªã‚«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            'level_2': 'local_cache_fallback', # ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            'level_3': 'database_fallback',    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            'level_4': 'minimal_operation',    # æœ€å°å‹•ä½œãƒ¢ãƒ¼ãƒ‰
        }
        self.current_level = 'level_0'
        
    def check_redis_health(self) -> bool:
        """Redisæ¥ç¶šå¥å…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        try:
            # Primaryæ¥ç¶šãƒã‚§ãƒƒã‚¯
            redis_primary.ping()
            return True
        except redis.ConnectionError:
            return False
        except redis.TimeoutError:
            return False
    
    def get_session_with_fallback(self, session_id: str, category: str) -> dict:
        """
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä»˜ãã‚»ãƒƒã‚·ãƒ§ãƒ³å–å¾—
        """
        # Level 0: æ­£å¸¸å‹•ä½œï¼ˆPrimary Redisï¼‰
        if self.current_level == 'level_0':
            try:
                return self._get_session_from_redis(session_id, category, 'primary')
            except Exception as e:
                logger.warning(f"Primary Redis failed: {e}")
                self.current_level = 'level_1'
        
        # Level 1: ãƒ¬ãƒ—ãƒªã‚«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if self.current_level == 'level_1':
            try:
                return self._get_session_from_redis(session_id, category, 'replica')
            except Exception as e:
                logger.warning(f"Replica Redis failed: {e}")
                self.current_level = 'level_2'
        
        # Level 2: ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if self.current_level == 'level_2':
            cached_data = self._get_session_from_local_cache(session_id, category)
            if cached_data:
                return cached_data
            else:
                self.current_level = 'level_3'
        
        # Level 3: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆèªè¨¼æƒ…å ±ã®ã¿ï¼‰
        if self.current_level == 'level_3':
            if category == 'auth':
                return self._get_session_from_database(session_id)
            else:
                self.current_level = 'level_4'
        
        # Level 4: æœ€å°å‹•ä½œãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
        return self._get_default_session_data(category)
    
    def set_session_with_fallback(self, session_id: str, category: str, data: dict) -> bool:
        """
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä»˜ãã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
        """
        success = False
        
        # Redisæ›¸ãè¾¼ã¿è©¦è¡Œ
        try:
            if self.current_level in ['level_0', 'level_1']:
                redis_primary.hset(f"session:{category}:{session_id}", mapping=data)
                success = True
        except Exception as e:
            logger.warning(f"Redis write failed: {e}")
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚‚ä¿å­˜ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰
        self._set_session_to_local_cache(session_id, category, data)
        
        # Criticalæƒ…å ±ã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚‚ä¿å­˜
        if category == 'auth':
            self._set_session_to_database(session_id, data)
        
        return success
```

#### ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
```python
import threading
import time
from typing import Dict, Optional

class LocalSessionCache:
    """
    Rediséšœå®³æ™‚ã®ãƒ­ãƒ¼ã‚«ãƒ«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    """
    
    def __init__(self, max_size: int = 1000, ttl: int = 3600):
        self.cache: Dict[str, Dict] = {}
        self.access_times: Dict[str, float] = {}
        self.max_size = max_size
        self.default_ttl = ttl
        self._lock = threading.RLock()
        
        # å®šæœŸã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ãƒ¬ãƒƒãƒ‰
        self._cleanup_thread = threading.Thread(target=self._cleanup_expired, daemon=True)
        self._cleanup_thread.start()
    
    def get(self, key: str) -> Optional[Dict]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        with self._lock:
            if key in self.cache:
                # TTLãƒã‚§ãƒƒã‚¯
                if time.time() - self.access_times[key] < self.default_ttl:
                    self.access_times[key] = time.time()  # ã‚¢ã‚¯ã‚»ã‚¹æ™‚åˆ»æ›´æ–°
                    return self.cache[key].copy()
                else:
                    # æœŸé™åˆ‡ã‚Œå‰Šé™¤
                    del self.cache[key]
                    del self.access_times[key]
            return None
    
    def set(self, key: str, data: Dict, ttl: Optional[int] = None) -> None:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãƒ‡ãƒ¼ã‚¿è¨­å®š"""
        with self._lock:
            # ã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯
            if len(self.cache) >= self.max_size and key not in self.cache:
                self._evict_lru()
            
            self.cache[key] = data.copy()
            self.access_times[key] = time.time()
    
    def _evict_lru(self) -> None:
        """LRUå‰Šé™¤"""
        if self.access_times:
            lru_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
            del self.cache[lru_key]
            del self.access_times[lru_key]
    
    def _cleanup_expired(self) -> None:
        """æœŸé™åˆ‡ã‚Œã‚¨ãƒ³ãƒˆãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        while True:
            time.sleep(300)  # 5åˆ†æ¯
            current_time = time.time()
            
            with self._lock:
                expired_keys = [
                    key for key, access_time in self.access_times.items()
                    if current_time - access_time > self.default_ttl
                ]
                
                for key in expired_keys:
                    del self.cache[key]
                    del self.access_times[key]

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
local_session_cache = LocalSessionCache(max_size=1000, ttl=1800)  # 30åˆ†TTL
```

### 2.2 Graceful Degradationå®Ÿè£…æ–¹é‡

#### æ©Ÿèƒ½ãƒ¬ãƒ™ãƒ«åˆ¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
```python
class FeatureLevelFallback:
    """
    æ©Ÿèƒ½ãƒ¬ãƒ™ãƒ«åˆ¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…
    """
    
    FEATURE_PRIORITIES = {
        # Criticalï¼ˆã‚µãƒ¼ãƒ“ã‚¹ç¶™ç¶šã«å¿…é ˆï¼‰
        'authentication': 'critical',
        'csrf_protection': 'critical',
        'session_security': 'critical',
        
        # Highï¼ˆä¸»è¦æ©Ÿèƒ½ï¼‰
        'translation_state': 'high',
        'engine_selection': 'high',
        'usage_tracking': 'high',
        
        # Mediumï¼ˆåˆ©ä¾¿æ€§æ©Ÿèƒ½ï¼‰
        'language_preference': 'medium',
        'ui_settings': 'medium',
        'statistics': 'medium',
        
        # Lowï¼ˆä»˜åŠ æ©Ÿèƒ½ï¼‰
        'analysis_cache': 'low',
        'temporary_data': 'low',
    }
    
    @staticmethod
    def get_fallback_behavior(feature: str, degradation_level: int) -> str:
        """
        æ©Ÿèƒ½åˆ¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‹•ä½œæ±ºå®š
        """
        priority = FeatureLevelFallback.FEATURE_PRIORITIES.get(feature, 'medium')
        
        if degradation_level == 1:  # è»½å¾®ãªéšœå®³
            if priority == 'critical':
                return 'database_fallback'  # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            elif priority == 'high':
                return 'local_cache_fallback'  # ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            else:
                return 'default_value'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä½¿ç”¨
        
        elif degradation_level == 2:  # ä¸­ç¨‹åº¦ã®éšœå®³
            if priority == 'critical':
                return 'database_fallback'
            else:
                return 'disable_feature'  # æ©Ÿèƒ½ç„¡åŠ¹åŒ–
        
        elif degradation_level >= 3:  # é‡å¤§ãªéšœå®³
            if priority == 'critical':
                return 'emergency_mode'  # ç·Šæ€¥ãƒ¢ãƒ¼ãƒ‰
            else:
                return 'disable_feature'
        
        return 'disable_feature'
    
    @staticmethod
    def handle_authentication_fallback(session_id: str) -> dict:
        """èªè¨¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†"""
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰èªè¨¼æƒ…å ±å–å¾—
            auth_data = database_auth_fallback(session_id)
            if auth_data:
                return {
                    'logged_in': True,
                    'username': auth_data['username'],
                    'user_role': auth_data['role'],
                    'fallback_mode': True
                }
        except Exception as e:
            logger.error(f"Database auth fallback failed: {e}")
        
        # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚²ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰
        return {
            'logged_in': False,
            'username': 'guest',
            'user_role': 'guest',
            'emergency_mode': True
        }
```

### 2.3 è‡ªå‹•å¾©æ—§æ©Ÿèƒ½

#### ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ãƒ»è‡ªå‹•å¾©æ—§
```python
import asyncio
from datetime import datetime, timedelta

class RedisHealthMonitor:
    """
    Rediså¥å…¨æ€§ç›£è¦–ãƒ»è‡ªå‹•å¾©æ—§
    """
    
    def __init__(self):
        self.check_interval = 30  # 30ç§’æ¯ãƒã‚§ãƒƒã‚¯
        self.failure_count = 0
        self.last_failure_time = None
        self.recovery_attempts = 0
        self.max_recovery_attempts = 5
        
    async def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        while True:
            try:
                await self._health_check()
                await asyncio.sleep(self.check_interval)
            except Exception as e:
                logger.error(f"Health monitor error: {e}")
                await asyncio.sleep(10)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯çŸ­ç¸®é–“éš”
    
    async def _health_check(self):
        """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
        try:
            # Basic connectivity check
            redis_primary.ping()
            
            # Performance check
            start_time = time.time()
            redis_primary.set('health:check', 'ok', ex=60)
            response_time = (time.time() - start_time) * 1000
            
            # Data integrity check
            test_value = redis_primary.get('health:check')
            
            if test_value == 'ok' and response_time < 100:  # 100msä»¥ä¸‹
                await self._handle_recovery()
            else:
                await self._handle_performance_degradation(response_time)
                
        except Exception as e:
            await self._handle_failure(e)
    
    async def _handle_failure(self, error: Exception):
        """éšœå®³å‡¦ç†"""
        self.failure_count += 1
        self.last_failure_time = datetime.now()
        
        logger.error(f"Redis health check failed: {error} (failure #{self.failure_count})")
        
        # Circuit breaker pattern
        if self.failure_count >= 3:
            degradation_strategy.escalate_degradation_level()
            
        # è‡ªå‹•å¾©æ—§è©¦è¡Œ
        if self.recovery_attempts < self.max_recovery_attempts:
            await self._attempt_recovery()
    
    async def _attempt_recovery(self):
        """è‡ªå‹•å¾©æ—§è©¦è¡Œ"""
        self.recovery_attempts += 1
        logger.info(f"Attempting Redis recovery (attempt #{self.recovery_attempts})")
        
        try:
            # æ¥ç¶šãƒ—ãƒ¼ãƒ«ãƒªã‚»ãƒƒãƒˆ
            redis_connection_manager.reset_connection_pool()
            
            # ä»£æ›¿ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè©¦è¡Œ
            await self._try_alternative_endpoints()
            
            # å¾©æ—§ç¢ºèª
            redis_primary.ping()
            logger.info("Redis recovery successful")
            
        except Exception as e:
            logger.error(f"Recovery attempt failed: {e}")
            if self.recovery_attempts >= self.max_recovery_attempts:
                logger.critical("Max recovery attempts reached. Manual intervention required.")
    
    async def _handle_recovery(self):
        """å¾©æ—§å‡¦ç†"""
        if self.failure_count > 0:
            logger.info("Redis service recovered")
            self.failure_count = 0
            self.recovery_attempts = 0
            degradation_strategy.restore_normal_operation()
```

---

## ğŸ“Š 3. ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆè¨­è¨ˆ

### 3.1 CloudWatch ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–

#### ElastiCacheæ¨™æº–ãƒ¡ãƒˆãƒªã‚¯ã‚¹
```yaml
# CloudWatch ã‚¢ãƒ©ãƒ¼ãƒ è¨­å®š
ElastiCacheAlarms:
  # CPUä½¿ç”¨ç‡ç›£è¦–
  - AlarmName: langpont-redis-cpu-high
    MetricName: CPUUtilization
    Namespace: AWS/ElastiCache
    Dimensions:
      - Name: CacheClusterId
        Value: !Ref RedisCluster
    Statistic: Average
    Period: 300
    EvaluationPeriods: 2
    Threshold: 80
    ComparisonOperator: GreaterThanThreshold
    AlarmActions:
      - !Ref SNSTopicArn
  
  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ç›£è¦–
  - AlarmName: langpont-redis-memory-high
    MetricName: DatabaseMemoryUsagePercentage
    Namespace: AWS/ElastiCache
    Statistic: Average
    Period: 300
    EvaluationPeriods: 2
    Threshold: 85
    ComparisonOperator: GreaterThanThreshold
    AlarmActions:
      - !Ref SNSTopicArn
  
  # æ¥ç¶šæ•°ç›£è¦–
  - AlarmName: langpont-redis-connections-high
    MetricName: CurrConnections
    Namespace: AWS/ElastiCache
    Statistic: Average
    Period: 300
    EvaluationPeriods: 2
    Threshold: 45  # 50æ¥ç¶šã®90%
    ComparisonOperator: GreaterThanThreshold
    AlarmActions:
      - !Ref SNSTopicArn
  
  # ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ©ã‚°ç›£è¦–
  - AlarmName: langpont-redis-replication-lag
    MetricName: ReplicationLag
    Namespace: AWS/ElastiCache
    Statistic: Average
    Period: 60
    EvaluationPeriods: 3
    Threshold: 5000  # 5ç§’
    ComparisonOperator: GreaterThanThreshold
    AlarmActions:
      - !Ref SNSTopicArn
```

#### ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
```python
import boto3
from datetime import datetime

class SessionMetricsCollector:
    """
    ã‚»ãƒƒã‚·ãƒ§ãƒ³é–¢é€£ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
    """
    
    def __init__(self):
        self.cloudwatch = boto3.client('cloudwatch')
        self.metrics_namespace = 'LangPont/Session'
    
    def collect_session_metrics(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ»é€ä¿¡"""
        try:
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°
            active_sessions = self._count_active_sessions()
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ“ä½œãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
            operation_latency = self._measure_operation_latency()
            
            # ã‚¨ãƒ©ãƒ¼ç‡
            error_rate = self._calculate_error_rate()
            
            # CloudWatchã«é€ä¿¡
            self._put_metrics([
                {
                    'MetricName': 'ActiveSessions',
                    'Value': active_sessions,
                    'Unit': 'Count'
                },
                {
                    'MetricName': 'OperationLatency',
                    'Value': operation_latency,
                    'Unit': 'Milliseconds'
                },
                {
                    'MetricName': 'ErrorRate',
                    'Value': error_rate,
                    'Unit': 'Percent'
                }
            ])
            
        except Exception as e:
            logger.error(f"Metrics collection failed: {e}")
    
    def _count_active_sessions(self) -> int:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ã‚«ã‚¦ãƒ³ãƒˆ"""
        try:
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚­ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚¹ã‚­ãƒ£ãƒ³
            pattern = "session:auth:*"
            count = 0
            
            for key in redis_primary.scan_iter(match=pattern, count=100):
                if redis_primary.exists(key):
                    count += 1
            
            return count
        except Exception:
            return 0
    
    def _measure_operation_latency(self) -> float:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³æ“ä½œãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¸¬å®š"""
        try:
            start_time = time.time()
            
            # ãƒ†ã‚¹ãƒˆæ“ä½œï¼ˆè»½é‡ï¼‰
            test_key = f"test:latency:{int(time.time())}"
            redis_primary.set(test_key, "test", ex=10)
            redis_primary.get(test_key)
            redis_primary.delete(test_key)
            
            latency_ms = (time.time() - start_time) * 1000
            return round(latency_ms, 2)
        except Exception:
            return 999.0  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯é«˜ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã¨ã—ã¦å ±å‘Š
    
    def _put_metrics(self, metric_data: list):
        """CloudWatchãƒ¡ãƒˆãƒªã‚¯ã‚¹é€ä¿¡"""
        try:
            self.cloudwatch.put_metric_data(
                Namespace=self.metrics_namespace,
                MetricData=[
                    {
                        **metric,
                        'Timestamp': datetime.utcnow(),
                        'Dimensions': [
                            {
                                'Name': 'Environment',
                                'Value': os.getenv('ENVIRONMENT', 'production')
                            },
                            {
                                'Name': 'Service',
                                'Value': 'LangPont'
                            }
                        ]
                    }
                    for metric in metric_data
                ]
            )
        except Exception as e:
            logger.error(f"CloudWatch metrics put failed: {e}")
```

### 3.2 ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­è¨ˆ

#### CloudWatch ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®š
```json
{
  "widgets": [
    {
      "type": "metric",
      "properties": {
        "metrics": [
          ["AWS/ElastiCache", "CPUUtilization", "CacheClusterId", "langpont-session-cluster-001"],
          [".", "DatabaseMemoryUsagePercentage", ".", "."],
          [".", "CurrConnections", ".", "."]
        ],
        "period": 300,
        "stat": "Average",
        "region": "ap-northeast-1",
        "title": "ElastiCache System Metrics"
      }
    },
    {
      "type": "metric", 
      "properties": {
        "metrics": [
          ["LangPont/Session", "ActiveSessions"],
          [".", "OperationLatency"],
          [".", "ErrorRate"]
        ],
        "period": 300,
        "stat": "Average",
        "region": "ap-northeast-1",
        "title": "Session Application Metrics"
      }
    },
    {
      "type": "log",
      "properties": {
        "query": "SOURCE '/aws/elasticache/langpont-session'\n| fields @timestamp, @message\n| filter @message like /ERROR/\n| sort @timestamp desc\n| limit 100",
        "region": "ap-northeast-1",
        "title": "Redis Error Logs"
      }
    }
  ]
}
```

### 3.3 éšœå®³æ¤œçŸ¥ãƒ»é€šçŸ¥è¨­è¨ˆ

#### ãƒãƒ«ãƒãƒ¬ãƒ™ãƒ«é€šçŸ¥
```python
class AlertManager:
    """
    ãƒãƒ«ãƒãƒ¬ãƒ™ãƒ«ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†
    """
    
    ALERT_LEVELS = {
        'info': {'sns_topic': 'langpont-info-alerts', 'urgency': 'low'},
        'warning': {'sns_topic': 'langpont-warning-alerts', 'urgency': 'medium'},
        'critical': {'sns_topic': 'langpont-critical-alerts', 'urgency': 'high'},
        'emergency': {'sns_topic': 'langpont-emergency-alerts', 'urgency': 'immediate'}
    }
    
    def __init__(self):
        self.sns = boto3.client('sns')
    
    def send_alert(self, level: str, message: str, details: dict = None):
        """ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡"""
        alert_config = self.ALERT_LEVELS.get(level, self.ALERT_LEVELS['warning'])
        
        alert_message = {
            'timestamp': datetime.utcnow().isoformat(),
            'service': 'LangPont Session Store',
            'level': level.upper(),
            'message': message,
            'details': details or {},
            'environment': os.getenv('ENVIRONMENT', 'production')
        }
        
        try:
            self.sns.publish(
                TopicArn=f"arn:aws:sns:ap-northeast-1:123456789012:{alert_config['sns_topic']}",
                Message=json.dumps(alert_message, indent=2),
                Subject=f"[{level.upper()}] LangPont Session Alert: {message[:50]}..."
            )
        except Exception as e:
            logger.error(f"Alert sending failed: {e}")
    
    def redis_connection_failed(self, error: str):
        """Redisæ¥ç¶šå¤±æ•—ã‚¢ãƒ©ãƒ¼ãƒˆ"""
        self.send_alert(
            'critical',
            'Redis connection failed',
            {
                'error': error,
                'impact': 'Session functionality degraded',
                'action_required': 'Check ElastiCache cluster status'
            }
        )
    
    def performance_degradation(self, latency_ms: float):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ã‚¢ãƒ©ãƒ¼ãƒˆ"""
        level = 'critical' if latency_ms > 100 else 'warning'
        self.send_alert(
            level,
            f'Session operation latency high: {latency_ms}ms',
            {
                'latency_ms': latency_ms,
                'threshold': '50ms normal, 100ms critical',
                'impact': 'User experience degraded'
            }
        )
```

---

## âœ… çµ±åˆè¨ˆç”»å®Œäº†ç¢ºèª

- âœ… **ElastiCacheæ§‹æˆ**: Multi-AZ Redis Clusterè¨­è¨ˆå®Œäº†
- âœ… **ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç®¡ç†**: Primary/Replicaæ§‹æˆå¯¾å¿œ
- âœ… **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥**: 4æ®µéšGraceful Degradationè¨­è¨ˆå®Œäº†
- âœ… **è‡ªå‹•å¾©æ—§æ©Ÿèƒ½**: ãƒ˜ãƒ«ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»è‡ªå‹•å¾©æ—§å®Ÿè£…è¨­è¨ˆå®Œäº†
- âœ… **ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆ**: CloudWatchçµ±åˆãƒ»ãƒãƒ«ãƒãƒ¬ãƒ™ãƒ«é€šçŸ¥è¨­è¨ˆå®Œäº†

**çµ±åˆè¨ˆç”»å®Œæˆ**: ElastiCacheçµ±åˆè¨­è¨ˆå®Œäº†  
**é‹ç”¨æº–å‚™åº¦**: 100%å®Œäº† - æœ¬ç•ªç’°å¢ƒæŠ•å…¥å¯èƒ½ãƒ¬ãƒ™ãƒ«