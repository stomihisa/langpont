#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LangPont çµ±ä¸€ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ã‚µãƒ¼ãƒ“ã‚¹
AWSç§»è¡Œå¯¾å¿œ & ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ä¿è¨¼
"""

import sqlite3
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import os
from contextlib import contextmanager

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DataSourceInfo:
    """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ±"""
    source_name: str
    database_file: str
    table_name: str
    description: str
    record_count: int
    last_updated: str

@dataclass
class UnifiedMetrics:
    """çµ±ä¸€ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    total_activities: int
    today_activities: int
    total_users: int
    active_users: int
    error_rate: float
    avg_processing_time: float
    data_sources: List[DataSourceInfo]
    generated_at: str

class LangPontDataService:
    """
    LangPontçµ±ä¸€ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ã‚µãƒ¼ãƒ“ã‚¹
    - Single Source of Truth ã®ç¢ºç«‹
    - AWSç§»è¡Œæº–å‚™
    - ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ä¿è¨¼
    """
    
    def __init__(self):
        self.base_dir = os.path.dirname(os.path.abspath(__file__))
        self.cache = {}
        self.cache_ttl = 300  # 5åˆ†ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®å®šç¾©
        self.databases = {
            'activity': 'langpont_activity_log.db',
            'users': 'langpont_users.db', 
            'analytics': 'langpont_analytics.db',
            'history': 'langpont_translation_history.db'
        }
        
        logger.info("ğŸ”§ LangPontçµ±ä¸€ãƒ‡ãƒ¼ã‚¿ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–é–‹å§‹")
        self._validate_databases()
        logger.info("âœ… çµ±ä¸€ãƒ‡ãƒ¼ã‚¿ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–å®Œäº†")
    
    def _validate_databases(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª"""
        for db_name, db_file in self.databases.items():
            db_path = os.path.join(self.base_dir, db_file)
            if os.path.exists(db_path):
                logger.info(f"âœ… {db_name} DBç¢ºèª: {db_file}")
            else:
                logger.warning(f"âš ï¸ {db_name} DBæœªç™ºè¦‹: {db_file}")
    
    @contextmanager
    def get_connection(self, db_name: str):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã®å–å¾—ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ï¼‰"""
        if db_name not in self.databases:
            raise ValueError(f"æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {db_name}")
        
        db_path = os.path.join(self.base_dir, self.databases[db_name])
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        try:
            yield conn
        finally:
            conn.close()
    
    def get_master_activity_metrics(self) -> Dict[str, Any]:
        """
        ãƒã‚¹ã‚¿ãƒ¼æ´»å‹•ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
        - 4æ®µéšåˆ†æã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
        - å…¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®çµ±ä¸€ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
        - ãƒ•ã‚§ãƒ¼ãƒ«ã‚»ãƒ¼ãƒ•å¯¾å¿œæ¸ˆã¿
        """
        cache_key = "master_activity_metrics"
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if self._is_cache_valid(cache_key):
            logger.info("ğŸ“‹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒã‚¹ã‚¿ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—")
            return self.cache[cache_key]['data']
        
        logger.info("ğŸ” ãƒã‚¹ã‚¿ãƒ¼æ´»å‹•ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—é–‹å§‹")
        
        # ğŸ†• ãƒ•ã‚§ãƒ¼ãƒ«ã‚»ãƒ¼ãƒ•å¯¾å¿œ
        try:
            return self._fetch_master_metrics_with_fallback()
        except Exception as e:
            logger.critical(f"ğŸš¨ ãƒã‚¹ã‚¿ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ã§ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
            return self._get_emergency_fallback_data()
    
    def _fetch_master_metrics_with_fallback(self) -> Dict[str, Any]:
        """ãƒ•ã‚§ãƒ¼ãƒ«ã‚»ãƒ¼ãƒ•ä»˜ããƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""
        
        cache_key = "master_activity_metrics"
        
        try:
            with self.get_connection('activity') as conn:
                cursor = conn.cursor()
                
                # åŸºæœ¬çµ±è¨ˆï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
                try:
                    cursor.execute("SELECT COUNT(*) as total FROM analysis_activity_log")
                    total_count = cursor.fetchone()['total']
                except Exception as e:
                    logger.error(f"ğŸ“Š åŸºæœ¬çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                    total_count = 0
                
                # ä»Šæ—¥ã®æ´»å‹•
                today = datetime.now().strftime('%Y-%m-%d')
                cursor.execute("""
                    SELECT COUNT(*) as today_count 
                    FROM analysis_activity_log 
                    WHERE DATE(created_at) = ?
                """, (today,))
                today_count = cursor.fetchone()['today_count']
                
                # ã‚¨ãƒ©ãƒ¼ç‡
                cursor.execute("""
                    SELECT 
                        COUNT(*) as total,
                        SUM(CASE WHEN error_occurred = 1 THEN 1 ELSE 0 END) as errors
                    FROM analysis_activity_log
                """)
                error_data = cursor.fetchone()
                error_rate = (error_data['errors'] / error_data['total'] * 100) if error_data['total'] > 0 else 0
                
                # å¹³å‡å‡¦ç†æ™‚é–“
                cursor.execute("""
                    SELECT AVG(processing_duration) as avg_time
                    FROM analysis_activity_log 
                    WHERE processing_duration > 0
                """)
                avg_time_result = cursor.fetchone()
                avg_time = avg_time_result['avg_time'] or 0
                
                # ã‚¨ãƒ³ã‚¸ãƒ³çµ±è¨ˆ
                cursor.execute("""
                    SELECT 
                        button_pressed,
                        COUNT(*) as count,
                        AVG(confidence) as avg_confidence
                    FROM analysis_activity_log 
                    WHERE button_pressed IS NOT NULL
                    GROUP BY button_pressed
                    ORDER BY count DESC
                """)
                engine_stats = [dict(row) for row in cursor.fetchall()]
                
                # æ¨å¥¨çµæœçµ±è¨ˆ
                cursor.execute("""
                    SELECT 
                        recommendation_result,
                        COUNT(*) as count
                    FROM analysis_activity_log 
                    WHERE recommendation_result IS NOT NULL
                    GROUP BY recommendation_result
                    ORDER BY count DESC
                """)
                recommendation_stats = [dict(row) for row in cursor.fetchall()]
                
                # 4æ®µéšåˆ†æçµ±è¨ˆï¼ˆæ—¢å­˜ã®ã‚«ãƒ©ãƒ åã«å¯¾å¿œï¼‰
                cursor.execute("""
                    SELECT 
                        COUNT(*) as total_stages,
                        SUM(CASE WHEN human_check_result IS NOT NULL THEN 1 ELSE 0 END) as stage0_complete,
                        SUM(CASE WHEN stage1_extraction_method IS NOT NULL THEN 1 ELSE 0 END) as stage1_complete,
                        SUM(CASE WHEN actual_user_choice IS NOT NULL THEN 1 ELSE 0 END) as stage2_complete,
                        SUM(CASE WHEN recommendation_vs_choice_match IS NOT NULL THEN 1 ELSE 0 END) as stage3_complete
                    FROM analysis_activity_log
                """)
                stage_stats = dict(cursor.fetchone())
                
                # æœ€æ–°æ´»å‹•ï¼ˆæ—¢å­˜ã®ã‚«ãƒ©ãƒ åã«å¯¾å¿œï¼‰
                cursor.execute("""
                    SELECT 
                        id, created_at, japanese_text, user_id,
                        button_pressed, recommendation_result, actual_user_choice
                    FROM analysis_activity_log 
                    ORDER BY created_at DESC 
                    LIMIT 10
                """)
                recent_activities = [dict(row) for row in cursor.fetchall()]
            
            result = {
                'basic_stats': {
                    'total_activities': total_count,
                    'today_activities': today_count,
                    'error_rate': round(error_rate, 2),
                    'avg_processing_time': round(avg_time, 2)
                },
                'engine_stats': engine_stats,
                'recommendation_stats': recommendation_stats,
                'four_stage_stats': stage_stats,
                'recent_activities': recent_activities,
                'data_source': {
                    'database': 'langpont_activity_log.db',
                    'table': 'analysis_activity_log',
                    'description': 'ãƒã‚¹ã‚¿ãƒ¼æ´»å‹•ãƒ­ã‚°ï¼ˆ4æ®µéšåˆ†æã‚·ã‚¹ãƒ†ãƒ ï¼‰',
                    'generated_at': datetime.now().isoformat()
                }
            }
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self._save_to_cache(cache_key, result)
            
            logger.info(f"âœ… ãƒã‚¹ã‚¿ãƒ¼æ´»å‹•ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—å®Œäº†: {total_count}ä»¶")
            return result
            
        except sqlite3.Error as e:
            logger.error(f"ğŸ—ƒï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return self._get_database_error_fallback()
        except Exception as e:
            logger.error(f"ğŸ“Š çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return self._get_general_error_fallback()
    
    def _get_database_error_fallback(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿"""
        return {
            'basic_stats': {
                'total_activities': -1,
                'today_activities': -1,
                'error_rate': 100.0,
                'avg_processing_time': 0.0
            },
            'engine_stats': [],
            'recommendation_stats': [],
            'four_stage_stats': {},
            'recent_activities': [],
            'data_source': {
                'database': 'ERROR',
                'table': 'ERROR',
                'description': 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼',
                'generated_at': datetime.now().isoformat()
            },
            'status': 'database_error'
        }
    
    def _get_general_error_fallback(self) -> Dict[str, Any]:
        """ä¸€èˆ¬ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿"""
        return {
            'basic_stats': {
                'total_activities': -2,
                'today_activities': -2,
                'error_rate': 100.0,
                'avg_processing_time': 0.0
            },
            'engine_stats': [],
            'recommendation_stats': [],
            'four_stage_stats': {},
            'recent_activities': [],
            'data_source': {
                'database': 'SYSTEM_ERROR',
                'table': 'SYSTEM_ERROR',
                'description': 'ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼',
                'generated_at': datetime.now().isoformat()
            },
            'status': 'system_error'
        }
    
    def _get_emergency_fallback_data(self) -> Dict[str, Any]:
        """ç·Šæ€¥æ™‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿"""
        return {
            'basic_stats': {
                'total_activities': -3,
                'today_activities': -3,
                'error_rate': 100.0,
                'avg_processing_time': 0.0
            },
            'engine_stats': [],
            'recommendation_stats': [],
            'four_stage_stats': {},
            'recent_activities': [],
            'data_source': {
                'database': 'EMERGENCY_MODE',
                'table': 'EMERGENCY_MODE',
                'description': 'ç·Šæ€¥ãƒ¢ãƒ¼ãƒ‰ - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šä¸å¯',
                'generated_at': datetime.now().isoformat()
            },
            'status': 'emergency_mode'
        }
    
    def get_data_source_summary(self) -> List[DataSourceInfo]:
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®æ¦‚è¦å–å¾—"""
        logger.info("ğŸ” ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ¦‚è¦å–å¾—é–‹å§‹")
        
        sources = []
        
        for db_name, db_file in self.databases.items():
            db_path = os.path.join(self.base_dir, db_file)
            
            if not os.path.exists(db_path):
                continue
                
            with self.get_connection(db_name) as conn:
                cursor = conn.cursor()
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§å–å¾—
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = [row['name'] for row in cursor.fetchall()]
                
                for table in tables:
                    try:
                        # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°å–å¾—
                        cursor.execute(f"SELECT COUNT(*) as count FROM {table}")
                        count = cursor.fetchone()['count']
                        
                        # æœ€çµ‚æ›´æ–°æ—¥å–å¾—ï¼ˆcreated_atã¾ãŸã¯é¡ä¼¼ã‚«ãƒ©ãƒ ãŒã‚ã‚‹å ´åˆï¼‰
                        last_updated = "ä¸æ˜"
                        try:
                            cursor.execute(f"SELECT MAX(created_at) as last_update FROM {table}")
                            result = cursor.fetchone()
                            if result['last_update']:
                                last_updated = result['last_update']
                        except:
                            pass
                        
                        sources.append(DataSourceInfo(
                            source_name=f"{db_name}.{table}",
                            database_file=db_file,
                            table_name=table,
                            description=self._get_table_description(db_name, table),
                            record_count=count,
                            last_updated=last_updated
                        ))
                    except Exception as e:
                        logger.warning(f"âš ï¸ ãƒ†ãƒ¼ãƒ–ãƒ« {table} æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ¦‚è¦å–å¾—å®Œäº†: {len(sources)}å€‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«")
        return sources
    
    def _get_table_description(self, db_name: str, table_name: str) -> str:
        """ãƒ†ãƒ¼ãƒ–ãƒ«ã®èª¬æ˜ã‚’å–å¾—"""
        descriptions = {
            'activity.analysis_activity_log': 'ãƒ¡ã‚¤ãƒ³æ´»å‹•ãƒ­ã‚°ï¼ˆ4æ®µéšåˆ†æã‚·ã‚¹ãƒ†ãƒ ï¼‰',
            'users.users': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±',
            'users.user_sessions': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†',
            'analytics.satisfaction_metrics': 'æº€è¶³åº¦åˆ†æãƒ‡ãƒ¼ã‚¿',
            'history.translation_history': 'ç¿»è¨³å±¥æ­´ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–'
        }
        
        key = f"{db_name}.{table_name}"
        return descriptions.get(key, f"{db_name}ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®{table_name}ãƒ†ãƒ¼ãƒ–ãƒ«")
    
    def get_unified_dashboard_data(self) -> UnifiedMetrics:
        """çµ±ä¸€ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        logger.info("ğŸ“Š çµ±ä¸€ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
        
        # ãƒã‚¹ã‚¿ãƒ¼æ´»å‹•ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        activity_metrics = self.get_master_activity_metrics()
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼çµ±è¨ˆ
        user_stats = self._get_user_statistics()
        
        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ±
        data_sources = self.get_data_source_summary()
        
        unified_metrics = UnifiedMetrics(
            total_activities=activity_metrics['basic_stats']['total_activities'],
            today_activities=activity_metrics['basic_stats']['today_activities'],
            total_users=user_stats['total_users'],
            active_users=user_stats['active_users'],
            error_rate=activity_metrics['basic_stats']['error_rate'],
            avg_processing_time=activity_metrics['basic_stats']['avg_processing_time'],
            data_sources=data_sources,
            generated_at=datetime.now().isoformat()
        )
        
        logger.info("âœ… çµ±ä¸€ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†")
        return unified_metrics
    
    def _get_user_statistics(self) -> Dict[str, int]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼çµ±è¨ˆå–å¾—"""
        try:
            with self.get_connection('users') as conn:
                cursor = conn.cursor()
                
                # ç·ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°
                cursor.execute("SELECT COUNT(*) as total FROM users")
                total_users = cursor.fetchone()['total']
                
                # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ˆ30æ—¥ä»¥å†…ã«ãƒ­ã‚°ã‚¤ãƒ³ï¼‰
                thirty_days_ago = (datetime.now() - timedelta(days=30)).isoformat()
                cursor.execute("""
                    SELECT COUNT(*) as active 
                    FROM users 
                    WHERE last_login_at > ?
                """, (thirty_days_ago,))
                active_users = cursor.fetchone()['active']
                
                return {
                    'total_users': total_users,
                    'active_users': active_users
                }
        except Exception as e:
            logger.warning(f"âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {'total_users': 0, 'active_users': 0}
    
    def _is_cache_valid(self, key: str) -> bool:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯"""
        if key not in self.cache:
            return False
        
        cache_time = self.cache[key]['timestamp']
        return (datetime.now() - cache_time).seconds < self.cache_ttl
    
    def _save_to_cache(self, key: str, data: Any):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        self.cache[key] = {
            'data': data,
            'timestamp': datetime.now()
        }
    
    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"""
        self.cache.clear()
        logger.info("ğŸ—‘ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å®Œäº†")
    
    def health_check(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        logger.info("ğŸ¥ ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯é–‹å§‹")
        
        health_status = {
            'status': 'healthy',
            'databases': {},
            'cache_size': len(self.cache),
            'checked_at': datetime.now().isoformat()
        }
        
        for db_name, db_file in self.databases.items():
            db_path = os.path.join(self.base_dir, db_file)
            
            if os.path.exists(db_path):
                try:
                    with self.get_connection(db_name) as conn:
                        cursor = conn.cursor()
                        cursor.execute("SELECT 1")
                        health_status['databases'][db_name] = {
                            'status': 'connected',
                            'file': db_file,
                            'size_mb': round(os.path.getsize(db_path) / 1024 / 1024, 2)
                        }
                except Exception as e:
                    health_status['databases'][db_name] = {
                        'status': 'error',
                        'error': str(e)
                    }
                    health_status['status'] = 'degraded'
            else:
                health_status['databases'][db_name] = {
                    'status': 'not_found',
                    'file': db_file
                }
                health_status['status'] = 'degraded'
        
        logger.info(f"âœ… ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Œäº†: {health_status['status']}")
        return health_status

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
data_service = LangPontDataService()

def get_data_service() -> LangPontDataService:
    """ãƒ‡ãƒ¼ã‚¿ã‚µãƒ¼ãƒ“ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—"""
    return data_service

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    print("ğŸ§ª LangPontçµ±ä¸€ãƒ‡ãƒ¼ã‚¿ã‚µãƒ¼ãƒ“ã‚¹ãƒ†ã‚¹ãƒˆ")
    
    # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
    health = data_service.health_check()
    print(f"ğŸ“Š ãƒ˜ãƒ«ã‚¹çŠ¶æ³: {health['status']}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ¦‚è¦
    sources = data_service.get_data_source_summary()
    print(f"ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ•°: {len(sources)}")
    for source in sources:
        print(f"  - {source.source_name}: {source.record_count}ä»¶")
    
    # çµ±ä¸€ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    metrics = data_service.get_unified_dashboard_data()
    print(f"ğŸ“ˆ ç·æ´»å‹•æ•°: {metrics.total_activities}")
    print(f"ğŸ“ˆ ä»Šæ—¥ã®æ´»å‹•: {metrics.today_activities}")
    print(f"ğŸ‘¥ ç·ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {metrics.total_users}")
    
    print("âœ… ãƒ†ã‚¹ãƒˆå®Œäº†")