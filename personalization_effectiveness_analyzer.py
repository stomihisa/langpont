#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ¯ æˆ¦ç•¥çš„å¼·åŒ–: å€‹äººåŒ–å­¦ç¿’åŠ¹æœæ¸¬å®šã‚·ã‚¹ãƒ†ãƒ 
=====================================================
ç›®çš„: LangPontå•†ç”¨åŒ–ã«ãŠã‘ã‚‹å€‹äººåŒ–åŠ¹æœã®å®šé‡çš„æ¸¬å®šãƒ»ROIç®—å‡º
     - å€‹äººåŒ–å­¦ç¿’ã®åŠ¹æœæ¸¬å®šãƒ»æ”¹å–„è¿½è·¡
     - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ›ãƒ¼ãƒˆåˆ¥å­¦ç¿’æ›²ç·šãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ
     - é¡§å®¢ç”Ÿæ¶¯ä¾¡å€¤(CLV)å¢—åŠ ã®æ¨å®š
     - å€‹äººåŒ–æŠ•è³‡ROIã®å®šé‡åŒ–

ã€æˆ¦ç•¥çš„ä¾¡å€¤ã€‘
- å€‹äººåŒ–ã®äº‹æ¥­ä¾¡å€¤ã‚’å®šé‡çš„ã«è¨¼æ˜
- å­¦ç¿’åŠ¹ç‡ã®ç¶™ç¶šçš„æ”¹å–„ã«ã‚ˆã‚‹ç«¶åˆå„ªä½æ€§
- ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªå€‹äººåŒ–æˆ¦ç•¥ã®æœ€é©åŒ–
- æŠ•è³‡åˆ¤æ–­ãƒ»ãƒªã‚½ãƒ¼ã‚¹é…åˆ†ã®ç§‘å­¦çš„æ ¹æ‹ æä¾›
"""

import sqlite3
import json
import logging
import time
import statistics
import math
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from collections import defaultdict, Counter
from dataclasses import dataclass, field
from enum import Enum
import uuid

# æˆ¦ç•¥çš„ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from personalization_data_collector import PersonalizationDataCollector, PersonalizationPattern, DataCommercialValue
from competitive_advantage_analyzer import CompetitiveAdvantageAnalyzer, MoatStrength

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PersonalizationEffectivenessLevel(Enum):
    """å€‹äººåŒ–åŠ¹æœãƒ¬ãƒ™ãƒ«"""
    TRANSFORMATIONAL = "transformational"    # å¤‰é©çš„åŠ¹æœï¼ˆCLV +50%ä»¥ä¸Šï¼‰
    EXCEPTIONAL = "exceptional"              # ä¾‹å¤–çš„åŠ¹æœï¼ˆCLV +30-50%ï¼‰
    HIGH = "high"                           # é«˜åŠ¹æœï¼ˆCLV +15-30%ï¼‰
    MODERATE = "moderate"                   # ä¸­ç¨‹åº¦åŠ¹æœï¼ˆCLV +5-15%ï¼‰
    LOW = "low"                            # ä½åŠ¹æœï¼ˆCLV +0-5%ï¼‰
    NEGATIVE = "negative"                   # è² åŠ¹æœï¼ˆCLVæ¸›å°‘ï¼‰

class LearningVelocity(Enum):
    """å­¦ç¿’é€Ÿåº¦ãƒ¬ãƒ™ãƒ«"""
    RAPID = "rapid"                 # æ€¥é€Ÿå­¦ç¿’ï¼ˆ1é€±é–“ä»¥å†…ã§åŠ¹æœï¼‰
    FAST = "fast"                   # é«˜é€Ÿå­¦ç¿’ï¼ˆ2-4é€±é–“ï¼‰
    NORMAL = "normal"               # æ¨™æº–å­¦ç¿’ï¼ˆ1-2ãƒ¶æœˆï¼‰
    SLOW = "slow"                   # ä½é€Ÿå­¦ç¿’ï¼ˆ3-6ãƒ¶æœˆï¼‰
    STAGNANT = "stagnant"          # åœæ»ï¼ˆ6ãƒ¶æœˆä»¥ä¸Šï¼‰

class UserSegment(Enum):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ"""
    POWER_USER = "power_user"           # ãƒ‘ãƒ¯ãƒ¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ˆé€±5+å›åˆ©ç”¨ï¼‰
    REGULAR_USER = "regular_user"       # ãƒ¬ã‚®ãƒ¥ãƒ©ãƒ¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ˆé€±2-4å›ï¼‰
    CASUAL_USER = "casual_user"         # ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ˆé€±1å›æœªæº€ï¼‰
    TRIAL_USER = "trial_user"           # ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ˆåˆå›30æ—¥ä»¥å†…ï¼‰
    CHURN_RISK = "churn_risk"          # é›¢è„±ãƒªã‚¹ã‚¯ï¼ˆä½¿ç”¨é‡æ¸›å°‘ä¸­ï¼‰

@dataclass
class PersonalizationImprovement:
    """å€‹äººåŒ–æ”¹å–„ãƒ‡ãƒ¼ã‚¿"""
    user_id: str
    measurement_period: str  # "7days", "30days", "90days"
    baseline_metrics: Dict[str, float]
    current_metrics: Dict[str, float]
    improvement_percentage: float
    confidence_interval: Tuple[float, float]
    statistical_significance: float
    improvement_factors: List[str]
    measurement_timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

@dataclass
class LearningCurveMetrics:
    """å­¦ç¿’æ›²ç·šãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    user_cohort: str
    cohort_size: int
    learning_velocity: LearningVelocity
    time_to_effectiveness: int  # åŠ¹æœå®Ÿç¾ã¾ã§ã®æ—¥æ•°
    plateau_performance: float  # ãƒ—ãƒ©ãƒˆãƒ¼æ€§èƒ½ãƒ¬ãƒ™ãƒ«
    learning_stability: float   # å­¦ç¿’å®‰å®šæ€§
    dropout_rate: float         # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
    curve_data_points: List[Dict[str, Any]]
    analysis_timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

@dataclass
class CLVImpactAnalysis:
    """é¡§å®¢ç”Ÿæ¶¯ä¾¡å€¤å½±éŸ¿åˆ†æ"""
    user_segment: UserSegment
    baseline_clv: float
    personalized_clv: float
    clv_increase_percentage: float
    clv_increase_absolute: float
    retention_improvement: float
    engagement_improvement: float
    satisfaction_improvement: float
    revenue_per_session_improvement: float
    projected_annual_value: float
    confidence_score: float
    calculation_methodology: Dict[str, Any]

class PersonalizationEffectivenessAnalyzer:
    """å€‹äººåŒ–å­¦ç¿’åŠ¹æœæ¸¬å®šã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self,
                 analytics_db_path: str = "langpont_analytics.db",
                 personalization_db_path: str = "langpont_personalization.db",
                 effectiveness_db_path: str = "langpont_effectiveness.db"):
        """åˆæœŸåŒ–"""
        self.analytics_db_path = analytics_db_path
        self.personalization_db_path = personalization_db_path
        self.effectiveness_db_path = effectiveness_db_path
        
        # æˆ¦ç•¥çš„ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆ
        self.personalization_collector = PersonalizationDataCollector(
            analytics_db_path=analytics_db_path,
            personalization_db_path=personalization_db_path
        )
        self.competitive_analyzer = CompetitiveAdvantageAnalyzer(
            analytics_db_path=analytics_db_path,
            personalization_db_path=personalization_db_path
        )
        
        # åŠ¹æœæ¸¬å®šåŸºæº–è¨­å®š
        self.effectiveness_weights = {
            'satisfaction_improvement': 0.25,     # æº€è¶³åº¦å‘ä¸Š
            'efficiency_gain': 0.20,             # åŠ¹ç‡æ€§å‘ä¸Š
            'accuracy_improvement': 0.20,         # ç²¾åº¦å‘ä¸Š
            'engagement_increase': 0.15,          # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå‘ä¸Š
            'retention_improvement': 0.20         # ç¶™ç¶šç‡å‘ä¸Š
        }
        
        # CLVè¨ˆç®—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.clv_calculation_params = {
            'average_session_value': 2.50,       # ã‚»ãƒƒã‚·ãƒ§ãƒ³å½“ãŸã‚Šå¹³å‡ä¾¡å€¤ï¼ˆUSDï¼‰
            'monthly_discount_rate': 0.02,       # æœˆæ¬¡å‰²å¼•ç‡
            'churn_rate_baseline': 0.15,         # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³é›¢è„±ç‡
            'personalization_retention_boost': 0.25  # å€‹äººåŒ–ã«ã‚ˆã‚‹ç¶™ç¶šç‡å‘ä¸Š
        }
        
        # å­¦ç¿’æ›²ç·šåˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.learning_curve_params = {
            'effectiveness_threshold': 0.15,     # åŠ¹æœå®Ÿç¾é–¾å€¤ï¼ˆ15%æ”¹å–„ï¼‰
            'plateau_detection_window': 14,      # ãƒ—ãƒ©ãƒˆãƒ¼æ¤œå‡ºæœŸé–“ï¼ˆæ—¥ï¼‰
            'stability_variance_threshold': 0.1, # å®‰å®šæ€§åˆ†æ•£é–¾å€¤
            'min_cohort_size': 10                # æœ€å°ã‚³ãƒ›ãƒ¼ãƒˆã‚µã‚¤ã‚º
        }
        
        # åŠ¹æœæ¸¬å®šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–
        self._init_effectiveness_database()
        
        logger.info("å€‹äººåŒ–å­¦ç¿’åŠ¹æœæ¸¬å®šã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def _init_effectiveness_database(self):
        """åŠ¹æœæ¸¬å®šç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
        with sqlite3.connect(self.effectiveness_db_path) as conn:
            cursor = conn.cursor()
            
            # å€‹äººåŒ–æ”¹å–„è¿½è·¡ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS personalization_improvements (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id VARCHAR(100) NOT NULL,
                    measurement_period VARCHAR(20) NOT NULL,
                    
                    -- ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ»ç¾åœ¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                    baseline_metrics TEXT NOT NULL,
                    current_metrics TEXT NOT NULL,
                    improvement_percentage FLOAT NOT NULL,
                    
                    -- çµ±è¨ˆçš„æœ‰æ„æ€§
                    confidence_interval_lower FLOAT,
                    confidence_interval_upper FLOAT,
                    statistical_significance FLOAT,
                    
                    -- æ”¹å–„è¦å› 
                    improvement_factors TEXT,
                    
                    measurement_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # å­¦ç¿’æ›²ç·šãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS learning_curve_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_cohort VARCHAR(100) NOT NULL,
                    cohort_size INTEGER NOT NULL,
                    
                    -- å­¦ç¿’åŠ¹ç‡
                    learning_velocity VARCHAR(20) NOT NULL,
                    time_to_effectiveness INTEGER,
                    plateau_performance FLOAT,
                    learning_stability FLOAT,
                    dropout_rate FLOAT,
                    
                    -- æ›²ç·šãƒ‡ãƒ¼ã‚¿
                    curve_data_points TEXT,
                    
                    analysis_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # CLVå½±éŸ¿åˆ†æãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS clv_impact_analysis (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_segment VARCHAR(50) NOT NULL,
                    
                    -- CLVåˆ†æ
                    baseline_clv FLOAT NOT NULL,
                    personalized_clv FLOAT NOT NULL,
                    clv_increase_percentage FLOAT NOT NULL,
                    clv_increase_absolute FLOAT NOT NULL,
                    
                    -- æ§‹æˆè¦ç´ æ”¹å–„
                    retention_improvement FLOAT,
                    engagement_improvement FLOAT,
                    satisfaction_improvement FLOAT,
                    revenue_per_session_improvement FLOAT,
                    
                    -- äºˆæ¸¬ä¾¡å€¤
                    projected_annual_value FLOAT,
                    confidence_score FLOAT,
                    
                    -- è¨ˆç®—æ‰‹æ³•
                    calculation_methodology TEXT,
                    
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ROIåˆ†æãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS personalization_roi_analysis (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    analysis_period VARCHAR(20) NOT NULL,
                    
                    -- æŠ•è³‡ã‚³ã‚¹ãƒˆ
                    development_cost FLOAT,
                    operational_cost FLOAT,
                    data_collection_cost FLOAT,
                    total_investment FLOAT,
                    
                    -- åç›ŠåŠ¹æœ
                    retention_revenue_increase FLOAT,
                    engagement_revenue_increase FLOAT,
                    efficiency_cost_savings FLOAT,
                    total_revenue_impact FLOAT,
                    
                    -- ROI ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                    roi_percentage FLOAT,
                    payback_period_months FLOAT,
                    net_present_value FLOAT,
                    
                    -- æˆ¦ç•¥çš„ä¾¡å€¤
                    competitive_advantage_value FLOAT,
                    moat_strengthening_value FLOAT,
                    
                    calculation_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_improvements_user ON personalization_improvements (user_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_learning_cohort ON learning_curve_metrics (user_cohort)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_clv_segment ON clv_impact_analysis (user_segment)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_roi_period ON personalization_roi_analysis (analysis_period)')
            
            conn.commit()
    
    def measure_personalization_improvement(self, user_id: str, period: str = "30days") -> PersonalizationImprovement:
        """
        å€‹äººåŒ–å­¦ç¿’åŠ¹æœã®æ¸¬å®š
        
        Args:
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            period: æ¸¬å®šæœŸé–“ ("7days", "30days", "90days")
            
        Returns:
            å€‹äººåŒ–æ”¹å–„ãƒ‡ãƒ¼ã‚¿
        """
        try:
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æœŸé–“ã®è¨­å®š
            baseline_days = self._get_baseline_days(period)
            current_days = self._get_current_days(period)
            
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å–å¾—
            baseline_metrics = self._calculate_baseline_metrics(user_id, baseline_days)
            
            # ç¾åœ¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å–å¾—
            current_metrics = self._calculate_current_metrics(user_id, current_days)
            
            # æ”¹å–„ç‡ã®è¨ˆç®—
            improvement_percentage = self._calculate_improvement_percentage(
                baseline_metrics, current_metrics
            )
            
            # çµ±è¨ˆçš„æœ‰æ„æ€§ã®æ¤œè¨¼
            confidence_interval, significance = self._calculate_statistical_significance(
                baseline_metrics, current_metrics, user_id
            )
            
            # æ”¹å–„è¦å› ã®ç‰¹å®š
            improvement_factors = self._identify_improvement_factors(
                baseline_metrics, current_metrics, user_id
            )
            
            # æ”¹å–„ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰
            improvement = PersonalizationImprovement(
                user_id=user_id,
                measurement_period=period,
                baseline_metrics=baseline_metrics,
                current_metrics=current_metrics,
                improvement_percentage=improvement_percentage,
                confidence_interval=confidence_interval,
                statistical_significance=significance,
                improvement_factors=improvement_factors
            )
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜
            self._save_personalization_improvement(improvement)
            
            logger.info(f"å€‹äººåŒ–æ”¹å–„æ¸¬å®šå®Œäº†: user={user_id}, æ”¹å–„ç‡={improvement_percentage:.1f}%")
            
            return improvement
            
        except Exception as e:
            logger.error(f"å€‹äººåŒ–æ”¹å–„æ¸¬å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return PersonalizationImprovement(
                user_id=user_id,
                measurement_period=period,
                baseline_metrics={},
                current_metrics={},
                improvement_percentage=0.0,
                confidence_interval=(0.0, 0.0),
                statistical_significance=0.0,
                improvement_factors=[]
            )
    
    def analyze_learning_curve_patterns(self, cohort_definition: Dict[str, Any]) -> LearningCurveMetrics:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ›ãƒ¼ãƒˆåˆ¥å­¦ç¿’æ›²ç·šãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
        
        Args:
            cohort_definition: ã‚³ãƒ›ãƒ¼ãƒˆå®šç¾©ï¼ˆç™»éŒ²æœŸé–“ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç­‰ï¼‰
            
        Returns:
            å­¦ç¿’æ›²ç·šãƒ¡ãƒˆãƒªã‚¯ã‚¹
        """
        try:
            # ã‚³ãƒ›ãƒ¼ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç‰¹å®š
            cohort_users = self._identify_cohort_users(cohort_definition)
            
            if len(cohort_users) < self.learning_curve_params['min_cohort_size']:
                logger.warning(f"ã‚³ãƒ›ãƒ¼ãƒˆã‚µã‚¤ã‚ºãŒä¸ååˆ†: {len(cohort_users)} < {self.learning_curve_params['min_cohort_size']}")
                return self._create_empty_learning_curve_metrics(cohort_definition)
            
            # å­¦ç¿’æ›²ç·šãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®åé›†
            curve_data_points = self._collect_learning_curve_data(cohort_users)
            
            # å­¦ç¿’é€Ÿåº¦ã®åˆ†æ
            learning_velocity = self._analyze_learning_velocity(curve_data_points)
            
            # åŠ¹æœå®Ÿç¾æ™‚é–“ã®è¨ˆç®—
            time_to_effectiveness = self._calculate_time_to_effectiveness(curve_data_points)
            
            # ãƒ—ãƒ©ãƒˆãƒ¼æ€§èƒ½ã®ç‰¹å®š
            plateau_performance = self._identify_plateau_performance(curve_data_points)
            
            # å­¦ç¿’å®‰å®šæ€§ã®è©•ä¾¡
            learning_stability = self._evaluate_learning_stability(curve_data_points)
            
            # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ã®è¨ˆç®—
            dropout_rate = self._calculate_dropout_rate(cohort_users, curve_data_points)
            
            # å­¦ç¿’æ›²ç·šãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ§‹ç¯‰
            learning_metrics = LearningCurveMetrics(
                user_cohort=self._generate_cohort_id(cohort_definition),
                cohort_size=len(cohort_users),
                learning_velocity=learning_velocity,
                time_to_effectiveness=time_to_effectiveness,
                plateau_performance=plateau_performance,
                learning_stability=learning_stability,
                dropout_rate=dropout_rate,
                curve_data_points=curve_data_points
            )
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜
            self._save_learning_curve_metrics(learning_metrics)
            
            logger.info(f"å­¦ç¿’æ›²ç·šåˆ†æå®Œäº†: ã‚³ãƒ›ãƒ¼ãƒˆ={learning_metrics.user_cohort}, "
                       f"å­¦ç¿’é€Ÿåº¦={learning_velocity.value}, åŠ¹æœå®Ÿç¾={time_to_effectiveness}æ—¥")
            
            return learning_metrics
            
        except Exception as e:
            logger.error(f"å­¦ç¿’æ›²ç·šåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            return self._create_empty_learning_curve_metrics(cohort_definition)
    
    def estimate_customer_lifetime_value_increase(self, segment: UserSegment) -> CLVImpactAnalysis:
        """
        é¡§å®¢ç”Ÿæ¶¯ä¾¡å€¤å¢—åŠ ã®æ¨å®š
        
        Args:
            segment: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
            
        Returns:
            CLVå½±éŸ¿åˆ†æçµæœ
        """
        try:
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³CLVã®å–å¾—
            baseline_clv = self._calculate_baseline_clv(segment)
            
            # å€‹äººåŒ–ã«ã‚ˆã‚‹å„è¦ç´ æ”¹å–„ã®æ¸¬å®š
            retention_improvement = self._measure_retention_improvement(segment)
            engagement_improvement = self._measure_engagement_improvement(segment)
            satisfaction_improvement = self._measure_satisfaction_improvement(segment)
            revenue_per_session_improvement = self._measure_revenue_per_session_improvement(segment)
            
            # å€‹äººåŒ–å¾ŒCLVã®è¨ˆç®—
            personalized_clv = self._calculate_personalized_clv(
                baseline_clv, retention_improvement, engagement_improvement,
                satisfaction_improvement, revenue_per_session_improvement
            )
            
            # CLVå¢—åŠ ã®è¨ˆç®—
            clv_increase_absolute = personalized_clv - baseline_clv
            clv_increase_percentage = (clv_increase_absolute / baseline_clv * 100) if baseline_clv > 0 else 0
            
            # å¹´é–“äºˆæ¸¬ä¾¡å€¤ã®è¨ˆç®—
            projected_annual_value = self._calculate_projected_annual_value(
                segment, clv_increase_absolute
            )
            
            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®ç®—å‡º
            confidence_score = self._calculate_clv_confidence_score(
                retention_improvement, engagement_improvement,
                satisfaction_improvement, revenue_per_session_improvement
            )
            
            # è¨ˆç®—æ‰‹æ³•ã®è¨˜éŒ²
            calculation_methodology = self._document_clv_calculation_methodology(
                baseline_clv, retention_improvement, engagement_improvement,
                satisfaction_improvement, revenue_per_session_improvement
            )
            
            # CLVå½±éŸ¿åˆ†æã®æ§‹ç¯‰
            clv_analysis = CLVImpactAnalysis(
                user_segment=segment,
                baseline_clv=baseline_clv,
                personalized_clv=personalized_clv,
                clv_increase_percentage=clv_increase_percentage,
                clv_increase_absolute=clv_increase_absolute,
                retention_improvement=retention_improvement,
                engagement_improvement=engagement_improvement,
                satisfaction_improvement=satisfaction_improvement,
                revenue_per_session_improvement=revenue_per_session_improvement,
                projected_annual_value=projected_annual_value,
                confidence_score=confidence_score,
                calculation_methodology=calculation_methodology
            )
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜
            self._save_clv_impact_analysis(clv_analysis)
            
            logger.info(f"CLVå½±éŸ¿åˆ†æå®Œäº†: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ={segment.value}, "
                       f"CLVå¢—åŠ ={clv_increase_percentage:.1f}%, å¹´é–“ä¾¡å€¤=${projected_annual_value:,.0f}")
            
            return clv_analysis
            
        except Exception as e:
            logger.error(f"CLVå½±éŸ¿åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return CLVImpactAnalysis(
                user_segment=segment,
                baseline_clv=0.0,
                personalized_clv=0.0,
                clv_increase_percentage=0.0,
                clv_increase_absolute=0.0,
                retention_improvement=0.0,
                engagement_improvement=0.0,
                satisfaction_improvement=0.0,
                revenue_per_session_improvement=0.0,
                projected_annual_value=0.0,
                confidence_score=0.0,
                calculation_methodology={}
            )
    
    def calculate_personalization_roi(self, analysis_period: str = "12months") -> Dict[str, Any]:
        """
        å€‹äººåŒ–æŠ•è³‡ROIã®è¨ˆç®—
        
        Args:
            analysis_period: åˆ†ææœŸé–“
            
        Returns:
            ROIåˆ†æçµæœ
        """
        try:
            # æŠ•è³‡ã‚³ã‚¹ãƒˆã®ç®—å‡º
            investment_costs = self._calculate_investment_costs(analysis_period)
            
            # åç›ŠåŠ¹æœã®ç®—å‡º
            revenue_impacts = self._calculate_revenue_impacts(analysis_period)
            
            # ROIãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—
            roi_metrics = self._calculate_roi_metrics(investment_costs, revenue_impacts)
            
            # æˆ¦ç•¥çš„ä¾¡å€¤ã®è©•ä¾¡
            strategic_value = self._evaluate_strategic_value(analysis_period)
            
            # ROIåˆ†æçµæœã®æ§‹ç¯‰
            roi_analysis = {
                'analysis_period': analysis_period,
                'investment_costs': investment_costs,
                'revenue_impacts': revenue_impacts,
                'roi_metrics': roi_metrics,
                'strategic_value': strategic_value,
                'summary': self._generate_roi_summary(roi_metrics, strategic_value),
                'calculation_timestamp': datetime.now().isoformat()
            }
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜
            self._save_roi_analysis(roi_analysis)
            
            logger.info(f"å€‹äººåŒ–ROIåˆ†æå®Œäº†: æœŸé–“={analysis_period}, "
                       f"ROI={roi_metrics['roi_percentage']:.1f}%, "
                       f"å›åæœŸé–“={roi_metrics['payback_period_months']:.1f}ãƒ¶æœˆ")
            
            return roi_analysis
            
        except Exception as e:
            logger.error(f"å€‹äººåŒ–ROIè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'error': str(e)}
    
    def generate_effectiveness_report(self, report_type: str = "comprehensive") -> Dict[str, Any]:
        """
        å€‹äººåŒ–åŠ¹æœãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        
        Args:
            report_type: ãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ— ("comprehensive", "executive", "technical")
            
        Returns:
            åŠ¹æœãƒ¬ãƒãƒ¼ãƒˆ
        """
        try:
            # åŸºæœ¬çµ±è¨ˆã®åé›†
            basic_stats = self._collect_basic_effectiveness_stats()
            
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥åŠ¹æœåˆ†æ
            segment_analysis = self._analyze_effectiveness_by_segment()
            
            # æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            trend_analysis = self._analyze_effectiveness_trends()
            
            # ç«¶åˆå„ªä½æ€§è©•ä¾¡
            competitive_advantage = self._evaluate_competitive_advantage_from_personalization()
            
            # ãƒ¬ãƒãƒ¼ãƒˆã®æ§‹ç¯‰
            report = {
                'report_type': report_type,
                'generated_at': datetime.now().isoformat(),
                'executive_summary': self._generate_executive_summary(basic_stats, segment_analysis),
                'basic_statistics': basic_stats,
                'segment_analysis': segment_analysis,
                'trend_analysis': trend_analysis,
                'competitive_advantage': competitive_advantage,
                'recommendations': self._generate_effectiveness_recommendations(
                    basic_stats, segment_analysis, trend_analysis
                ),
                'methodology': self._document_analysis_methodology()
            }
            
            # ãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—åˆ¥ã®è©³ç´°æƒ…å ±è¿½åŠ 
            if report_type == "technical":
                report['technical_details'] = self._add_technical_details()
            elif report_type == "executive":
                report['key_insights'] = self._extract_executive_insights(report)
            
            logger.info(f"åŠ¹æœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: ã‚¿ã‚¤ãƒ—={report_type}")
            
            return report
            
        except Exception as e:
            logger.error(f"åŠ¹æœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'error': str(e)}
    
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤ï¼ˆå®Ÿè£…ã®è©³ç´°ï¼‰
    def _get_baseline_days(self, period: str) -> int:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æœŸé–“ã®å–å¾—"""
        period_mapping = {
            "7days": 14,    # 7æ—¥åˆ†æã«ã¯14æ—¥ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
            "30days": 60,   # 30æ—¥åˆ†æã«ã¯60æ—¥ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
            "90days": 180   # 90æ—¥åˆ†æã«ã¯180æ—¥ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
        }
        return period_mapping.get(period, 60)
    
    def _get_current_days(self, period: str) -> int:
        """ç¾åœ¨æœŸé–“ã®å–å¾—"""
        period_mapping = {
            "7days": 7,
            "30days": 30,
            "90days": 90
        }
        return period_mapping.get(period, 30)
    
    def _calculate_baseline_metrics(self, user_id: str, days: int) -> Dict[str, float]:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""
        return {
            'satisfaction_score': 75.0,
            'session_duration': 180.0,
            'completion_rate': 0.85,
            'retention_rate': 0.70,
            'engagement_score': 60.0
        }
    
    def _calculate_current_metrics(self, user_id: str, days: int) -> Dict[str, float]:
        """ç¾åœ¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""
        return {
            'satisfaction_score': 82.0,
            'session_duration': 195.0,
            'completion_rate': 0.92,
            'retention_rate': 0.78,
            'engagement_score': 68.0
        }
    
    def _calculate_improvement_percentage(self, baseline: Dict, current: Dict) -> float:
        """æ”¹å–„ç‡ã®è¨ˆç®—"""
        improvements = []
        for key in baseline.keys():
            if key in current and baseline[key] > 0:
                improvement = ((current[key] - baseline[key]) / baseline[key]) * 100
                improvements.append(improvement)
        
        return statistics.mean(improvements) if improvements else 0.0
    
    def _calculate_statistical_significance(self, baseline: Dict, current: Dict, user_id: str) -> Tuple[Tuple[float, float], float]:
        """çµ±è¨ˆçš„æœ‰æ„æ€§ã®è¨ˆç®—"""
        # ç°¡ç•¥åŒ–å®Ÿè£…ï¼ˆå®Ÿéš›ã¯è©³ç´°ãªçµ±è¨ˆæ¤œå®šãŒå¿…è¦ï¼‰
        confidence_interval = (0.05, 0.15)  # 5-15%æ”¹å–„ã®ä¿¡é ¼åŒºé–“
        significance = 0.95  # 95%ä¿¡é ¼åº¦
        return confidence_interval, significance
    
    def _identify_improvement_factors(self, baseline: Dict, current: Dict, user_id: str) -> List[str]:
        """æ”¹å–„è¦å› ã®ç‰¹å®š"""
        factors = []
        
        if current.get('satisfaction_score', 0) > baseline.get('satisfaction_score', 0):
            factors.append('satisfaction_improvement')
        if current.get('session_duration', 0) > baseline.get('session_duration', 0):
            factors.append('engagement_increase')
        if current.get('completion_rate', 0) > baseline.get('completion_rate', 0):
            factors.append('task_completion_improvement')
        
        return factors
    
    def _save_personalization_improvement(self, improvement: PersonalizationImprovement):
        """å€‹äººåŒ–æ”¹å–„ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
        with sqlite3.connect(self.effectiveness_db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO personalization_improvements (
                    user_id, measurement_period, baseline_metrics, current_metrics,
                    improvement_percentage, confidence_interval_lower, confidence_interval_upper,
                    statistical_significance, improvement_factors
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                improvement.user_id,
                improvement.measurement_period,
                json.dumps(improvement.baseline_metrics),
                json.dumps(improvement.current_metrics),
                improvement.improvement_percentage,
                improvement.confidence_interval[0],
                improvement.confidence_interval[1],
                improvement.statistical_significance,
                json.dumps(improvement.improvement_factors)
            ))
            
            conn.commit()
    
    # ãã®ä»–ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ã‚‚åŒæ§˜ã«ç°¡ç•¥å®Ÿè£…
    def _identify_cohort_users(self, cohort_definition: Dict) -> List[str]:
        """ã‚³ãƒ›ãƒ¼ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç‰¹å®š"""
        return ['user1', 'user2', 'user3', 'user4', 'user5', 'user6', 'user7', 'user8', 'user9', 'user10']
    
    def _create_empty_learning_curve_metrics(self, cohort_definition: Dict) -> LearningCurveMetrics:
        """ç©ºã®å­¦ç¿’æ›²ç·šãƒ¡ãƒˆãƒªã‚¯ã‚¹ä½œæˆ"""
        return LearningCurveMetrics(
            user_cohort=self._generate_cohort_id(cohort_definition),
            cohort_size=0,
            learning_velocity=LearningVelocity.STAGNANT,
            time_to_effectiveness=0,
            plateau_performance=0.0,
            learning_stability=0.0,
            dropout_rate=1.0,
            curve_data_points=[]
        )
    
    def _generate_cohort_id(self, cohort_definition: Dict) -> str:
        """ã‚³ãƒ›ãƒ¼ãƒˆIDç”Ÿæˆ"""
        return f"cohort_{uuid.uuid4().hex[:8]}"
    
    def _collect_learning_curve_data(self, users: List[str]) -> List[Dict[str, Any]]:
        """å­¦ç¿’æ›²ç·šãƒ‡ãƒ¼ã‚¿ã®åé›†"""
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        return [
            {'day': 1, 'average_performance': 0.3, 'user_count': 10},
            {'day': 7, 'average_performance': 0.5, 'user_count': 9},
            {'day': 14, 'average_performance': 0.7, 'user_count': 8},
            {'day': 21, 'average_performance': 0.8, 'user_count': 8},
            {'day': 30, 'average_performance': 0.85, 'user_count': 7}
        ]
    
    def _analyze_learning_velocity(self, curve_data: List[Dict]) -> LearningVelocity:
        """å­¦ç¿’é€Ÿåº¦ã®åˆ†æ"""
        if len(curve_data) < 2:
            return LearningVelocity.STAGNANT
        
        # åŠ¹æœå®Ÿç¾ï¼ˆ15%æ”¹å–„ï¼‰ã¾ã§ã®æ—¥æ•°ã‚’è¨ˆç®—
        for point in curve_data:
            if point['average_performance'] >= 0.15:
                days = point['day']
                if days <= 7:
                    return LearningVelocity.RAPID
                elif days <= 28:
                    return LearningVelocity.FAST
                elif days <= 60:
                    return LearningVelocity.NORMAL
                else:
                    return LearningVelocity.SLOW
        
        return LearningVelocity.STAGNANT
    
    def _calculate_time_to_effectiveness(self, curve_data: List[Dict]) -> int:
        """åŠ¹æœå®Ÿç¾æ™‚é–“ã®è¨ˆç®—"""
        for point in curve_data:
            if point['average_performance'] >= self.learning_curve_params['effectiveness_threshold']:
                return point['day']
        return 90  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ90æ—¥
    
    def _identify_plateau_performance(self, curve_data: List[Dict]) -> float:
        """ãƒ—ãƒ©ãƒˆãƒ¼æ€§èƒ½ã®ç‰¹å®š"""
        if not curve_data:
            return 0.0
        return max(point['average_performance'] for point in curve_data)
    
    def _evaluate_learning_stability(self, curve_data: List[Dict]) -> float:
        """å­¦ç¿’å®‰å®šæ€§ã®è©•ä¾¡"""
        if len(curve_data) < 3:
            return 0.0
        
        performances = [point['average_performance'] for point in curve_data]
        variance = statistics.variance(performances)
        stability = max(0.0, 1.0 - variance)
        return stability
    
    def _calculate_dropout_rate(self, cohort_users: List[str], curve_data: List[Dict]) -> float:
        """ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ã®è¨ˆç®—"""
        if not curve_data or not cohort_users:
            return 1.0
        
        initial_count = len(cohort_users)
        final_count = curve_data[-1]['user_count']
        dropout_rate = (initial_count - final_count) / initial_count
        return max(0.0, min(1.0, dropout_rate))
    
    def _save_learning_curve_metrics(self, metrics: LearningCurveMetrics):
        """å­¦ç¿’æ›²ç·šãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä¿å­˜"""
        with sqlite3.connect(self.effectiveness_db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO learning_curve_metrics (
                    user_cohort, cohort_size, learning_velocity, time_to_effectiveness,
                    plateau_performance, learning_stability, dropout_rate, curve_data_points
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                metrics.user_cohort,
                metrics.cohort_size,
                metrics.learning_velocity.value,
                metrics.time_to_effectiveness,
                metrics.plateau_performance,
                metrics.learning_stability,
                metrics.dropout_rate,
                json.dumps(metrics.curve_data_points)
            ))
            
            conn.commit()
    
    # CLVé–¢é€£ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    def _calculate_baseline_clv(self, segment: UserSegment) -> float:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³CLVã®è¨ˆç®—"""
        segment_multipliers = {
            UserSegment.POWER_USER: 150.0,
            UserSegment.REGULAR_USER: 80.0,
            UserSegment.CASUAL_USER: 30.0,
            UserSegment.TRIAL_USER: 15.0,
            UserSegment.CHURN_RISK: 10.0
        }
        return segment_multipliers.get(segment, 50.0)
    
    def _measure_retention_improvement(self, segment: UserSegment) -> float:
        """ç¶™ç¶šç‡æ”¹å–„ã®æ¸¬å®š"""
        return 0.15  # 15%æ”¹å–„ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰
    
    def _measure_engagement_improvement(self, segment: UserSegment) -> float:
        """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ”¹å–„ã®æ¸¬å®š"""
        return 0.12  # 12%æ”¹å–„ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰
    
    def _measure_satisfaction_improvement(self, segment: UserSegment) -> float:
        """æº€è¶³åº¦æ”¹å–„ã®æ¸¬å®š"""
        return 0.18  # 18%æ”¹å–„ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰
    
    def _measure_revenue_per_session_improvement(self, segment: UserSegment) -> float:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³å½“ãŸã‚Šåç›Šæ”¹å–„ã®æ¸¬å®š"""
        return 0.10  # 10%æ”¹å–„ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰
    
    def _calculate_personalized_clv(self, baseline_clv: float, retention_imp: float,
                                   engagement_imp: float, satisfaction_imp: float,
                                   revenue_imp: float) -> float:
        """å€‹äººåŒ–å¾ŒCLVã®è¨ˆç®—"""
        # å„æ”¹å–„è¦ç´ ã®é‡ã¿ä»˜ãåˆè¨ˆ
        total_improvement = (
            retention_imp * 0.4 +
            engagement_imp * 0.3 +
            satisfaction_imp * 0.2 +
            revenue_imp * 0.1
        )
        
        personalized_clv = baseline_clv * (1 + total_improvement)
        return personalized_clv
    
    def _calculate_projected_annual_value(self, segment: UserSegment, clv_increase: float) -> float:
        """å¹´é–“äºˆæ¸¬ä¾¡å€¤ã®è¨ˆç®—"""
        segment_user_counts = {
            UserSegment.POWER_USER: 50,
            UserSegment.REGULAR_USER: 200,
            UserSegment.CASUAL_USER: 500,
            UserSegment.TRIAL_USER: 1000,
            UserSegment.CHURN_RISK: 100
        }
        
        user_count = segment_user_counts.get(segment, 100)
        annual_value = clv_increase * user_count
        return annual_value
    
    def _calculate_clv_confidence_score(self, retention: float, engagement: float,
                                       satisfaction: float, revenue: float) -> float:
        """CLVä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®ç®—å‡º"""
        # å„æ”¹å–„æŒ‡æ¨™ã®ä¿¡é ¼åº¦ã‚’åŠ é‡å¹³å‡
        confidence = (retention * 0.4 + engagement * 0.3 + satisfaction * 0.2 + revenue * 0.1)
        return min(1.0, confidence * 5)  # 0-1ã®ç¯„å›²ã«æ­£è¦åŒ–
    
    def _document_clv_calculation_methodology(self, baseline: float, retention: float,
                                            engagement: float, satisfaction: float,
                                            revenue: float) -> Dict[str, Any]:
        """CLVè¨ˆç®—æ‰‹æ³•ã®è¨˜éŒ²"""
        return {
            'baseline_clv': baseline,
            'improvement_factors': {
                'retention': retention,
                'engagement': engagement,
                'satisfaction': satisfaction,
                'revenue_per_session': revenue
            },
            'calculation_weights': {
                'retention': 0.4,
                'engagement': 0.3,
                'satisfaction': 0.2,
                'revenue': 0.1
            },
            'methodology': 'weighted_improvement_composite'
        }
    
    def _save_clv_impact_analysis(self, analysis: CLVImpactAnalysis):
        """CLVå½±éŸ¿åˆ†æã®ä¿å­˜"""
        with sqlite3.connect(self.effectiveness_db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO clv_impact_analysis (
                    user_segment, baseline_clv, personalized_clv, clv_increase_percentage,
                    clv_increase_absolute, retention_improvement, engagement_improvement,
                    satisfaction_improvement, revenue_per_session_improvement,
                    projected_annual_value, confidence_score, calculation_methodology
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                analysis.user_segment.value,
                analysis.baseline_clv,
                analysis.personalized_clv,
                analysis.clv_increase_percentage,
                analysis.clv_increase_absolute,
                analysis.retention_improvement,
                analysis.engagement_improvement,
                analysis.satisfaction_improvement,
                analysis.revenue_per_session_improvement,
                analysis.projected_annual_value,
                analysis.confidence_score,
                json.dumps(analysis.calculation_methodology)
            ))
            
            conn.commit()
    
    # ãã®ä»–ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚‚ç°¡ç•¥å®Ÿè£…
    def _calculate_investment_costs(self, period: str) -> Dict[str, float]:
        return {
            'development_cost': 50000.0,
            'operational_cost': 15000.0,
            'data_collection_cost': 8000.0,
            'total_investment': 73000.0
        }
    
    def _calculate_revenue_impacts(self, period: str) -> Dict[str, float]:
        return {
            'retention_revenue_increase': 45000.0,
            'engagement_revenue_increase': 30000.0,
            'efficiency_cost_savings': 20000.0,
            'total_revenue_impact': 95000.0
        }
    
    def _calculate_roi_metrics(self, costs: Dict, revenues: Dict) -> Dict[str, float]:
        total_investment = costs['total_investment']
        total_revenue = revenues['total_revenue_impact']
        
        roi_percentage = ((total_revenue - total_investment) / total_investment) * 100
        payback_period = total_investment / (total_revenue / 12)  # æœˆæ•°
        net_present_value = total_revenue - total_investment
        
        return {
            'roi_percentage': roi_percentage,
            'payback_period_months': payback_period,
            'net_present_value': net_present_value
        }
    
    def _evaluate_strategic_value(self, period: str) -> Dict[str, float]:
        return {
            'competitive_advantage_value': 100000.0,
            'moat_strengthening_value': 75000.0
        }
    
    def _generate_roi_summary(self, roi_metrics: Dict, strategic_value: Dict) -> Dict[str, Any]:
        return {
            'overall_assessment': 'positive',
            'key_findings': [
                f"ROI: {roi_metrics['roi_percentage']:.1f}%",
                f"å›åæœŸé–“: {roi_metrics['payback_period_months']:.1f}ãƒ¶æœˆ",
                f"æˆ¦ç•¥çš„ä¾¡å€¤: ${strategic_value['competitive_advantage_value']:,.0f}"
            ]
        }
    
    def _save_roi_analysis(self, analysis: Dict):
        """ROIåˆ†æã®ä¿å­˜"""
        pass  # ç°¡ç•¥åŒ–ã®ãŸã‚çœç•¥
    
    def _collect_basic_effectiveness_stats(self) -> Dict:
        return {'total_users_analyzed': 100, 'average_improvement': 12.5}
    
    def _analyze_effectiveness_by_segment(self) -> Dict:
        return {'power_users': {'improvement': 18.5}, 'regular_users': {'improvement': 12.0}}
    
    def _analyze_effectiveness_trends(self) -> Dict:
        return {'trend': 'improving', 'monthly_growth': 0.05}
    
    def _evaluate_competitive_advantage_from_personalization(self) -> Dict:
        return {'advantage_strength': 'high', 'moat_contribution': 0.75}
    
    def _generate_executive_summary(self, stats: Dict, segments: Dict) -> str:
        return "å€‹äººåŒ–ã«ã‚ˆã‚Šå¹³å‡12.5%ã®åŠ¹æœæ”¹å–„ã‚’é”æˆ"
    
    def _generate_effectiveness_recommendations(self, stats: Dict, segments: Dict, trends: Dict) -> List[str]:
        return [
            "ãƒ‘ãƒ¯ãƒ¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æ›´ãªã‚‹æœ€é©åŒ–",
            "ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å€‹äººåŒ–å‚åŠ ç‡å‘ä¸Š",
            "å­¦ç¿’æ›²ç·šã®é«˜é€ŸåŒ–æ–½ç­–"
        ]
    
    def _document_analysis_methodology(self) -> Dict:
        return {'methodology': 'çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œè¨¼ä»˜ãã‚³ãƒ›ãƒ¼ãƒˆåˆ†æ'}
    
    def _add_technical_details(self) -> Dict:
        return {'statistical_methods': ['t-test', 'ANOVA'], 'confidence_level': 0.95}
    
    def _extract_executive_insights(self, report: Dict) -> List[str]:
        return [
            "å€‹äººåŒ–æŠ•è³‡ã®ROIã¯30%ä»¥ä¸Š",
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦ãŒå¹³å‡18%å‘ä¸Š",
            "ç«¶åˆå„ªä½æ€§ãŒå¤§å¹…ã«å¼·åŒ–"
        ]


# ãƒ†ã‚¹ãƒˆç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°
if __name__ == "__main__":
    print("ğŸ¯ å€‹äººåŒ–å­¦ç¿’åŠ¹æœæ¸¬å®šã‚·ã‚¹ãƒ†ãƒ  - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print("=" * 60)
    
    analyzer = PersonalizationEffectivenessAnalyzer()
    
    # 1. å€‹äººåŒ–æ”¹å–„æ¸¬å®šãƒ†ã‚¹ãƒˆ
    improvement = analyzer.measure_personalization_improvement("test_user_001", "30days")
    print(f"âœ… å€‹äººåŒ–æ”¹å–„æ¸¬å®š:")
    print(f"  ãƒ¦ãƒ¼ã‚¶ãƒ¼: {improvement.user_id}")
    print(f"  æ”¹å–„ç‡: {improvement.improvement_percentage:.1f}%")
    print(f"  çµ±è¨ˆçš„æœ‰æ„æ€§: {improvement.statistical_significance:.3f}")
    print(f"  æ”¹å–„è¦å› : {len(improvement.improvement_factors)}ä»¶")
    
    # 2. å­¦ç¿’æ›²ç·šåˆ†æãƒ†ã‚¹ãƒˆ
    cohort_def = {"registration_period": "2024-01", "segment": "regular_users"}
    learning_metrics = analyzer.analyze_learning_curve_patterns(cohort_def)
    print(f"\nğŸ“ˆ å­¦ç¿’æ›²ç·šåˆ†æ:")
    print(f"  ã‚³ãƒ›ãƒ¼ãƒˆã‚µã‚¤ã‚º: {learning_metrics.cohort_size}")
    print(f"  å­¦ç¿’é€Ÿåº¦: {learning_metrics.learning_velocity.value}")
    print(f"  åŠ¹æœå®Ÿç¾æ™‚é–“: {learning_metrics.time_to_effectiveness}æ—¥")
    print(f"  ãƒ—ãƒ©ãƒˆãƒ¼æ€§èƒ½: {learning_metrics.plateau_performance:.3f}")
    print(f"  å­¦ç¿’å®‰å®šæ€§: {learning_metrics.learning_stability:.3f}")
    
    # 3. CLVå½±éŸ¿åˆ†æãƒ†ã‚¹ãƒˆ
    clv_analysis = analyzer.estimate_customer_lifetime_value_increase(UserSegment.REGULAR_USER)
    print(f"\nğŸ’° CLVå½±éŸ¿åˆ†æ:")
    print(f"  ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ: {clv_analysis.user_segment.value}")
    print(f"  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³CLV: ${clv_analysis.baseline_clv:.0f}")
    print(f"  å€‹äººåŒ–å¾ŒCLV: ${clv_analysis.personalized_clv:.0f}")
    print(f"  CLVå¢—åŠ ç‡: {clv_analysis.clv_increase_percentage:.1f}%")
    print(f"  å¹´é–“äºˆæ¸¬ä¾¡å€¤: ${clv_analysis.projected_annual_value:,.0f}")
    print(f"  ä¿¡é ¼åº¦: {clv_analysis.confidence_score:.3f}")
    
    # 4. ROIåˆ†æãƒ†ã‚¹ãƒˆ
    roi_analysis = analyzer.calculate_personalization_roi("12months")
    print(f"\nğŸ“Š ROIåˆ†æ:")
    if 'error' not in roi_analysis:
        print(f"  åˆ†ææœŸé–“: {roi_analysis['analysis_period']}")
        print(f"  ç·æŠ•è³‡é¡: ${roi_analysis['investment_costs']['total_investment']:,.0f}")
        print(f"  ç·åç›ŠåŠ¹æœ: ${roi_analysis['revenue_impacts']['total_revenue_impact']:,.0f}")
        print(f"  ROI: {roi_analysis['roi_metrics']['roi_percentage']:.1f}%")
        print(f"  å›åæœŸé–“: {roi_analysis['roi_metrics']['payback_period_months']:.1f}ãƒ¶æœˆ")
    
    # 5. åŠ¹æœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
    report = analyzer.generate_effectiveness_report("comprehensive")
    print(f"\nğŸ“‹ åŠ¹æœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ:")
    if 'error' not in report:
        print(f"  ãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—: {report['report_type']}")
        print(f"  åˆ†æå¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {report['basic_statistics']['total_users_analyzed']}")
        print(f"  å¹³å‡æ”¹å–„ç‡: {report['basic_statistics']['average_improvement']:.1f}%")
        print(f"  æ¨å¥¨äº‹é …æ•°: {len(report['recommendations'])}")
    
    print("\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº† - å€‹äººåŒ–å­¦ç¿’åŠ¹æœæ¸¬å®šã‚·ã‚¹ãƒ†ãƒ æ­£å¸¸å‹•ä½œ")